{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = np.loadtxt(\"diabetes_ml.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[:,0:8]\n",
    "Y = df[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim = 8, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 3.4853 - acc: 0.5768     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 1.7897 - acc: 0.5000     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 1.0588 - acc: 0.5534     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.8649 - acc: 0.5938     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.7804 - acc: 0.5911     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.7232 - acc: 0.6185     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6898 - acc: 0.6458     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6794 - acc: 0.6367     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6559 - acc: 0.6719     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6513 - acc: 0.6719     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6476 - acc: 0.6706     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6285 - acc: 0.6862     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6857 - acc: 0.6536     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6082 - acc: 0.7044     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6168 - acc: 0.6914     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.6281 - acc: 0.6992     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.6061 - acc: 0.7135     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5915 - acc: 0.7148     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5972 - acc: 0.7005     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5963 - acc: 0.7083     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.6178 - acc: 0.7148     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5871 - acc: 0.7148     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5940 - acc: 0.7083     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5663 - acc: 0.7148     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5764 - acc: 0.7187     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.6048 - acc: 0.7096     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5814 - acc: 0.7357     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5688 - acc: 0.7253     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5962 - acc: 0.7161     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5816 - acc: 0.7057     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5539 - acc: 0.7279     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5557 - acc: 0.7383     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5817 - acc: 0.7122     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5540 - acc: 0.7292     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5557 - acc: 0.7396     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5485 - acc: 0.7305     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5453 - acc: 0.7331     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5621 - acc: 0.7305     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5536 - acc: 0.7279     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5451 - acc: 0.7292     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5420 - acc: 0.7383     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5476 - acc: 0.7461     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5450 - acc: 0.7305     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5432 - acc: 0.7305     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5298 - acc: 0.7344     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5536 - acc: 0.7292     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5531 - acc: 0.7266     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5482 - acc: 0.7331     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5254 - acc: 0.7448     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5362 - acc: 0.7292     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5613 - acc: 0.7201     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5363 - acc: 0.7539     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5143 - acc: 0.7448     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5315 - acc: 0.7409     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5330 - acc: 0.7357     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5250 - acc: 0.7370     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5657 - acc: 0.7122     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.6007 - acc: 0.7018     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5608 - acc: 0.7279     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5312 - acc: 0.7383     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5244 - acc: 0.7422     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5416 - acc: 0.7370     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5365 - acc: 0.7448     - ETA: 0s - loss: 0.5405 - acc: 0\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5335 - acc: 0.739 - 0s - loss: 0.5392 - acc: 0.7396     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5302 - acc: 0.750 - 0s - loss: 0.5180 - acc: 0.7578     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5307 - acc: 0.7487     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5072 - acc: 0.7617     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5107 - acc: 0.7487     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5192 - acc: 0.7578     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5187 - acc: 0.7565     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5308 - acc: 0.7409     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5252 - acc: 0.7461     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.4965 - acc: 0.7591     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5309 - acc: 0.7565     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5004 - acc: 0.7500     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5026 - acc: 0.7500     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5089 - acc: 0.7539     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.4986 - acc: 0.7826     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5100 - acc: 0.7578     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5172 - acc: 0.7708     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5159 - acc: 0.7643     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5256 - acc: 0.7461     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.5106 - acc: 0.7669     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.5061 - acc: 0.7578     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5152 - acc: 0.7539     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5030 - acc: 0.7565     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4993 - acc: 0.7500     \n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s - loss: 0.5179 - acc: 0.7539     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.5041 - acc: 0.7526     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5024 - acc: 0.7604     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5315 - acc: 0.7396     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.5025 - acc: 0.7565     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4975 - acc: 0.7669     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.5038 - acc: 0.7539     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.4927 - acc: 0.7630     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4983 - acc: 0.7682     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.5220 - acc: 0.7461     - ETA: 0s - loss: 0.6006 - acc: 0.\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.5011 - acc: 0.7747     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.5107 - acc: 0.7552     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4920 - acc: 0.7669     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4876 - acc: 0.7656     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4913 - acc: 0.7630     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4993 - acc: 0.7630     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4925 - acc: 0.7617     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.4788 - acc: 0.7839     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4842 - acc: 0.7760     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4952 - acc: 0.7604     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4843 - acc: 0.7852     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4918 - acc: 0.7578     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4784 - acc: 0.7682     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4937 - acc: 0.7513     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.5211 - acc: 0.7643     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4860 - acc: 0.7669     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4959 - acc: 0.7604     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4932 - acc: 0.7617     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4886 - acc: 0.7643     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4905 - acc: 0.7747     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7656     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4868 - acc: 0.7773     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.5277 - acc: 0.7331     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4807 - acc: 0.7552     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4776 - acc: 0.7760     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4795 - acc: 0.7747     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4937 - acc: 0.7565     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4983 - acc: 0.7565     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4923 - acc: 0.7526     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4674 - acc: 0.7786     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4997 - acc: 0.7617     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.5005 - acc: 0.7643     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.5030 - acc: 0.7539     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4887 - acc: 0.7760     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4975 - acc: 0.7656     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4787 - acc: 0.7695     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4706 - acc: 0.7760     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4740 - acc: 0.7682     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4674 - acc: 0.7891     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4715 - acc: 0.7695     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4671 - acc: 0.7786     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4860 - acc: 0.7799     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4814 - acc: 0.7708     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4799 - acc: 0.7630     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4858 - acc: 0.7708     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4793 - acc: 0.7773     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4728 - acc: 0.7695     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4766 - acc: 0.7656     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4729 - acc: 0.7799     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4678 - acc: 0.7682     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4759 - acc: 0.7799     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4766 - acc: 0.7747     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4672 - acc: 0.7852     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120452b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/768 [=========>....................] - ETA: 0s\n",
      "acc: 79.43%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s - loss: 0.4717 - acc: 0.7646 - val_loss: 0.4512 - val_acc: 0.7795\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s - loss: 0.5252 - acc: 0.7743 - val_loss: 0.5314 - val_acc: 0.7717\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s - loss: 0.5025 - acc: 0.7393 - val_loss: 0.4785 - val_acc: 0.7756\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s - loss: 0.4777 - acc: 0.7704 - val_loss: 0.4452 - val_acc: 0.8228\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s - loss: 0.4952 - acc: 0.7549 - val_loss: 0.4574 - val_acc: 0.8228\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s - loss: 0.4727 - acc: 0.7626 - val_loss: 0.4688 - val_acc: 0.7835\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s - loss: 0.4820 - acc: 0.7724 - val_loss: 0.5345 - val_acc: 0.7559\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s - loss: 0.5126 - acc: 0.7335 - val_loss: 0.4807 - val_acc: 0.7638\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s - loss: 0.4797 - acc: 0.7704 - val_loss: 0.4637 - val_acc: 0.8071\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s - loss: 0.4647 - acc: 0.7860 - val_loss: 0.4861 - val_acc: 0.7598\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s - loss: 0.4840 - acc: 0.7685 - val_loss: 0.4653 - val_acc: 0.7874\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s - loss: 0.4895 - acc: 0.7685 - val_loss: 0.4712 - val_acc: 0.7638\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s - loss: 0.4986 - acc: 0.7588 - val_loss: 0.4966 - val_acc: 0.7756\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s - loss: 0.4956 - acc: 0.7471 - val_loss: 0.4416 - val_acc: 0.7992\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s - loss: 0.5127 - acc: 0.7374 - val_loss: 0.4716 - val_acc: 0.7756\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s - loss: 0.4967 - acc: 0.7626 - val_loss: 0.5000 - val_acc: 0.7559\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s - loss: 0.4847 - acc: 0.7588 - val_loss: 0.4559 - val_acc: 0.8150\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s - loss: 0.4814 - acc: 0.7607 - val_loss: 0.4645 - val_acc: 0.7835\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s - loss: 0.4762 - acc: 0.7588 - val_loss: 0.4765 - val_acc: 0.7756\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s - loss: 0.4755 - acc: 0.7685 - val_loss: 0.4850 - val_acc: 0.7520\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s - loss: 0.4872 - acc: 0.7665 - val_loss: 0.4728 - val_acc: 0.7795\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s - loss: 0.4696 - acc: 0.7510 - val_loss: 0.4477 - val_acc: 0.7953\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s - loss: 0.4874 - acc: 0.7588 - val_loss: 0.4523 - val_acc: 0.7992\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s - loss: 0.4720 - acc: 0.7607 - val_loss: 0.4683 - val_acc: 0.79130.7\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s - loss: 0.4706 - acc: 0.7607 - val_loss: 0.5149 - val_acc: 0.7598\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s - loss: 0.4824 - acc: 0.7471 - val_loss: 0.4848 - val_acc: 0.7677\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s - loss: 0.4815 - acc: 0.7743 - val_loss: 0.4546 - val_acc: 0.7992\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s - loss: 0.4824 - acc: 0.7724 - val_loss: 0.5300 - val_acc: 0.7480\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s - loss: 0.4753 - acc: 0.7568 - val_loss: 0.5378 - val_acc: 0.7323\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s - loss: 0.4730 - acc: 0.7724 - val_loss: 0.4795 - val_acc: 0.7756\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s - loss: 0.4624 - acc: 0.7607 - val_loss: 0.4846 - val_acc: 0.7677\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s - loss: 0.4970 - acc: 0.7607 - val_loss: 0.4652 - val_acc: 0.7874\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s - loss: 0.4723 - acc: 0.7529 - val_loss: 0.4540 - val_acc: 0.8071\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s - loss: 0.4854 - acc: 0.7724 - val_loss: 0.4813 - val_acc: 0.7835\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s - loss: 0.4693 - acc: 0.7840 - val_loss: 0.4896 - val_acc: 0.7874\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s - loss: 0.4749 - acc: 0.7782 - val_loss: 0.4652 - val_acc: 0.8031\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s - loss: 0.4851 - acc: 0.7626 - val_loss: 0.5085 - val_acc: 0.7677\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s - loss: 0.4638 - acc: 0.7840 - val_loss: 0.4685 - val_acc: 0.7913\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s - loss: 0.4729 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7795\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s - loss: 0.4767 - acc: 0.7743 - val_loss: 0.4857 - val_acc: 0.7638\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4750 - val_acc: 0.7795\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s - loss: 0.4649 - acc: 0.7743 - val_loss: 0.4616 - val_acc: 0.8110\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s - loss: 0.4684 - acc: 0.7685 - val_loss: 0.5328 - val_acc: 0.7165\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s - loss: 0.4844 - acc: 0.7471 - val_loss: 0.5046 - val_acc: 0.7756\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s - loss: 0.4665 - acc: 0.7918 - val_loss: 0.4718 - val_acc: 0.7913\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s - loss: 0.4988 - acc: 0.7335 - val_loss: 0.5072 - val_acc: 0.7913\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s - loss: 0.4848 - acc: 0.7626 - val_loss: 0.4676 - val_acc: 0.8031\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s - loss: 0.4718 - acc: 0.7626 - val_loss: 0.4588 - val_acc: 0.7953\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s - loss: 0.4586 - acc: 0.7782 - val_loss: 0.4854 - val_acc: 0.7835\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s - loss: 0.4861 - acc: 0.7510 - val_loss: 0.4649 - val_acc: 0.7756\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s - loss: 0.4753 - acc: 0.7665 - val_loss: 0.4944 - val_acc: 0.7992\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s - loss: 0.4850 - acc: 0.7704 - val_loss: 0.5054 - val_acc: 0.7520\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s - loss: 0.4850 - acc: 0.7704 - val_loss: 0.4738 - val_acc: 0.7992\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s - loss: 0.4607 - acc: 0.7588 - val_loss: 0.5066 - val_acc: 0.7598\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s - loss: 0.4982 - acc: 0.7549 - val_loss: 0.4989 - val_acc: 0.7598\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s - loss: 0.5160 - acc: 0.7529 - val_loss: 0.6080 - val_acc: 0.6772\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s - loss: 0.4986 - acc: 0.7685 - val_loss: 0.4825 - val_acc: 0.7677\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s - loss: 0.4801 - acc: 0.7665 - val_loss: 0.4889 - val_acc: 0.7756\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s - loss: 0.4589 - acc: 0.7743 - val_loss: 0.4694 - val_acc: 0.7953\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s - loss: 0.4628 - acc: 0.7685 - val_loss: 0.4685 - val_acc: 0.7953\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4448 - acc: 0.780 - 0s - loss: 0.4475 - acc: 0.7782 - val_loss: 0.4781 - val_acc: 0.7913\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s - loss: 0.4568 - acc: 0.7704 - val_loss: 0.4813 - val_acc: 0.7795\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s - loss: 0.4545 - acc: 0.7802 - val_loss: 0.5072 - val_acc: 0.7795\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s - loss: 0.4509 - acc: 0.7899 - val_loss: 0.4692 - val_acc: 0.7992\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s - loss: 0.4778 - acc: 0.7626 - val_loss: 0.4876 - val_acc: 0.7913\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s - loss: 0.4501 - acc: 0.7782 - val_loss: 0.4757 - val_acc: 0.7874\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s - loss: 0.4614 - acc: 0.7802 - val_loss: 0.5147 - val_acc: 0.7441\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s - loss: 0.4655 - acc: 0.7724 - val_loss: 0.5193 - val_acc: 0.7559\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s - loss: 0.4739 - acc: 0.7704 - val_loss: 0.5290 - val_acc: 0.7559\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s - loss: 0.4573 - acc: 0.7899 - val_loss: 0.5605 - val_acc: 0.7441\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s - loss: 0.4730 - acc: 0.7743 - val_loss: 0.4814 - val_acc: 0.7795\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s - loss: 0.4693 - acc: 0.7665 - val_loss: 0.4975 - val_acc: 0.7992\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s - loss: 0.5069 - acc: 0.7549 - val_loss: 0.6229 - val_acc: 0.7126\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s - loss: 0.4724 - acc: 0.7626 - val_loss: 0.4972 - val_acc: 0.7756\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s - loss: 0.4551 - acc: 0.7802 - val_loss: 0.5849 - val_acc: 0.7283\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s - loss: 0.4563 - acc: 0.7743 - val_loss: 0.4930 - val_acc: 0.7874\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s - loss: 0.4619 - acc: 0.7840 - val_loss: 0.4895 - val_acc: 0.7677\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s - loss: 0.4560 - acc: 0.7782 - val_loss: 0.4925 - val_acc: 0.7913\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s - loss: 0.4845 - acc: 0.7626 - val_loss: 0.4728 - val_acc: 0.7913\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s - loss: 0.4510 - acc: 0.7763 - val_loss: 0.4944 - val_acc: 0.7953\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s - loss: 0.4699 - acc: 0.7743 - val_loss: 0.4792 - val_acc: 0.7795\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s - loss: 0.4510 - acc: 0.7918 - val_loss: 0.4908 - val_acc: 0.7795\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s - loss: 0.4474 - acc: 0.7763 - val_loss: 0.5052 - val_acc: 0.7953\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s - loss: 0.4708 - acc: 0.7704 - val_loss: 0.4734 - val_acc: 0.7756\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s - loss: 0.4489 - acc: 0.7802 - val_loss: 0.5294 - val_acc: 0.7559\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s - loss: 0.4652 - acc: 0.7724 - val_loss: 0.4885 - val_acc: 0.7835\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s - loss: 0.4978 - acc: 0.7724 - val_loss: 0.4745 - val_acc: 0.7953\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s - loss: 0.4926 - acc: 0.7802 - val_loss: 0.5302 - val_acc: 0.7520\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s - loss: 0.4623 - acc: 0.7782 - val_loss: 0.4715 - val_acc: 0.7913\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s - loss: 0.4454 - acc: 0.7821 - val_loss: 0.4823 - val_acc: 0.7638\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s - loss: 0.4513 - acc: 0.7918 - val_loss: 0.4874 - val_acc: 0.7874\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s - loss: 0.4751 - acc: 0.7802 - val_loss: 0.4958 - val_acc: 0.7480\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s - loss: 0.4495 - acc: 0.7782 - val_loss: 0.4832 - val_acc: 0.7677\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s - loss: 0.4533 - acc: 0.7743 - val_loss: 0.5004 - val_acc: 0.7717\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s - loss: 0.4626 - acc: 0.7977 - val_loss: 0.5154 - val_acc: 0.7677\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s - loss: 0.4528 - acc: 0.7724 - val_loss: 0.4950 - val_acc: 0.7795\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s - loss: 0.4553 - acc: 0.7763 - val_loss: 0.4929 - val_acc: 0.7874\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s - loss: 0.4455 - acc: 0.7802 - val_loss: 0.5061 - val_acc: 0.7677\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s - loss: 0.4435 - acc: 0.7802 - val_loss: 0.5785 - val_acc: 0.7087\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s - loss: 0.4427 - acc: 0.7899 - val_loss: 0.4741 - val_acc: 0.7717\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4749 - acc: 0.778 - 0s - loss: 0.4775 - acc: 0.7704 - val_loss: 0.5175 - val_acc: 0.7559\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s - loss: 0.4553 - acc: 0.7802 - val_loss: 0.5224 - val_acc: 0.7559\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s - loss: 0.4486 - acc: 0.7879 - val_loss: 0.4845 - val_acc: 0.7677\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s - loss: 0.4435 - acc: 0.7724 - val_loss: 0.5219 - val_acc: 0.7480\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s - loss: 0.4865 - acc: 0.7626 - val_loss: 0.6637 - val_acc: 0.6890\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s - loss: 0.4819 - acc: 0.7782 - val_loss: 0.4906 - val_acc: 0.7520\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s - loss: 0.4563 - acc: 0.7821 - val_loss: 0.5745 - val_acc: 0.7165\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s - loss: 0.4432 - acc: 0.7763 - val_loss: 0.4855 - val_acc: 0.7677\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s - loss: 0.4636 - acc: 0.7860 - val_loss: 0.4912 - val_acc: 0.7756\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s - loss: 0.4522 - acc: 0.7977 - val_loss: 0.4911 - val_acc: 0.7795\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s - loss: 0.4550 - acc: 0.7840 - val_loss: 0.4874 - val_acc: 0.7874\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4401 - acc: 0.797 - 0s - loss: 0.4574 - acc: 0.7860 - val_loss: 0.5263 - val_acc: 0.7598\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s - loss: 0.4646 - acc: 0.7763 - val_loss: 0.5032 - val_acc: 0.7756\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s - loss: 0.4469 - acc: 0.7996 - val_loss: 0.5042 - val_acc: 0.7677\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s - loss: 0.4593 - acc: 0.7704 - val_loss: 0.5081 - val_acc: 0.8031\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s - loss: 0.4472 - acc: 0.7860 - val_loss: 0.5048 - val_acc: 0.7677\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s - loss: 0.4777 - acc: 0.7802 - val_loss: 0.5070 - val_acc: 0.7598\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s - loss: 0.4442 - acc: 0.7840 - val_loss: 0.4924 - val_acc: 0.7795\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s - loss: 0.4629 - acc: 0.7899 - val_loss: 0.4916 - val_acc: 0.7756\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s - loss: 0.4470 - acc: 0.7860 - val_loss: 0.5312 - val_acc: 0.7520\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s - loss: 0.4531 - acc: 0.7782 - val_loss: 0.5247 - val_acc: 0.7323\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s - loss: 0.4589 - acc: 0.7957 - val_loss: 0.5196 - val_acc: 0.7480\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s - loss: 0.4417 - acc: 0.7860 - val_loss: 0.5335 - val_acc: 0.7677\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s - loss: 0.4515 - acc: 0.7704 - val_loss: 0.4818 - val_acc: 0.7835\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s - loss: 0.4427 - acc: 0.7802 - val_loss: 0.4667 - val_acc: 0.7795\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s - loss: 0.4436 - acc: 0.7879 - val_loss: 0.4841 - val_acc: 0.7717\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s - loss: 0.4520 - acc: 0.7938 - val_loss: 0.4896 - val_acc: 0.7756\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s - loss: 0.4415 - acc: 0.7821 - val_loss: 0.5238 - val_acc: 0.7835\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s - loss: 0.4397 - acc: 0.7879 - val_loss: 0.4776 - val_acc: 0.7756\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s - loss: 0.4559 - acc: 0.7743 - val_loss: 0.4856 - val_acc: 0.8031\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s - loss: 0.4345 - acc: 0.7938 - val_loss: 0.5881 - val_acc: 0.7087\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s - loss: 0.4361 - acc: 0.7724 - val_loss: 0.5106 - val_acc: 0.7717\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s - loss: 0.4375 - acc: 0.7918 - val_loss: 0.4900 - val_acc: 0.8031\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s - loss: 0.4477 - acc: 0.7957 - val_loss: 0.4772 - val_acc: 0.7835\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s - loss: 0.4384 - acc: 0.7860 - val_loss: 0.5061 - val_acc: 0.7874\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s - loss: 0.4356 - acc: 0.8016 - val_loss: 0.5489 - val_acc: 0.7598\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s - loss: 0.4612 - acc: 0.7782 - val_loss: 0.5446 - val_acc: 0.7520\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s - loss: 0.4457 - acc: 0.7977 - val_loss: 0.4843 - val_acc: 0.7874\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s - loss: 0.4594 - acc: 0.7782 - val_loss: 0.4997 - val_acc: 0.7717\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s - loss: 0.4614 - acc: 0.7782 - val_loss: 0.4923 - val_acc: 0.8031\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s - loss: 0.4608 - acc: 0.7782 - val_loss: 0.4753 - val_acc: 0.7795\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s - loss: 0.4316 - acc: 0.7957 - val_loss: 0.6084 - val_acc: 0.7126\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s - loss: 0.4772 - acc: 0.7704 - val_loss: 0.4918 - val_acc: 0.7717\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s - loss: 0.4330 - acc: 0.7840 - val_loss: 0.5841 - val_acc: 0.7323\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s - loss: 0.4966 - acc: 0.7471 - val_loss: 0.4848 - val_acc: 0.7756\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s - loss: 0.4399 - acc: 0.7918 - val_loss: 0.4880 - val_acc: 0.7520\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s - loss: 0.4429 - acc: 0.7879 - val_loss: 0.4875 - val_acc: 0.7835\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s - loss: 0.4441 - acc: 0.7879 - val_loss: 0.5069 - val_acc: 0.7913\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s - loss: 0.4545 - acc: 0.7704 - val_loss: 0.4965 - val_acc: 0.7717\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s - loss: 0.4398 - acc: 0.7879 - val_loss: 0.4889 - val_acc: 0.7677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1203f6810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s - loss: 0.4549 - acc: 0.7802 - val_loss: 0.4575 - val_acc: 0.8071\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s - loss: 0.4563 - acc: 0.7840 - val_loss: 0.4851 - val_acc: 0.79130.\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s - loss: 0.4585 - acc: 0.7704 - val_loss: 0.4736 - val_acc: 0.8031\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s - loss: 0.4529 - acc: 0.7782 - val_loss: 0.5112 - val_acc: 0.7677\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s - loss: 0.4490 - acc: 0.7821 - val_loss: 0.4610 - val_acc: 0.7835\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s - loss: 0.4364 - acc: 0.7763 - val_loss: 0.4837 - val_acc: 0.8071\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s - loss: 0.4385 - acc: 0.7860 - val_loss: 0.4578 - val_acc: 0.8228\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s - loss: 0.4274 - acc: 0.8035 - val_loss: 0.4596 - val_acc: 0.79130.812\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s - loss: 0.4417 - acc: 0.7938 - val_loss: 0.5105 - val_acc: 0.7638\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s - loss: 0.4473 - acc: 0.7860 - val_loss: 0.4642 - val_acc: 0.8110\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s - loss: 0.4264 - acc: 0.7821 - val_loss: 0.4527 - val_acc: 0.8031\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s - loss: 0.4430 - acc: 0.7860 - val_loss: 0.4529 - val_acc: 0.7835\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s - loss: 0.4413 - acc: 0.7879 - val_loss: 0.4720 - val_acc: 0.7638\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s - loss: 0.4431 - acc: 0.7996 - val_loss: 0.5034 - val_acc: 0.7756\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s - loss: 0.4410 - acc: 0.7821 - val_loss: 0.4651 - val_acc: 0.7874\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s - loss: 0.4319 - acc: 0.7957 - val_loss: 0.4619 - val_acc: 0.7835\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s - loss: 0.4319 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7795\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s - loss: 0.4287 - acc: 0.7977 - val_loss: 0.4760 - val_acc: 0.7795\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s - loss: 0.4638 - acc: 0.7685 - val_loss: 0.5160 - val_acc: 0.7441\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s - loss: 0.4381 - acc: 0.7782 - val_loss: 0.4581 - val_acc: 0.8189\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s - loss: 0.4271 - acc: 0.7918 - val_loss: 0.4743 - val_acc: 0.7953\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s - loss: 0.4276 - acc: 0.7879 - val_loss: 0.4560 - val_acc: 0.8110\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s - loss: 0.4493 - acc: 0.7802 - val_loss: 0.4797 - val_acc: 0.7756\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s - loss: 0.4398 - acc: 0.7996 - val_loss: 0.4557 - val_acc: 0.8150\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s - loss: 0.4711 - acc: 0.7646 - val_loss: 0.4833 - val_acc: 0.8031\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s - loss: 0.4357 - acc: 0.7938 - val_loss: 0.4656 - val_acc: 0.8071\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s - loss: 0.5424 - acc: 0.7568 - val_loss: 0.4782 - val_acc: 0.7717\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s - loss: 0.4259 - acc: 0.7977 - val_loss: 0.4642 - val_acc: 0.7913\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s - loss: 0.4266 - acc: 0.7977 - val_loss: 0.4845 - val_acc: 0.7795\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s - loss: 0.4361 - acc: 0.7938 - val_loss: 0.4575 - val_acc: 0.8031\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s - loss: 0.4185 - acc: 0.8093 - val_loss: 0.4634 - val_acc: 0.8268\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s - loss: 0.4280 - acc: 0.7977 - val_loss: 0.4732 - val_acc: 0.7874\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s - loss: 0.4356 - acc: 0.7879 - val_loss: 0.4811 - val_acc: 0.7795\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s - loss: 0.4437 - acc: 0.7918 - val_loss: 0.4768 - val_acc: 0.7874\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s - loss: 0.4181 - acc: 0.8191 - val_loss: 0.5151 - val_acc: 0.7677\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s - loss: 0.4380 - acc: 0.7860 - val_loss: 0.5648 - val_acc: 0.7283\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s - loss: 0.4237 - acc: 0.8035 - val_loss: 0.4703 - val_acc: 0.7874\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s - loss: 0.4290 - acc: 0.8074 - val_loss: 0.4869 - val_acc: 0.7835\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s - loss: 0.4275 - acc: 0.7977 - val_loss: 0.4810 - val_acc: 0.7913\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s - loss: 0.4330 - acc: 0.7899 - val_loss: 0.4751 - val_acc: 0.8150\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s - loss: 0.4192 - acc: 0.8113 - val_loss: 0.5070 - val_acc: 0.7795\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s - loss: 0.4246 - acc: 0.7977 - val_loss: 0.4748 - val_acc: 0.7913\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s - loss: 0.4171 - acc: 0.7977 - val_loss: 0.4757 - val_acc: 0.7874\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s - loss: 0.4128 - acc: 0.8016 - val_loss: 0.4722 - val_acc: 0.8031\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s - loss: 0.4290 - acc: 0.7899 - val_loss: 0.4763 - val_acc: 0.8150\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s - loss: 0.4323 - acc: 0.7938 - val_loss: 0.4946 - val_acc: 0.7756\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s - loss: 0.4192 - acc: 0.8113 - val_loss: 0.4773 - val_acc: 0.8031\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s - loss: 0.4318 - acc: 0.7899 - val_loss: 0.5080 - val_acc: 0.7717\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s - loss: 0.4155 - acc: 0.8074 - val_loss: 0.5025 - val_acc: 0.7756\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s - loss: 0.4274 - acc: 0.7840 - val_loss: 0.4742 - val_acc: 0.8031\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s - loss: 0.4272 - acc: 0.7957 - val_loss: 0.4771 - val_acc: 0.8110\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s - loss: 0.4314 - acc: 0.7840 - val_loss: 0.4905 - val_acc: 0.7913\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s - loss: 0.4250 - acc: 0.7996 - val_loss: 0.4804 - val_acc: 0.7913\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s - loss: 0.4336 - acc: 0.7821 - val_loss: 0.5103 - val_acc: 0.7520\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s - loss: 0.4158 - acc: 0.8074 - val_loss: 0.4932 - val_acc: 0.7874\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s - loss: 0.4058 - acc: 0.8191 - val_loss: 0.4987 - val_acc: 0.8071\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s - loss: 0.4023 - acc: 0.8171 - val_loss: 0.5319 - val_acc: 0.7756\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s - loss: 0.4642 - acc: 0.7724 - val_loss: 0.5054 - val_acc: 0.8110\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s - loss: 0.4210 - acc: 0.7860 - val_loss: 0.4916 - val_acc: 0.7756\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s - loss: 0.4454 - acc: 0.7821 - val_loss: 0.4727 - val_acc: 0.7874\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s - loss: 0.4222 - acc: 0.7938 - val_loss: 0.4868 - val_acc: 0.7992\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s - loss: 0.4181 - acc: 0.8152 - val_loss: 0.4831 - val_acc: 0.7992\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s - loss: 0.4053 - acc: 0.8152 - val_loss: 0.5114 - val_acc: 0.7598\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s - loss: 0.4245 - acc: 0.8171 - val_loss: 0.4853 - val_acc: 0.8110\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s - loss: 0.4141 - acc: 0.8054 - val_loss: 0.4893 - val_acc: 0.8031\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s - loss: 0.4104 - acc: 0.8113 - val_loss: 0.4875 - val_acc: 0.7913\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s - loss: 0.4176 - acc: 0.7899 - val_loss: 0.5791 - val_acc: 0.7283\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s - loss: 0.4268 - acc: 0.7821 - val_loss: 0.4946 - val_acc: 0.7874\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s - loss: 0.4291 - acc: 0.7899 - val_loss: 0.5072 - val_acc: 0.7992\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s - loss: 0.4115 - acc: 0.8132 - val_loss: 0.5051 - val_acc: 0.7717\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s - loss: 0.4483 - acc: 0.8016 - val_loss: 0.5088 - val_acc: 0.7913\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s - loss: 0.4062 - acc: 0.7938 - val_loss: 0.4895 - val_acc: 0.7953\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4103 - acc: 0.802 - 0s - loss: 0.4126 - acc: 0.7996 - val_loss: 0.4977 - val_acc: 0.7874\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s - loss: 0.4283 - acc: 0.7977 - val_loss: 0.5545 - val_acc: 0.7362\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s - loss: 0.4371 - acc: 0.7957 - val_loss: 0.5510 - val_acc: 0.7480\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s - loss: 0.4253 - acc: 0.8035 - val_loss: 0.4894 - val_acc: 0.8031\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s - loss: 0.4155 - acc: 0.7938 - val_loss: 0.4986 - val_acc: 0.7953\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s - loss: 0.4255 - acc: 0.7938 - val_loss: 0.4877 - val_acc: 0.8031\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s - loss: 0.4019 - acc: 0.8132 - val_loss: 0.4957 - val_acc: 0.7953\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s - loss: 0.4090 - acc: 0.8113 - val_loss: 0.4941 - val_acc: 0.7795\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s - loss: 0.4249 - acc: 0.7879 - val_loss: 0.4993 - val_acc: 0.8071\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s - loss: 0.4188 - acc: 0.7957 - val_loss: 0.4976 - val_acc: 0.7913\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s - loss: 0.4084 - acc: 0.8113 - val_loss: 0.4974 - val_acc: 0.7874\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s - loss: 0.4067 - acc: 0.7977 - val_loss: 0.5185 - val_acc: 0.7835\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s - loss: 0.4074 - acc: 0.8035 - val_loss: 0.5023 - val_acc: 0.7835\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s - loss: 0.4414 - acc: 0.7743 - val_loss: 0.5083 - val_acc: 0.7756\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s - loss: 0.4180 - acc: 0.7860 - val_loss: 0.5538 - val_acc: 0.7638\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s - loss: 0.4166 - acc: 0.7977 - val_loss: 0.4910 - val_acc: 0.7953\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s - loss: 0.4045 - acc: 0.8132 - val_loss: 0.4948 - val_acc: 0.8031\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s - loss: 0.4221 - acc: 0.8016 - val_loss: 0.4897 - val_acc: 0.8150\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s - loss: 0.4041 - acc: 0.8132 - val_loss: 0.5010 - val_acc: 0.7992\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s - loss: 0.4036 - acc: 0.7996 - val_loss: 0.5184 - val_acc: 0.7756\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s - loss: 0.4050 - acc: 0.7957 - val_loss: 0.5061 - val_acc: 0.7953\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s - loss: 0.4137 - acc: 0.7918 - val_loss: 0.5165 - val_acc: 0.7756\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s - loss: 0.4048 - acc: 0.7918 - val_loss: 0.5043 - val_acc: 0.7992\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s - loss: 0.4318 - acc: 0.7918 - val_loss: 0.5090 - val_acc: 0.7874\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s - loss: 0.4111 - acc: 0.7957 - val_loss: 0.5077 - val_acc: 0.7835\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s - loss: 0.3914 - acc: 0.8132 - val_loss: 0.5074 - val_acc: 0.7913\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s - loss: 0.4022 - acc: 0.7957 - val_loss: 0.5201 - val_acc: 0.8031\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s - loss: 0.4063 - acc: 0.8035 - val_loss: 0.5073 - val_acc: 0.7992\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s - loss: 0.4107 - acc: 0.8074 - val_loss: 0.5150 - val_acc: 0.7835\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s - loss: 0.4163 - acc: 0.7977 - val_loss: 0.5258 - val_acc: 0.7913\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s - loss: 0.4523 - acc: 0.7860 - val_loss: 0.5095 - val_acc: 0.7835\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s - loss: 0.4141 - acc: 0.8035 - val_loss: 0.5275 - val_acc: 0.7874\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s - loss: 0.3976 - acc: 0.8230 - val_loss: 0.5124 - val_acc: 0.8031\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s - loss: 0.4172 - acc: 0.7996 - val_loss: 0.5090 - val_acc: 0.7953\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s - loss: 0.4141 - acc: 0.7996 - val_loss: 0.5133 - val_acc: 0.8031\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s - loss: 0.4296 - acc: 0.7918 - val_loss: 0.5055 - val_acc: 0.7992\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s - loss: 0.3977 - acc: 0.8210 - val_loss: 0.5029 - val_acc: 0.8031\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s - loss: 0.3988 - acc: 0.8074 - val_loss: 0.5226 - val_acc: 0.7874\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s - loss: 0.4004 - acc: 0.7996 - val_loss: 0.5249 - val_acc: 0.7913\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s - loss: 0.4119 - acc: 0.7899 - val_loss: 0.5281 - val_acc: 0.7953\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s - loss: 0.4092 - acc: 0.8113 - val_loss: 0.5363 - val_acc: 0.7835\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s - loss: 0.4181 - acc: 0.7938 - val_loss: 0.5225 - val_acc: 0.8071\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s - loss: 0.4072 - acc: 0.7996 - val_loss: 0.5477 - val_acc: 0.7835\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s - loss: 0.4196 - acc: 0.8016 - val_loss: 0.5011 - val_acc: 0.7913\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s - loss: 0.4140 - acc: 0.7938 - val_loss: 0.5176 - val_acc: 0.7795\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s - loss: 0.4065 - acc: 0.8113 - val_loss: 0.5099 - val_acc: 0.7874\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s - loss: 0.4147 - acc: 0.8054 - val_loss: 0.5425 - val_acc: 0.7677\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s - loss: 0.4127 - acc: 0.8171 - val_loss: 0.5334 - val_acc: 0.7756\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s - loss: 0.4078 - acc: 0.8152 - val_loss: 0.5237 - val_acc: 0.7598\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s - loss: 0.4023 - acc: 0.8113 - val_loss: 0.6241 - val_acc: 0.7402\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s - loss: 0.4133 - acc: 0.8132 - val_loss: 0.5171 - val_acc: 0.7992\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s - loss: 0.3964 - acc: 0.8093 - val_loss: 0.5625 - val_acc: 0.7795\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s - loss: 0.4403 - acc: 0.7977 - val_loss: 0.5474 - val_acc: 0.7717\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s - loss: 0.4050 - acc: 0.8093 - val_loss: 0.5229 - val_acc: 0.7913\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s - loss: 0.3997 - acc: 0.7938 - val_loss: 0.5213 - val_acc: 0.7913\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s - loss: 0.4002 - acc: 0.8093 - val_loss: 0.5151 - val_acc: 0.8031\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s - loss: 0.3858 - acc: 0.8113 - val_loss: 0.5289 - val_acc: 0.7874\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s - loss: 0.4070 - acc: 0.8132 - val_loss: 0.5339 - val_acc: 0.7953\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s - loss: 0.3921 - acc: 0.8132 - val_loss: 0.5340 - val_acc: 0.7795\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s - loss: 0.3971 - acc: 0.8191 - val_loss: 0.5396 - val_acc: 0.7717\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s - loss: 0.4472 - acc: 0.7840 - val_loss: 0.5691 - val_acc: 0.7598\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s - loss: 0.4126 - acc: 0.7977 - val_loss: 0.5553 - val_acc: 0.7795\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s - loss: 0.4300 - acc: 0.8152 - val_loss: 0.5326 - val_acc: 0.7913\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s - loss: 0.4230 - acc: 0.7957 - val_loss: 0.6208 - val_acc: 0.7323\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s - loss: 0.3980 - acc: 0.8093 - val_loss: 0.5182 - val_acc: 0.7913\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s - loss: 0.3933 - acc: 0.8249 - val_loss: 0.5442 - val_acc: 0.7913\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s - loss: 0.3878 - acc: 0.8132 - val_loss: 0.5693 - val_acc: 0.7756\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s - loss: 0.3948 - acc: 0.8152 - val_loss: 0.5191 - val_acc: 0.7913\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s - loss: 0.3988 - acc: 0.8268 - val_loss: 0.5200 - val_acc: 0.7795\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s - loss: 0.3937 - acc: 0.8093 - val_loss: 0.5253 - val_acc: 0.7992\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s - loss: 0.4036 - acc: 0.8171 - val_loss: 0.5310 - val_acc: 0.7717\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s - loss: 0.3972 - acc: 0.8035 - val_loss: 0.5232 - val_acc: 0.7874\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s - loss: 0.3888 - acc: 0.8249 - val_loss: 0.5287 - val_acc: 0.8071\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s - loss: 0.3956 - acc: 0.8093 - val_loss: 0.5128 - val_acc: 0.8031\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s - loss: 0.4001 - acc: 0.7977 - val_loss: 0.5164 - val_acc: 0.7992\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s - loss: 0.4120 - acc: 0.8054 - val_loss: 0.5208 - val_acc: 0.7953\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s - loss: 0.3894 - acc: 0.8054 - val_loss: 0.5252 - val_acc: 0.7992\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s - loss: 0.3946 - acc: 0.8035 - val_loss: 0.5289 - val_acc: 0.7992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121523350>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12, input_dim=8, activation= 'relu'))\n",
    "  model.add(Dense(8, activation= 'relu'))\n",
    "  model.add(Dense(1, activation= 'sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71877990488\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(8, input_dim=4, activation= 'relu'))\n",
    "  model.add(Dense(3, activation= 'softmax'))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d732750>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = sc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22811346, -0.52855966, -1.2580894 , ..., -0.17009327,\n",
       "        -0.18565155, -0.58090117],\n",
       "       [-0.83879221, -1.296537  , -0.63888294, ...,  0.15067259,\n",
       "         2.38939242, -0.75576688],\n",
       "       [ 1.60392279,  0.1472604 ,  0.08352461, ...,  0.42011591,\n",
       "        -0.54270817,  0.11856167],\n",
       "       ..., \n",
       "       [-1.14413159, -1.91091887, -0.43248078, ..., -1.33768101,\n",
       "         0.82802616,  3.00384591],\n",
       "       [-0.83879221, -0.43640238, -0.53568186, ..., -1.00408452,\n",
       "        -0.83016902, -1.01806545],\n",
       "       [ 1.29858341,  1.83681055,  0.18672569, ...,  0.07368878,\n",
       "         0.7796117 ,  0.29342738]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n",
    "model.add(Dense(3, init='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "return model\n",
    "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size=0.33, random_state=seed)\n",
    "estimator.fit(X_train, Y_train)\n",
    "predictions = estimator.predict(X_test)\n",
    "print(predictions)\n",
    "print(encoder.inverse_transform(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAD8CAYAAAABraMFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5kdETmLzIyIn/R+KxB3xQTzcVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d828e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap( 'gray' ))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap( 'gray' ))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap( 'gray' ))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer= 'normal' ,\n",
    "      activation= 'relu' ))\n",
    "  model.add(Dense(num_classes, kernel_initializer= 'normal' , activation= 'softmax' ))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "9s - loss: 0.2784 - acc: 0.9207 - val_loss: 0.1388 - val_acc: 0.9592\n",
      "Epoch 2/10\n",
      "8s - loss: 0.1093 - acc: 0.9685 - val_loss: 0.0942 - val_acc: 0.9716\n",
      "Epoch 3/10\n",
      "8s - loss: 0.0696 - acc: 0.9797 - val_loss: 0.0806 - val_acc: 0.9742\n",
      "Epoch 4/10\n",
      "8s - loss: 0.0488 - acc: 0.9864 - val_loss: 0.0678 - val_acc: 0.9794\n",
      "Epoch 5/10\n",
      "8s - loss: 0.0347 - acc: 0.9904 - val_loss: 0.0657 - val_acc: 0.9787\n",
      "Epoch 6/10\n",
      "8s - loss: 0.0264 - acc: 0.9927 - val_loss: 0.0622 - val_acc: 0.9803\n",
      "Epoch 7/10\n",
      "8s - loss: 0.0192 - acc: 0.9952 - val_loss: 0.0620 - val_acc: 0.9800\n",
      "Epoch 8/10\n",
      "9s - loss: 0.0143 - acc: 0.9964 - val_loss: 0.0577 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "10s - loss: 0.0097 - acc: 0.9982 - val_loss: 0.0585 - val_acc: 0.9815\n",
      "Epoch 10/10\n",
      "8s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.0573 - val_acc: 0.9812\n",
      "Baseline Error: 1.88%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation= 'relu' ))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation= 'relu' ))\n",
    "  model.add(Dense(num_classes, activation= 'softmax' ))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "199s - loss: 0.2459 - acc: 0.9298 - val_loss: 0.0730 - val_acc: 0.9783\n",
      "Epoch 2/10\n",
      "192s - loss: 0.0740 - acc: 0.9783 - val_loss: 0.0488 - val_acc: 0.9853\n",
      "Epoch 3/10\n",
      "193s - loss: 0.0529 - acc: 0.9844 - val_loss: 0.0396 - val_acc: 0.9872\n",
      "Epoch 4/10\n",
      "190s - loss: 0.0404 - acc: 0.9877 - val_loss: 0.0381 - val_acc: 0.9877\n",
      "Epoch 5/10\n",
      "191s - loss: 0.0338 - acc: 0.9895 - val_loss: 0.0453 - val_acc: 0.9863\n",
      "Epoch 6/10\n",
      "192s - loss: 0.0278 - acc: 0.9914 - val_loss: 0.0359 - val_acc: 0.9879\n",
      "Epoch 7/10\n",
      "183s - loss: 0.0232 - acc: 0.9922 - val_loss: 0.0346 - val_acc: 0.9882\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d92e94033226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n\u001b[0;32m----> 5\u001b[0;31m     verbose=2)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncBLohRBihZNAL4QQK9z/Dxb9B3vY4hpdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126444e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "dataset = read_csv('international-airline-passengers.csv', usecols=[1], engine= 'python',\n",
    "    skipfooter=3)\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine= 'python' ,\n",
    "    skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 48)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "  dataX, dataY = [], []\n",
    "  for i in range(len(dataset)-look_back-1):\n",
    "    a = dataset[i:(i+look_back), 0]\n",
    "    dataX.append(a)\n",
    "    dataY.append(dataset[i + look_back, 0])\n",
    "  return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "0s - loss: 92581.6739\n",
      "Epoch 2/200\n",
      "0s - loss: 82321.5508\n",
      "Epoch 3/200\n",
      "0s - loss: 73059.0133\n",
      "Epoch 4/200\n",
      "0s - loss: 64222.7209\n",
      "Epoch 5/200\n",
      "0s - loss: 56128.8210\n",
      "Epoch 6/200\n",
      "0s - loss: 48408.1763\n",
      "Epoch 7/200\n",
      "0s - loss: 41263.1472\n",
      "Epoch 8/200\n",
      "0s - loss: 34639.6312\n",
      "Epoch 9/200\n",
      "0s - loss: 28653.4593\n",
      "Epoch 10/200\n",
      "0s - loss: 23257.5762\n",
      "Epoch 11/200\n",
      "0s - loss: 18502.4420\n",
      "Epoch 12/200\n",
      "0s - loss: 14398.1293\n",
      "Epoch 13/200\n",
      "0s - loss: 10977.2336\n",
      "Epoch 14/200\n",
      "0s - loss: 8188.5292\n",
      "Epoch 15/200\n",
      "0s - loss: 5961.3480\n",
      "Epoch 16/200\n",
      "0s - loss: 4267.6629\n",
      "Epoch 17/200\n",
      "0s - loss: 3028.5164\n",
      "Epoch 18/200\n",
      "0s - loss: 2144.4038\n",
      "Epoch 19/200\n",
      "0s - loss: 1537.1162\n",
      "Epoch 20/200\n",
      "0s - loss: 1139.7383\n",
      "Epoch 21/200\n",
      "0s - loss: 888.0275\n",
      "Epoch 22/200\n",
      "0s - loss: 735.2228\n",
      "Epoch 23/200\n",
      "0s - loss: 643.5322\n",
      "Epoch 24/200\n",
      "0s - loss: 591.9308\n",
      "Epoch 25/200\n",
      "0s - loss: 564.4277\n",
      "Epoch 26/200\n",
      "0s - loss: 548.3208\n",
      "Epoch 27/200\n",
      "0s - loss: 540.8447\n",
      "Epoch 28/200\n",
      "0s - loss: 537.3168\n",
      "Epoch 29/200\n",
      "0s - loss: 536.0485\n",
      "Epoch 30/200\n",
      "0s - loss: 534.6103\n",
      "Epoch 31/200\n",
      "0s - loss: 535.1876\n",
      "Epoch 32/200\n",
      "0s - loss: 534.3366\n",
      "Epoch 33/200\n",
      "0s - loss: 534.7765\n",
      "Epoch 34/200\n",
      "0s - loss: 536.0788\n",
      "Epoch 35/200\n",
      "0s - loss: 534.1053\n",
      "Epoch 36/200\n",
      "0s - loss: 539.3809\n",
      "Epoch 37/200\n",
      "0s - loss: 534.9179\n",
      "Epoch 38/200\n",
      "0s - loss: 534.8226\n",
      "Epoch 39/200\n",
      "0s - loss: 537.7279\n",
      "Epoch 40/200\n",
      "0s - loss: 534.6133\n",
      "Epoch 41/200\n",
      "0s - loss: 534.1406\n",
      "Epoch 42/200\n",
      "0s - loss: 534.3035\n",
      "Epoch 43/200\n",
      "0s - loss: 534.6000\n",
      "Epoch 44/200\n",
      "0s - loss: 534.2101\n",
      "Epoch 45/200\n",
      "0s - loss: 536.1745\n",
      "Epoch 46/200\n",
      "0s - loss: 537.2013\n",
      "Epoch 47/200\n",
      "0s - loss: 535.2894\n",
      "Epoch 48/200\n",
      "0s - loss: 535.3585\n",
      "Epoch 49/200\n",
      "0s - loss: 533.9480\n",
      "Epoch 50/200\n",
      "0s - loss: 533.9156\n",
      "Epoch 51/200\n",
      "0s - loss: 536.7005\n",
      "Epoch 52/200\n",
      "0s - loss: 537.5538\n",
      "Epoch 53/200\n",
      "0s - loss: 538.5220\n",
      "Epoch 54/200\n",
      "0s - loss: 538.2271\n",
      "Epoch 55/200\n",
      "0s - loss: 535.4472\n",
      "Epoch 56/200\n",
      "0s - loss: 539.0042\n",
      "Epoch 57/200\n",
      "0s - loss: 538.5405\n",
      "Epoch 58/200\n",
      "0s - loss: 535.1514\n",
      "Epoch 59/200\n",
      "0s - loss: 535.0420\n",
      "Epoch 60/200\n",
      "0s - loss: 535.5001\n",
      "Epoch 61/200\n",
      "0s - loss: 539.6180\n",
      "Epoch 62/200\n",
      "0s - loss: 536.6881\n",
      "Epoch 63/200\n",
      "0s - loss: 534.7501\n",
      "Epoch 64/200\n",
      "0s - loss: 537.2557\n",
      "Epoch 65/200\n",
      "0s - loss: 540.5562\n",
      "Epoch 66/200\n",
      "0s - loss: 536.0397\n",
      "Epoch 67/200\n",
      "0s - loss: 536.3118\n",
      "Epoch 68/200\n",
      "0s - loss: 535.4272\n",
      "Epoch 69/200\n",
      "0s - loss: 535.0454\n",
      "Epoch 70/200\n",
      "0s - loss: 534.4448\n",
      "Epoch 71/200\n",
      "0s - loss: 535.7695\n",
      "Epoch 72/200\n",
      "0s - loss: 538.8038\n",
      "Epoch 73/200\n",
      "0s - loss: 540.4692\n",
      "Epoch 74/200\n",
      "0s - loss: 538.3470\n",
      "Epoch 75/200\n",
      "0s - loss: 534.8850\n",
      "Epoch 76/200\n",
      "0s - loss: 535.8626\n",
      "Epoch 77/200\n",
      "0s - loss: 543.0662\n",
      "Epoch 78/200\n",
      "0s - loss: 535.6450\n",
      "Epoch 79/200\n",
      "0s - loss: 536.8446\n",
      "Epoch 80/200\n",
      "0s - loss: 539.9595\n",
      "Epoch 81/200\n",
      "0s - loss: 544.0090\n",
      "Epoch 82/200\n",
      "0s - loss: 536.7851\n",
      "Epoch 83/200\n",
      "0s - loss: 536.8176\n",
      "Epoch 84/200\n",
      "0s - loss: 532.7788\n",
      "Epoch 85/200\n",
      "0s - loss: 535.5699\n",
      "Epoch 86/200\n",
      "0s - loss: 535.4430\n",
      "Epoch 87/200\n",
      "0s - loss: 535.3915\n",
      "Epoch 88/200\n",
      "0s - loss: 535.9322\n",
      "Epoch 89/200\n",
      "0s - loss: 535.7544\n",
      "Epoch 90/200\n",
      "0s - loss: 539.0598\n",
      "Epoch 91/200\n",
      "0s - loss: 540.0396\n",
      "Epoch 92/200\n",
      "0s - loss: 543.4268\n",
      "Epoch 93/200\n",
      "0s - loss: 543.5069\n",
      "Epoch 94/200\n",
      "0s - loss: 536.4345\n",
      "Epoch 95/200\n",
      "0s - loss: 536.6319\n",
      "Epoch 96/200\n",
      "0s - loss: 536.5041\n",
      "Epoch 97/200\n",
      "0s - loss: 534.7111\n",
      "Epoch 98/200\n",
      "0s - loss: 540.7586\n",
      "Epoch 99/200\n",
      "0s - loss: 535.8238\n",
      "Epoch 100/200\n",
      "0s - loss: 537.1280\n",
      "Epoch 101/200\n",
      "0s - loss: 536.5291\n",
      "Epoch 102/200\n",
      "0s - loss: 537.6265\n",
      "Epoch 103/200\n",
      "0s - loss: 538.2896\n",
      "Epoch 104/200\n",
      "0s - loss: 535.3372\n",
      "Epoch 105/200\n",
      "0s - loss: 538.6416\n",
      "Epoch 106/200\n",
      "0s - loss: 540.9589\n",
      "Epoch 107/200\n",
      "0s - loss: 537.9348\n",
      "Epoch 108/200\n",
      "0s - loss: 535.2662\n",
      "Epoch 109/200\n",
      "0s - loss: 541.0425\n",
      "Epoch 110/200\n",
      "0s - loss: 540.0590\n",
      "Epoch 111/200\n",
      "0s - loss: 538.8979\n",
      "Epoch 112/200\n",
      "0s - loss: 539.5079\n",
      "Epoch 113/200\n",
      "0s - loss: 545.7537\n",
      "Epoch 114/200\n",
      "0s - loss: 534.7136\n",
      "Epoch 115/200\n",
      "0s - loss: 539.3290\n",
      "Epoch 116/200\n",
      "0s - loss: 534.1153\n",
      "Epoch 117/200\n",
      "0s - loss: 536.1356\n",
      "Epoch 118/200\n",
      "0s - loss: 540.0176\n",
      "Epoch 119/200\n",
      "0s - loss: 536.0405\n",
      "Epoch 120/200\n",
      "0s - loss: 535.9293\n",
      "Epoch 121/200\n",
      "0s - loss: 537.3699\n",
      "Epoch 122/200\n",
      "0s - loss: 546.0588\n",
      "Epoch 123/200\n",
      "0s - loss: 540.1845\n",
      "Epoch 124/200\n",
      "0s - loss: 535.1129\n",
      "Epoch 125/200\n",
      "0s - loss: 548.6649\n",
      "Epoch 126/200\n",
      "0s - loss: 540.9149\n",
      "Epoch 127/200\n",
      "0s - loss: 537.1888\n",
      "Epoch 128/200\n",
      "0s - loss: 539.2177\n",
      "Epoch 129/200\n",
      "0s - loss: 536.5729\n",
      "Epoch 130/200\n",
      "0s - loss: 537.3755\n",
      "Epoch 131/200\n",
      "0s - loss: 542.7411\n",
      "Epoch 132/200\n",
      "0s - loss: 535.7362\n",
      "Epoch 133/200\n",
      "0s - loss: 537.1458\n",
      "Epoch 134/200\n",
      "0s - loss: 535.2168\n",
      "Epoch 135/200\n",
      "0s - loss: 546.7315\n",
      "Epoch 136/200\n",
      "0s - loss: 536.0883\n",
      "Epoch 137/200\n",
      "0s - loss: 539.6497\n",
      "Epoch 138/200\n",
      "0s - loss: 539.6522\n",
      "Epoch 139/200\n",
      "0s - loss: 540.6604\n",
      "Epoch 140/200\n",
      "0s - loss: 538.2159\n",
      "Epoch 141/200\n",
      "0s - loss: 542.6616\n",
      "Epoch 142/200\n",
      "0s - loss: 536.7095\n",
      "Epoch 143/200\n",
      "0s - loss: 542.0112\n",
      "Epoch 144/200\n",
      "0s - loss: 533.5962\n",
      "Epoch 145/200\n",
      "0s - loss: 543.7719\n",
      "Epoch 146/200\n",
      "0s - loss: 554.2777\n",
      "Epoch 147/200\n",
      "0s - loss: 538.9875\n",
      "Epoch 148/200\n",
      "0s - loss: 539.9244\n",
      "Epoch 149/200\n",
      "0s - loss: 535.1385\n",
      "Epoch 150/200\n",
      "0s - loss: 534.7162\n",
      "Epoch 151/200\n",
      "0s - loss: 539.8655\n",
      "Epoch 152/200\n",
      "0s - loss: 538.1542\n",
      "Epoch 153/200\n",
      "0s - loss: 537.5269\n",
      "Epoch 154/200\n",
      "0s - loss: 535.4577\n",
      "Epoch 155/200\n",
      "0s - loss: 535.2143\n",
      "Epoch 156/200\n",
      "0s - loss: 537.4044\n",
      "Epoch 157/200\n",
      "0s - loss: 536.4398\n",
      "Epoch 158/200\n",
      "0s - loss: 534.2143\n",
      "Epoch 159/200\n",
      "0s - loss: 535.6826\n",
      "Epoch 160/200\n",
      "0s - loss: 542.1145\n",
      "Epoch 161/200\n",
      "0s - loss: 545.9456\n",
      "Epoch 162/200\n",
      "0s - loss: 551.2986\n",
      "Epoch 163/200\n",
      "0s - loss: 546.2685\n",
      "Epoch 164/200\n",
      "0s - loss: 542.6158\n",
      "Epoch 165/200\n",
      "0s - loss: 535.0669\n",
      "Epoch 166/200\n",
      "0s - loss: 540.1721\n",
      "Epoch 167/200\n",
      "0s - loss: 541.5062\n",
      "Epoch 168/200\n",
      "0s - loss: 537.4967\n",
      "Epoch 169/200\n",
      "0s - loss: 540.9484\n",
      "Epoch 170/200\n",
      "0s - loss: 544.8755\n",
      "Epoch 171/200\n",
      "0s - loss: 543.3360\n",
      "Epoch 172/200\n",
      "0s - loss: 533.7061\n",
      "Epoch 173/200\n",
      "0s - loss: 542.3592\n",
      "Epoch 174/200\n",
      "0s - loss: 546.6837\n",
      "Epoch 175/200\n",
      "0s - loss: 542.0006\n",
      "Epoch 176/200\n",
      "0s - loss: 535.5930\n",
      "Epoch 177/200\n",
      "0s - loss: 537.0014\n",
      "Epoch 178/200\n",
      "0s - loss: 533.2089\n",
      "Epoch 179/200\n",
      "0s - loss: 540.8105\n",
      "Epoch 180/200\n",
      "0s - loss: 545.3699\n",
      "Epoch 181/200\n",
      "0s - loss: 536.7351\n",
      "Epoch 182/200\n",
      "0s - loss: 533.3866\n",
      "Epoch 183/200\n",
      "0s - loss: 534.4183\n",
      "Epoch 184/200\n",
      "0s - loss: 536.8830\n",
      "Epoch 185/200\n",
      "0s - loss: 540.8524\n",
      "Epoch 186/200\n",
      "0s - loss: 538.8979\n",
      "Epoch 187/200\n",
      "0s - loss: 534.7749\n",
      "Epoch 188/200\n",
      "0s - loss: 540.1595\n",
      "Epoch 189/200\n",
      "0s - loss: 534.6213\n",
      "Epoch 190/200\n",
      "0s - loss: 549.6605\n",
      "Epoch 191/200\n",
      "0s - loss: 535.5397\n",
      "Epoch 192/200\n",
      "0s - loss: 535.2846\n",
      "Epoch 193/200\n",
      "0s - loss: 539.9133\n",
      "Epoch 194/200\n",
      "0s - loss: 543.1889\n",
      "Epoch 195/200\n",
      "0s - loss: 537.9320\n",
      "Epoch 196/200\n",
      "0s - loss: 544.1280\n",
      "Epoch 197/200\n",
      "0s - loss: 534.3086\n",
      "Epoch 198/200\n",
      "0s - loss: 537.3473\n",
      "Epoch 199/200\n",
      "0s - loss: 539.6234\n",
      "Epoch 200/200\n",
      "0s - loss: 534.4758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1246f1750>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit Multilayer Perceptron model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=look_back, activation= 'relu' ))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss= 'mean_squared_error' , optimizer= 'adam' )\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 530.91 MSE (23.04 RMSE)\n",
      "Test Score: 2352.32 MSE (48.50 RMSE)\n"
     ]
    }
   ],
   "source": [
    "# Estimate model performance\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)'  % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)'  % (testScore, math.sqrt(testScore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1246fe790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape dataset\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "0s - loss: 98930.1662\n",
      "Epoch 2/400\n",
      "0s - loss: 45235.3739\n",
      "Epoch 3/400\n",
      "0s - loss: 28850.0739\n",
      "Epoch 4/400\n",
      "0s - loss: 19613.5814\n",
      "Epoch 5/400\n",
      "0s - loss: 10791.5157\n",
      "Epoch 6/400\n",
      "0s - loss: 4703.0042\n",
      "Epoch 7/400\n",
      "0s - loss: 1940.9627\n",
      "Epoch 8/400\n",
      "0s - loss: 1228.7566\n",
      "Epoch 9/400\n",
      "0s - loss: 1156.3631\n",
      "Epoch 10/400\n",
      "0s - loss: 1144.0372\n",
      "Epoch 11/400\n",
      "0s - loss: 1136.2854\n",
      "Epoch 12/400\n",
      "0s - loss: 1144.7673\n",
      "Epoch 13/400\n",
      "0s - loss: 1130.0957\n",
      "Epoch 14/400\n",
      "0s - loss: 1140.9707\n",
      "Epoch 15/400\n",
      "0s - loss: 1152.6303\n",
      "Epoch 16/400\n",
      "0s - loss: 1130.9295\n",
      "Epoch 17/400\n",
      "0s - loss: 1129.5859\n",
      "Epoch 18/400\n",
      "0s - loss: 1121.4159\n",
      "Epoch 19/400\n",
      "0s - loss: 1131.2908\n",
      "Epoch 20/400\n",
      "0s - loss: 1124.5176\n",
      "Epoch 21/400\n",
      "0s - loss: 1131.0620\n",
      "Epoch 22/400\n",
      "0s - loss: 1115.1027\n",
      "Epoch 23/400\n",
      "0s - loss: 1118.9219\n",
      "Epoch 24/400\n",
      "0s - loss: 1112.8587\n",
      "Epoch 25/400\n",
      "0s - loss: 1118.3855\n",
      "Epoch 26/400\n",
      "0s - loss: 1101.6554\n",
      "Epoch 27/400\n",
      "0s - loss: 1112.8881\n",
      "Epoch 28/400\n",
      "0s - loss: 1097.3741\n",
      "Epoch 29/400\n",
      "0s - loss: 1115.7363\n",
      "Epoch 30/400\n",
      "0s - loss: 1107.4064\n",
      "Epoch 31/400\n",
      "0s - loss: 1089.1055\n",
      "Epoch 32/400\n",
      "0s - loss: 1084.5087\n",
      "Epoch 33/400\n",
      "0s - loss: 1099.1955\n",
      "Epoch 34/400\n",
      "0s - loss: 1124.0705\n",
      "Epoch 35/400\n",
      "0s - loss: 1117.7550\n",
      "Epoch 36/400\n",
      "0s - loss: 1073.7273\n",
      "Epoch 37/400\n",
      "0s - loss: 1087.4021\n",
      "Epoch 38/400\n",
      "0s - loss: 1078.0311\n",
      "Epoch 39/400\n",
      "0s - loss: 1067.5423\n",
      "Epoch 40/400\n",
      "0s - loss: 1084.4868\n",
      "Epoch 41/400\n",
      "0s - loss: 1074.0650\n",
      "Epoch 42/400\n",
      "0s - loss: 1073.7817\n",
      "Epoch 43/400\n",
      "0s - loss: 1056.9209\n",
      "Epoch 44/400\n",
      "0s - loss: 1063.2956\n",
      "Epoch 45/400\n",
      "0s - loss: 1070.8204\n",
      "Epoch 46/400\n",
      "0s - loss: 1057.1562\n",
      "Epoch 47/400\n",
      "0s - loss: 1051.2669\n",
      "Epoch 48/400\n",
      "0s - loss: 1043.4526\n",
      "Epoch 49/400\n",
      "0s - loss: 1041.0333\n",
      "Epoch 50/400\n",
      "0s - loss: 1045.0394\n",
      "Epoch 51/400\n",
      "0s - loss: 1047.6674\n",
      "Epoch 52/400\n",
      "0s - loss: 1041.8978\n",
      "Epoch 53/400\n",
      "0s - loss: 1049.1728\n",
      "Epoch 54/400\n",
      "0s - loss: 1025.1089\n",
      "Epoch 55/400\n",
      "0s - loss: 1022.8654\n",
      "Epoch 56/400\n",
      "0s - loss: 1031.9257\n",
      "Epoch 57/400\n",
      "0s - loss: 1017.6364\n",
      "Epoch 58/400\n",
      "0s - loss: 1006.8460\n",
      "Epoch 59/400\n",
      "0s - loss: 1014.8659\n",
      "Epoch 60/400\n",
      "0s - loss: 999.6557\n",
      "Epoch 61/400\n",
      "0s - loss: 1039.0484\n",
      "Epoch 62/400\n",
      "0s - loss: 1007.8187\n",
      "Epoch 63/400\n",
      "0s - loss: 1024.1246\n",
      "Epoch 64/400\n",
      "0s - loss: 1039.6786\n",
      "Epoch 65/400\n",
      "0s - loss: 985.6485\n",
      "Epoch 66/400\n",
      "0s - loss: 986.9768\n",
      "Epoch 67/400\n",
      "0s - loss: 975.9887\n",
      "Epoch 68/400\n",
      "0s - loss: 971.9680\n",
      "Epoch 69/400\n",
      "0s - loss: 993.2985\n",
      "Epoch 70/400\n",
      "0s - loss: 971.0251\n",
      "Epoch 71/400\n",
      "0s - loss: 953.2932\n",
      "Epoch 72/400\n",
      "0s - loss: 972.2181\n",
      "Epoch 73/400\n",
      "0s - loss: 964.2143\n",
      "Epoch 74/400\n",
      "0s - loss: 958.3161\n",
      "Epoch 75/400\n",
      "0s - loss: 940.8216\n",
      "Epoch 76/400\n",
      "0s - loss: 942.2573\n",
      "Epoch 77/400\n",
      "0s - loss: 959.7577\n",
      "Epoch 78/400\n",
      "0s - loss: 936.7550\n",
      "Epoch 79/400\n",
      "0s - loss: 938.9259\n",
      "Epoch 80/400\n",
      "0s - loss: 930.8052\n",
      "Epoch 81/400\n",
      "0s - loss: 931.4827\n",
      "Epoch 82/400\n",
      "0s - loss: 924.5866\n",
      "Epoch 83/400\n",
      "0s - loss: 970.4777\n",
      "Epoch 84/400\n",
      "0s - loss: 918.2662\n",
      "Epoch 85/400\n",
      "0s - loss: 925.0426\n",
      "Epoch 86/400\n",
      "0s - loss: 896.3579\n",
      "Epoch 87/400\n",
      "0s - loss: 908.8667\n",
      "Epoch 88/400\n",
      "0s - loss: 889.7707\n",
      "Epoch 89/400\n",
      "0s - loss: 908.0579\n",
      "Epoch 90/400\n",
      "0s - loss: 913.8138\n",
      "Epoch 91/400\n",
      "0s - loss: 926.3636\n",
      "Epoch 92/400\n",
      "0s - loss: 891.2584\n",
      "Epoch 93/400\n",
      "0s - loss: 876.0880\n",
      "Epoch 94/400\n",
      "0s - loss: 881.7246\n",
      "Epoch 95/400\n",
      "0s - loss: 879.3930\n",
      "Epoch 96/400\n",
      "0s - loss: 879.3223\n",
      "Epoch 97/400\n",
      "0s - loss: 882.7085\n",
      "Epoch 98/400\n",
      "0s - loss: 854.0528\n",
      "Epoch 99/400\n",
      "0s - loss: 860.9235\n",
      "Epoch 100/400\n",
      "0s - loss: 880.5160\n",
      "Epoch 101/400\n",
      "0s - loss: 865.6202\n",
      "Epoch 102/400\n",
      "0s - loss: 862.4935\n",
      "Epoch 103/400\n",
      "0s - loss: 848.4045\n",
      "Epoch 104/400\n",
      "0s - loss: 839.7691\n",
      "Epoch 105/400\n",
      "0s - loss: 848.6856\n",
      "Epoch 106/400\n",
      "0s - loss: 863.5470\n",
      "Epoch 107/400\n",
      "0s - loss: 858.0732\n",
      "Epoch 108/400\n",
      "0s - loss: 875.5786\n",
      "Epoch 109/400\n",
      "0s - loss: 832.0030\n",
      "Epoch 110/400\n",
      "0s - loss: 828.9590\n",
      "Epoch 111/400\n",
      "0s - loss: 849.3092\n",
      "Epoch 112/400\n",
      "0s - loss: 825.0339\n",
      "Epoch 113/400\n",
      "0s - loss: 846.7010\n",
      "Epoch 114/400\n",
      "0s - loss: 855.6397\n",
      "Epoch 115/400\n",
      "0s - loss: 858.6180\n",
      "Epoch 116/400\n",
      "0s - loss: 828.0410\n",
      "Epoch 117/400\n",
      "0s - loss: 800.1594\n",
      "Epoch 118/400\n",
      "0s - loss: 794.4915\n",
      "Epoch 119/400\n",
      "0s - loss: 792.6855\n",
      "Epoch 120/400\n",
      "0s - loss: 813.1656\n",
      "Epoch 121/400\n",
      "0s - loss: 831.1468\n",
      "Epoch 122/400\n",
      "0s - loss: 790.4715\n",
      "Epoch 123/400\n",
      "0s - loss: 801.1206\n",
      "Epoch 124/400\n",
      "0s - loss: 789.3920\n",
      "Epoch 125/400\n",
      "0s - loss: 781.5170\n",
      "Epoch 126/400\n",
      "0s - loss: 784.1463\n",
      "Epoch 127/400\n",
      "0s - loss: 812.7454\n",
      "Epoch 128/400\n",
      "0s - loss: 760.8696\n",
      "Epoch 129/400\n",
      "0s - loss: 808.2625\n",
      "Epoch 130/400\n",
      "0s - loss: 795.0787\n",
      "Epoch 131/400\n",
      "0s - loss: 809.1310\n",
      "Epoch 132/400\n",
      "0s - loss: 774.5888\n",
      "Epoch 133/400\n",
      "0s - loss: 752.3611\n",
      "Epoch 134/400\n",
      "0s - loss: 787.3535\n",
      "Epoch 135/400\n",
      "0s - loss: 777.2396\n",
      "Epoch 136/400\n",
      "0s - loss: 768.6177\n",
      "Epoch 137/400\n",
      "0s - loss: 772.7747\n",
      "Epoch 138/400\n",
      "0s - loss: 754.1093\n",
      "Epoch 139/400\n",
      "0s - loss: 754.5795\n",
      "Epoch 140/400\n",
      "0s - loss: 740.0220\n",
      "Epoch 141/400\n",
      "0s - loss: 741.7971\n",
      "Epoch 142/400\n",
      "0s - loss: 756.5479\n",
      "Epoch 143/400\n",
      "0s - loss: 789.8993\n",
      "Epoch 144/400\n",
      "0s - loss: 728.5487\n",
      "Epoch 145/400\n",
      "0s - loss: 741.1904\n",
      "Epoch 146/400\n",
      "0s - loss: 737.9997\n",
      "Epoch 147/400\n",
      "0s - loss: 743.1095\n",
      "Epoch 148/400\n",
      "0s - loss: 725.8337\n",
      "Epoch 149/400\n",
      "0s - loss: 733.7236\n",
      "Epoch 150/400\n",
      "0s - loss: 750.2367\n",
      "Epoch 151/400\n",
      "0s - loss: 731.8484\n",
      "Epoch 152/400\n",
      "0s - loss: 770.1753\n",
      "Epoch 153/400\n",
      "0s - loss: 712.7778\n",
      "Epoch 154/400\n",
      "0s - loss: 724.4082\n",
      "Epoch 155/400\n",
      "0s - loss: 719.4859\n",
      "Epoch 156/400\n",
      "0s - loss: 713.0871\n",
      "Epoch 157/400\n",
      "0s - loss: 705.7431\n",
      "Epoch 158/400\n",
      "0s - loss: 705.9327\n",
      "Epoch 159/400\n",
      "0s - loss: 715.3046\n",
      "Epoch 160/400\n",
      "0s - loss: 710.2953\n",
      "Epoch 161/400\n",
      "0s - loss: 696.6910\n",
      "Epoch 162/400\n",
      "0s - loss: 742.8635\n",
      "Epoch 163/400\n",
      "0s - loss: 689.3753\n",
      "Epoch 164/400\n",
      "0s - loss: 704.4961\n",
      "Epoch 165/400\n",
      "0s - loss: 693.9693\n",
      "Epoch 166/400\n",
      "0s - loss: 707.3448\n",
      "Epoch 167/400\n",
      "0s - loss: 790.9838\n",
      "Epoch 168/400\n",
      "0s - loss: 688.1489\n",
      "Epoch 169/400\n",
      "0s - loss: 676.9242\n",
      "Epoch 170/400\n",
      "0s - loss: 724.4889\n",
      "Epoch 171/400\n",
      "0s - loss: 683.0126\n",
      "Epoch 172/400\n",
      "0s - loss: 700.9737\n",
      "Epoch 173/400\n",
      "0s - loss: 681.8472\n",
      "Epoch 174/400\n",
      "0s - loss: 709.4585\n",
      "Epoch 175/400\n",
      "0s - loss: 688.3021\n",
      "Epoch 176/400\n",
      "0s - loss: 693.0306\n",
      "Epoch 177/400\n",
      "0s - loss: 685.1617\n",
      "Epoch 178/400\n",
      "0s - loss: 721.2378\n",
      "Epoch 179/400\n",
      "0s - loss: 658.5784\n",
      "Epoch 180/400\n",
      "0s - loss: 692.0861\n",
      "Epoch 181/400\n",
      "0s - loss: 674.5454\n",
      "Epoch 182/400\n",
      "0s - loss: 661.0934\n",
      "Epoch 183/400\n",
      "0s - loss: 676.1850\n",
      "Epoch 184/400\n",
      "0s - loss: 675.9435\n",
      "Epoch 185/400\n",
      "0s - loss: 669.2577\n",
      "Epoch 186/400\n",
      "0s - loss: 686.1695\n",
      "Epoch 187/400\n",
      "0s - loss: 675.8090\n",
      "Epoch 188/400\n",
      "0s - loss: 664.1304\n",
      "Epoch 189/400\n",
      "0s - loss: 677.9126\n",
      "Epoch 190/400\n",
      "0s - loss: 666.7008\n",
      "Epoch 191/400\n",
      "0s - loss: 669.5390\n",
      "Epoch 192/400\n",
      "0s - loss: 651.4427\n",
      "Epoch 193/400\n",
      "0s - loss: 718.1370\n",
      "Epoch 194/400\n",
      "0s - loss: 673.0747\n",
      "Epoch 195/400\n",
      "0s - loss: 684.1760\n",
      "Epoch 196/400\n",
      "0s - loss: 639.3956\n",
      "Epoch 197/400\n",
      "0s - loss: 678.0478\n",
      "Epoch 198/400\n",
      "0s - loss: 670.5481\n",
      "Epoch 199/400\n",
      "0s - loss: 650.4365\n",
      "Epoch 200/400\n",
      "0s - loss: 639.4540\n",
      "Epoch 201/400\n",
      "0s - loss: 659.4376\n",
      "Epoch 202/400\n",
      "0s - loss: 682.2646\n",
      "Epoch 203/400\n",
      "0s - loss: 662.5867\n",
      "Epoch 204/400\n",
      "0s - loss: 638.5617\n",
      "Epoch 205/400\n",
      "0s - loss: 651.9543\n",
      "Epoch 206/400\n",
      "0s - loss: 677.7007\n",
      "Epoch 207/400\n",
      "0s - loss: 659.7551\n",
      "Epoch 208/400\n",
      "0s - loss: 667.0883\n",
      "Epoch 209/400\n",
      "0s - loss: 647.8519\n",
      "Epoch 210/400\n",
      "0s - loss: 660.3301\n",
      "Epoch 211/400\n",
      "0s - loss: 632.0853\n",
      "Epoch 212/400\n",
      "0s - loss: 622.8620\n",
      "Epoch 213/400\n",
      "0s - loss: 631.3496\n",
      "Epoch 214/400\n",
      "0s - loss: 676.3686\n",
      "Epoch 215/400\n",
      "0s - loss: 633.3498\n",
      "Epoch 216/400\n",
      "0s - loss: 664.6010\n",
      "Epoch 217/400\n",
      "0s - loss: 651.5473\n",
      "Epoch 218/400\n",
      "0s - loss: 640.8979\n",
      "Epoch 219/400\n",
      "0s - loss: 627.7664\n",
      "Epoch 220/400\n",
      "0s - loss: 656.4952\n",
      "Epoch 221/400\n",
      "0s - loss: 670.0490\n",
      "Epoch 222/400\n",
      "0s - loss: 631.3355\n",
      "Epoch 223/400\n",
      "0s - loss: 642.3777\n",
      "Epoch 224/400\n",
      "0s - loss: 624.3846\n",
      "Epoch 225/400\n",
      "0s - loss: 621.0010\n",
      "Epoch 226/400\n",
      "0s - loss: 641.5603\n",
      "Epoch 227/400\n",
      "0s - loss: 647.0841\n",
      "Epoch 228/400\n",
      "0s - loss: 633.6303\n",
      "Epoch 229/400\n",
      "0s - loss: 613.4334\n",
      "Epoch 230/400\n",
      "0s - loss: 633.0264\n",
      "Epoch 231/400\n",
      "0s - loss: 634.6361\n",
      "Epoch 232/400\n",
      "0s - loss: 659.9805\n",
      "Epoch 233/400\n",
      "0s - loss: 619.5417\n",
      "Epoch 234/400\n",
      "0s - loss: 662.4938\n",
      "Epoch 235/400\n",
      "0s - loss: 616.8961\n",
      "Epoch 236/400\n",
      "0s - loss: 613.8063\n",
      "Epoch 237/400\n",
      "0s - loss: 618.7228\n",
      "Epoch 238/400\n",
      "0s - loss: 617.7964\n",
      "Epoch 239/400\n",
      "0s - loss: 614.3824\n",
      "Epoch 240/400\n",
      "0s - loss: 620.7223\n",
      "Epoch 241/400\n",
      "0s - loss: 632.3365\n",
      "Epoch 242/400\n",
      "0s - loss: 609.4610\n",
      "Epoch 243/400\n",
      "0s - loss: 623.6626\n",
      "Epoch 244/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 623.6476\n",
      "Epoch 245/400\n",
      "0s - loss: 602.5566\n",
      "Epoch 246/400\n",
      "0s - loss: 635.7544\n",
      "Epoch 247/400\n",
      "0s - loss: 637.2092\n",
      "Epoch 248/400\n",
      "0s - loss: 607.5022\n",
      "Epoch 249/400\n",
      "0s - loss: 611.0802\n",
      "Epoch 250/400\n",
      "0s - loss: 632.8874\n",
      "Epoch 251/400\n",
      "0s - loss: 606.4773\n",
      "Epoch 252/400\n",
      "0s - loss: 626.2330\n",
      "Epoch 253/400\n",
      "0s - loss: 611.8533\n",
      "Epoch 254/400\n",
      "0s - loss: 651.3563\n",
      "Epoch 255/400\n",
      "0s - loss: 630.3177\n",
      "Epoch 256/400\n",
      "0s - loss: 600.2970\n",
      "Epoch 257/400\n",
      "0s - loss: 613.6979\n",
      "Epoch 258/400\n",
      "0s - loss: 594.7935\n",
      "Epoch 259/400\n",
      "0s - loss: 591.9792\n",
      "Epoch 260/400\n",
      "0s - loss: 589.7648\n",
      "Epoch 261/400\n",
      "0s - loss: 592.3145\n",
      "Epoch 262/400\n",
      "0s - loss: 611.8424\n",
      "Epoch 263/400\n",
      "0s - loss: 593.7726\n",
      "Epoch 264/400\n",
      "0s - loss: 595.8936\n",
      "Epoch 265/400\n",
      "0s - loss: 612.8492\n",
      "Epoch 266/400\n",
      "0s - loss: 571.6748\n",
      "Epoch 267/400\n",
      "0s - loss: 611.6982\n",
      "Epoch 268/400\n",
      "0s - loss: 614.2226\n",
      "Epoch 269/400\n",
      "0s - loss: 630.9865\n",
      "Epoch 270/400\n",
      "0s - loss: 621.7188\n",
      "Epoch 271/400\n",
      "0s - loss: 604.6743\n",
      "Epoch 272/400\n",
      "0s - loss: 609.8573\n",
      "Epoch 273/400\n",
      "0s - loss: 596.3098\n",
      "Epoch 274/400\n",
      "0s - loss: 600.8898\n",
      "Epoch 275/400\n",
      "0s - loss: 604.5568\n",
      "Epoch 276/400\n",
      "0s - loss: 583.0524\n",
      "Epoch 277/400\n",
      "0s - loss: 601.5058\n",
      "Epoch 278/400\n",
      "0s - loss: 605.4091\n",
      "Epoch 279/400\n",
      "0s - loss: 607.8003\n",
      "Epoch 280/400\n",
      "0s - loss: 600.3026\n",
      "Epoch 281/400\n",
      "0s - loss: 605.3564\n",
      "Epoch 282/400\n",
      "0s - loss: 578.3415\n",
      "Epoch 283/400\n",
      "0s - loss: 616.0678\n",
      "Epoch 284/400\n",
      "0s - loss: 604.7032\n",
      "Epoch 285/400\n",
      "0s - loss: 562.6638\n",
      "Epoch 286/400\n",
      "0s - loss: 630.6684\n",
      "Epoch 287/400\n",
      "0s - loss: 613.9427\n",
      "Epoch 288/400\n",
      "0s - loss: 608.4965\n",
      "Epoch 289/400\n",
      "0s - loss: 571.5322\n",
      "Epoch 290/400\n",
      "0s - loss: 604.1821\n",
      "Epoch 291/400\n",
      "0s - loss: 576.0256\n",
      "Epoch 292/400\n",
      "0s - loss: 599.9941\n",
      "Epoch 293/400\n",
      "0s - loss: 567.2766\n",
      "Epoch 294/400\n",
      "0s - loss: 580.6212\n",
      "Epoch 295/400\n",
      "0s - loss: 581.8243\n",
      "Epoch 296/400\n",
      "0s - loss: 579.5408\n",
      "Epoch 297/400\n",
      "0s - loss: 565.9909\n",
      "Epoch 298/400\n",
      "0s - loss: 577.3508\n",
      "Epoch 299/400\n",
      "0s - loss: 585.1886\n",
      "Epoch 300/400\n",
      "0s - loss: 568.9026\n",
      "Epoch 301/400\n",
      "0s - loss: 573.3999\n",
      "Epoch 302/400\n",
      "0s - loss: 597.5709\n",
      "Epoch 303/400\n",
      "0s - loss: 594.4603\n",
      "Epoch 304/400\n",
      "0s - loss: 551.5027\n",
      "Epoch 305/400\n",
      "0s - loss: 606.3517\n",
      "Epoch 306/400\n",
      "0s - loss: 621.6460\n",
      "Epoch 307/400\n",
      "0s - loss: 570.5191\n",
      "Epoch 308/400\n",
      "0s - loss: 556.4258\n",
      "Epoch 309/400\n",
      "0s - loss: 588.8202\n",
      "Epoch 310/400\n",
      "0s - loss: 592.1851\n",
      "Epoch 311/400\n",
      "0s - loss: 571.3584\n",
      "Epoch 312/400\n",
      "0s - loss: 553.8831\n",
      "Epoch 313/400\n",
      "0s - loss: 546.8093\n",
      "Epoch 314/400\n",
      "0s - loss: 564.4229\n",
      "Epoch 315/400\n",
      "0s - loss: 584.8831\n",
      "Epoch 316/400\n",
      "0s - loss: 567.5152\n",
      "Epoch 317/400\n",
      "0s - loss: 574.9017\n",
      "Epoch 318/400\n",
      "0s - loss: 576.3693\n",
      "Epoch 319/400\n",
      "0s - loss: 558.9732\n",
      "Epoch 320/400\n",
      "0s - loss: 582.1362\n",
      "Epoch 321/400\n",
      "0s - loss: 570.1046\n",
      "Epoch 322/400\n",
      "0s - loss: 559.8782\n",
      "Epoch 323/400\n",
      "0s - loss: 561.7031\n",
      "Epoch 324/400\n",
      "0s - loss: 572.1890\n",
      "Epoch 325/400\n",
      "0s - loss: 575.9087\n",
      "Epoch 326/400\n",
      "0s - loss: 593.0480\n",
      "Epoch 327/400\n",
      "0s - loss: 554.2157\n",
      "Epoch 328/400\n",
      "0s - loss: 576.7178\n",
      "Epoch 329/400\n",
      "0s - loss: 555.1342\n",
      "Epoch 330/400\n",
      "0s - loss: 577.6546\n",
      "Epoch 331/400\n",
      "0s - loss: 553.6898\n",
      "Epoch 332/400\n",
      "0s - loss: 561.1046\n",
      "Epoch 333/400\n",
      "0s - loss: 580.1220\n",
      "Epoch 334/400\n",
      "0s - loss: 566.9985\n",
      "Epoch 335/400\n",
      "0s - loss: 563.7829\n",
      "Epoch 336/400\n",
      "0s - loss: 549.6160\n",
      "Epoch 337/400\n",
      "0s - loss: 572.7924\n",
      "Epoch 338/400\n",
      "0s - loss: 577.3272\n",
      "Epoch 339/400\n",
      "0s - loss: 577.5086\n",
      "Epoch 340/400\n",
      "0s - loss: 553.5920\n",
      "Epoch 341/400\n",
      "0s - loss: 555.4396\n",
      "Epoch 342/400\n",
      "0s - loss: 559.1939\n",
      "Epoch 343/400\n",
      "0s - loss: 588.4486\n",
      "Epoch 344/400\n",
      "0s - loss: 562.9133\n",
      "Epoch 345/400\n",
      "0s - loss: 539.4044\n",
      "Epoch 346/400\n",
      "0s - loss: 554.7874\n",
      "Epoch 347/400\n",
      "0s - loss: 562.9782\n",
      "Epoch 348/400\n",
      "0s - loss: 558.6702\n",
      "Epoch 349/400\n",
      "0s - loss: 572.7831\n",
      "Epoch 350/400\n",
      "0s - loss: 580.7544\n",
      "Epoch 351/400\n",
      "0s - loss: 577.2610\n",
      "Epoch 352/400\n",
      "0s - loss: 595.9763\n",
      "Epoch 353/400\n",
      "0s - loss: 593.1243\n",
      "Epoch 354/400\n",
      "0s - loss: 530.3980\n",
      "Epoch 355/400\n",
      "0s - loss: 568.7434\n",
      "Epoch 356/400\n",
      "0s - loss: 544.0594\n",
      "Epoch 357/400\n",
      "0s - loss: 551.8494\n",
      "Epoch 358/400\n",
      "0s - loss: 564.9724\n",
      "Epoch 359/400\n",
      "0s - loss: 568.7494\n",
      "Epoch 360/400\n",
      "0s - loss: 532.6717\n",
      "Epoch 361/400\n",
      "0s - loss: 561.8775\n",
      "Epoch 362/400\n",
      "0s - loss: 528.8728\n",
      "Epoch 363/400\n",
      "0s - loss: 535.1156\n",
      "Epoch 364/400\n",
      "0s - loss: 558.5935\n",
      "Epoch 365/400\n",
      "0s - loss: 604.3622\n",
      "Epoch 366/400\n",
      "0s - loss: 584.0101\n",
      "Epoch 367/400\n",
      "0s - loss: 544.9987\n",
      "Epoch 368/400\n",
      "0s - loss: 539.1497\n",
      "Epoch 369/400\n",
      "0s - loss: 537.8611\n",
      "Epoch 370/400\n",
      "0s - loss: 541.5705\n",
      "Epoch 371/400\n",
      "0s - loss: 542.1164\n",
      "Epoch 372/400\n",
      "0s - loss: 537.6556\n",
      "Epoch 373/400\n",
      "0s - loss: 533.2930\n",
      "Epoch 374/400\n",
      "0s - loss: 531.1352\n",
      "Epoch 375/400\n",
      "0s - loss: 537.9927\n",
      "Epoch 376/400\n",
      "0s - loss: 529.0455\n",
      "Epoch 377/400\n",
      "0s - loss: 562.9490\n",
      "Epoch 378/400\n",
      "0s - loss: 533.5964\n",
      "Epoch 379/400\n",
      "0s - loss: 545.7417\n",
      "Epoch 380/400\n",
      "0s - loss: 526.2620\n",
      "Epoch 381/400\n",
      "0s - loss: 542.2157\n",
      "Epoch 382/400\n",
      "0s - loss: 544.0447\n",
      "Epoch 383/400\n",
      "0s - loss: 563.3696\n",
      "Epoch 384/400\n",
      "0s - loss: 547.6089\n",
      "Epoch 385/400\n",
      "0s - loss: 521.7012\n",
      "Epoch 386/400\n",
      "0s - loss: 555.4944\n",
      "Epoch 387/400\n",
      "0s - loss: 526.9600\n",
      "Epoch 388/400\n",
      "0s - loss: 540.2317\n",
      "Epoch 389/400\n",
      "0s - loss: 539.7743\n",
      "Epoch 390/400\n",
      "0s - loss: 549.0294\n",
      "Epoch 391/400\n",
      "0s - loss: 534.8961\n",
      "Epoch 392/400\n",
      "0s - loss: 528.4066\n",
      "Epoch 393/400\n",
      "0s - loss: 558.4861\n",
      "Epoch 394/400\n",
      "0s - loss: 541.8755\n",
      "Epoch 395/400\n",
      "0s - loss: 536.9952\n",
      "Epoch 396/400\n",
      "0s - loss: 541.6130\n",
      "Epoch 397/400\n",
      "0s - loss: 535.8343\n",
      "Epoch 398/400\n",
      "0s - loss: 526.6102\n",
      "Epoch 399/400\n",
      "0s - loss: 542.2484\n",
      "Epoch 400/400\n",
      "0s - loss: 516.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1270aa850>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit Multilayer Perceptron model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=look_back, activation= 'relu'))\n",
    "model.add(Dense(8, activation= 'relu' ))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss= 'mean_squared_error' , optimizer= 'adam' )\n",
    "model.fit(trainX, trainY, epochs=400, batch_size=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "0s - loss: 520.7651\n",
      "Epoch 2/400\n",
      "0s - loss: 529.2288\n",
      "Epoch 3/400\n",
      "0s - loss: 527.5444\n",
      "Epoch 4/400\n",
      "0s - loss: 535.9317\n",
      "Epoch 5/400\n",
      "0s - loss: 532.6755\n",
      "Epoch 6/400\n",
      "0s - loss: 534.0423\n",
      "Epoch 7/400\n",
      "0s - loss: 533.6742\n",
      "Epoch 8/400\n",
      "0s - loss: 521.1781\n",
      "Epoch 9/400\n",
      "0s - loss: 518.5165\n",
      "Epoch 10/400\n",
      "0s - loss: 518.8598\n",
      "Epoch 11/400\n",
      "0s - loss: 524.9757\n",
      "Epoch 12/400\n",
      "0s - loss: 514.0973\n",
      "Epoch 13/400\n",
      "0s - loss: 548.8767\n",
      "Epoch 14/400\n",
      "0s - loss: 562.1722\n",
      "Epoch 15/400\n",
      "0s - loss: 549.7211\n",
      "Epoch 16/400\n",
      "0s - loss: 528.1211\n",
      "Epoch 17/400\n",
      "0s - loss: 526.7989\n",
      "Epoch 18/400\n",
      "0s - loss: 519.2701\n",
      "Epoch 19/400\n",
      "0s - loss: 536.0666\n",
      "Epoch 20/400\n",
      "0s - loss: 522.6419\n",
      "Epoch 21/400\n",
      "0s - loss: 514.3896\n",
      "Epoch 22/400\n",
      "0s - loss: 536.3952\n",
      "Epoch 23/400\n",
      "0s - loss: 507.4938\n",
      "Epoch 24/400\n",
      "0s - loss: 522.9843\n",
      "Epoch 25/400\n",
      "0s - loss: 556.6014\n",
      "Epoch 26/400\n",
      "0s - loss: 512.5304\n",
      "Epoch 27/400\n",
      "0s - loss: 538.7196\n",
      "Epoch 28/400\n",
      "0s - loss: 544.7907\n",
      "Epoch 29/400\n",
      "0s - loss: 529.2820\n",
      "Epoch 30/400\n",
      "0s - loss: 546.1564\n",
      "Epoch 31/400\n",
      "0s - loss: 518.6085\n",
      "Epoch 32/400\n",
      "0s - loss: 529.7067\n",
      "Epoch 33/400\n",
      "0s - loss: 511.8251\n",
      "Epoch 34/400\n",
      "0s - loss: 511.7154\n",
      "Epoch 35/400\n",
      "0s - loss: 522.3729\n",
      "Epoch 36/400\n",
      "0s - loss: 505.8212\n",
      "Epoch 37/400\n",
      "0s - loss: 520.6082\n",
      "Epoch 38/400\n",
      "0s - loss: 534.0384\n",
      "Epoch 39/400\n",
      "0s - loss: 507.2762\n",
      "Epoch 40/400\n",
      "0s - loss: 521.3851\n",
      "Epoch 41/400\n",
      "0s - loss: 522.7089\n",
      "Epoch 42/400\n",
      "0s - loss: 527.4552\n",
      "Epoch 43/400\n",
      "0s - loss: 534.5252\n",
      "Epoch 44/400\n",
      "0s - loss: 577.8263\n",
      "Epoch 45/400\n",
      "0s - loss: 536.2239\n",
      "Epoch 46/400\n",
      "0s - loss: 513.3308\n",
      "Epoch 47/400\n",
      "0s - loss: 533.2528\n",
      "Epoch 48/400\n",
      "0s - loss: 503.3366\n",
      "Epoch 49/400\n",
      "0s - loss: 512.6619\n",
      "Epoch 50/400\n",
      "0s - loss: 513.4172\n",
      "Epoch 51/400\n",
      "0s - loss: 516.5390\n",
      "Epoch 52/400\n",
      "0s - loss: 522.4654\n",
      "Epoch 53/400\n",
      "0s - loss: 573.9534\n",
      "Epoch 54/400\n",
      "0s - loss: 521.4397\n",
      "Epoch 55/400\n",
      "0s - loss: 497.3461\n",
      "Epoch 56/400\n",
      "0s - loss: 515.4259\n",
      "Epoch 57/400\n",
      "0s - loss: 510.2978\n",
      "Epoch 58/400\n",
      "0s - loss: 512.0718\n",
      "Epoch 59/400\n",
      "0s - loss: 516.0573\n",
      "Epoch 60/400\n",
      "0s - loss: 501.5769\n",
      "Epoch 61/400\n",
      "0s - loss: 524.3010\n",
      "Epoch 62/400\n",
      "0s - loss: 516.7977\n",
      "Epoch 63/400\n",
      "0s - loss: 519.4407\n",
      "Epoch 64/400\n",
      "0s - loss: 529.9162\n",
      "Epoch 65/400\n",
      "0s - loss: 515.5291\n",
      "Epoch 66/400\n",
      "0s - loss: 501.3590\n",
      "Epoch 67/400\n",
      "0s - loss: 528.9763\n",
      "Epoch 68/400\n",
      "0s - loss: 507.3794\n",
      "Epoch 69/400\n",
      "0s - loss: 508.0568\n",
      "Epoch 70/400\n",
      "0s - loss: 522.8489\n",
      "Epoch 71/400\n",
      "0s - loss: 546.3390\n",
      "Epoch 72/400\n",
      "0s - loss: 538.5164\n",
      "Epoch 73/400\n",
      "0s - loss: 509.4671\n",
      "Epoch 74/400\n",
      "0s - loss: 545.3439\n",
      "Epoch 75/400\n",
      "0s - loss: 514.5356\n",
      "Epoch 76/400\n",
      "0s - loss: 517.6315\n",
      "Epoch 77/400\n",
      "0s - loss: 499.7398\n",
      "Epoch 78/400\n",
      "0s - loss: 506.4575\n",
      "Epoch 79/400\n",
      "0s - loss: 510.6395\n",
      "Epoch 80/400\n",
      "0s - loss: 523.5135\n",
      "Epoch 81/400\n",
      "0s - loss: 511.8283\n",
      "Epoch 82/400\n",
      "0s - loss: 495.9968\n",
      "Epoch 83/400\n",
      "0s - loss: 506.0169\n",
      "Epoch 84/400\n",
      "0s - loss: 506.6285\n",
      "Epoch 85/400\n",
      "0s - loss: 504.2538\n",
      "Epoch 86/400\n",
      "0s - loss: 526.4783\n",
      "Epoch 87/400\n",
      "0s - loss: 516.1422\n",
      "Epoch 88/400\n",
      "0s - loss: 500.6820\n",
      "Epoch 89/400\n",
      "0s - loss: 506.0856\n",
      "Epoch 90/400\n",
      "0s - loss: 516.7801\n",
      "Epoch 91/400\n",
      "0s - loss: 498.9943\n",
      "Epoch 92/400\n",
      "0s - loss: 513.1175\n",
      "Epoch 93/400\n",
      "0s - loss: 544.5676\n",
      "Epoch 94/400\n",
      "0s - loss: 510.7244\n",
      "Epoch 95/400\n",
      "0s - loss: 543.9100\n",
      "Epoch 96/400\n",
      "0s - loss: 528.0306\n",
      "Epoch 97/400\n",
      "0s - loss: 513.8533\n",
      "Epoch 98/400\n",
      "0s - loss: 508.4095\n",
      "Epoch 99/400\n",
      "0s - loss: 535.1712\n",
      "Epoch 100/400\n",
      "0s - loss: 524.0924\n",
      "Epoch 101/400\n",
      "0s - loss: 532.2402\n",
      "Epoch 102/400\n",
      "0s - loss: 498.2315\n",
      "Epoch 103/400\n",
      "0s - loss: 502.5468\n",
      "Epoch 104/400\n",
      "0s - loss: 499.3540\n",
      "Epoch 105/400\n",
      "0s - loss: 506.5773\n",
      "Epoch 106/400\n",
      "0s - loss: 528.2584\n",
      "Epoch 107/400\n",
      "0s - loss: 495.4882\n",
      "Epoch 108/400\n",
      "0s - loss: 491.7168\n",
      "Epoch 109/400\n",
      "0s - loss: 502.6636\n",
      "Epoch 110/400\n",
      "0s - loss: 509.9082\n",
      "Epoch 111/400\n",
      "0s - loss: 504.7601\n",
      "Epoch 112/400\n",
      "0s - loss: 501.9634\n",
      "Epoch 113/400\n",
      "0s - loss: 493.6415\n",
      "Epoch 114/400\n",
      "0s - loss: 509.8521\n",
      "Epoch 115/400\n",
      "0s - loss: 499.5101\n",
      "Epoch 116/400\n",
      "0s - loss: 521.3514\n",
      "Epoch 117/400\n",
      "0s - loss: 513.1798\n",
      "Epoch 118/400\n",
      "0s - loss: 526.7131\n",
      "Epoch 119/400\n",
      "0s - loss: 485.3617\n",
      "Epoch 120/400\n",
      "0s - loss: 537.7420\n",
      "Epoch 121/400\n",
      "0s - loss: 533.1852\n",
      "Epoch 122/400\n",
      "0s - loss: 489.3482\n",
      "Epoch 123/400\n",
      "0s - loss: 511.6416\n",
      "Epoch 124/400\n",
      "0s - loss: 488.4964\n",
      "Epoch 125/400\n",
      "0s - loss: 505.3137\n",
      "Epoch 126/400\n",
      "0s - loss: 505.0061\n",
      "Epoch 127/400\n",
      "0s - loss: 514.6475\n",
      "Epoch 128/400\n",
      "0s - loss: 490.2471\n",
      "Epoch 129/400\n",
      "0s - loss: 504.6819\n",
      "Epoch 130/400\n",
      "0s - loss: 503.7168\n",
      "Epoch 131/400\n",
      "0s - loss: 500.2577\n",
      "Epoch 132/400\n",
      "0s - loss: 506.2901\n",
      "Epoch 133/400\n",
      "0s - loss: 490.6047\n",
      "Epoch 134/400\n",
      "0s - loss: 490.5117\n",
      "Epoch 135/400\n",
      "0s - loss: 511.9137\n",
      "Epoch 136/400\n",
      "0s - loss: 510.0878\n",
      "Epoch 137/400\n",
      "0s - loss: 530.0430\n",
      "Epoch 138/400\n",
      "0s - loss: 493.1710\n",
      "Epoch 139/400\n",
      "0s - loss: 506.6644\n",
      "Epoch 140/400\n",
      "0s - loss: 496.6495\n",
      "Epoch 141/400\n",
      "0s - loss: 497.3507\n",
      "Epoch 142/400\n",
      "0s - loss: 495.4221\n",
      "Epoch 143/400\n",
      "0s - loss: 489.6993\n",
      "Epoch 144/400\n",
      "0s - loss: 513.3677\n",
      "Epoch 145/400\n",
      "0s - loss: 500.2036\n",
      "Epoch 146/400\n",
      "0s - loss: 513.1079\n",
      "Epoch 147/400\n",
      "0s - loss: 515.5425\n",
      "Epoch 148/400\n",
      "0s - loss: 492.0075\n",
      "Epoch 149/400\n",
      "0s - loss: 491.6114\n",
      "Epoch 150/400\n",
      "0s - loss: 492.1969\n",
      "Epoch 151/400\n",
      "0s - loss: 497.0074\n",
      "Epoch 152/400\n",
      "0s - loss: 490.9878\n",
      "Epoch 153/400\n",
      "0s - loss: 471.4151\n",
      "Epoch 154/400\n",
      "0s - loss: 557.5171\n",
      "Epoch 155/400\n",
      "0s - loss: 529.3597\n",
      "Epoch 156/400\n",
      "0s - loss: 492.9662\n",
      "Epoch 157/400\n",
      "0s - loss: 495.2697\n",
      "Epoch 158/400\n",
      "0s - loss: 493.5134\n",
      "Epoch 159/400\n",
      "0s - loss: 504.4615\n",
      "Epoch 160/400\n",
      "0s - loss: 493.8676\n",
      "Epoch 161/400\n",
      "0s - loss: 485.5762\n",
      "Epoch 162/400\n",
      "0s - loss: 492.0758\n",
      "Epoch 163/400\n",
      "0s - loss: 526.5395\n",
      "Epoch 164/400\n",
      "0s - loss: 485.9837\n",
      "Epoch 165/400\n",
      "0s - loss: 505.5245\n",
      "Epoch 166/400\n",
      "0s - loss: 500.6123\n",
      "Epoch 167/400\n",
      "0s - loss: 516.1211\n",
      "Epoch 168/400\n",
      "0s - loss: 495.8239\n",
      "Epoch 169/400\n",
      "0s - loss: 493.4826\n",
      "Epoch 170/400\n",
      "0s - loss: 489.8335\n",
      "Epoch 171/400\n",
      "0s - loss: 560.1228\n",
      "Epoch 172/400\n",
      "0s - loss: 512.3586\n",
      "Epoch 173/400\n",
      "0s - loss: 493.9786\n",
      "Epoch 174/400\n",
      "0s - loss: 509.9474\n",
      "Epoch 175/400\n",
      "0s - loss: 521.3531\n",
      "Epoch 176/400\n",
      "0s - loss: 525.3270\n",
      "Epoch 177/400\n",
      "0s - loss: 527.8657\n",
      "Epoch 178/400\n",
      "0s - loss: 490.0070\n",
      "Epoch 179/400\n",
      "0s - loss: 503.4046\n",
      "Epoch 180/400\n",
      "0s - loss: 486.2021\n",
      "Epoch 181/400\n",
      "0s - loss: 493.8365\n",
      "Epoch 182/400\n",
      "0s - loss: 504.7036\n",
      "Epoch 183/400\n",
      "0s - loss: 479.8461\n",
      "Epoch 184/400\n",
      "0s - loss: 484.4269\n",
      "Epoch 185/400\n",
      "0s - loss: 485.5130\n",
      "Epoch 186/400\n",
      "0s - loss: 480.8934\n",
      "Epoch 187/400\n",
      "0s - loss: 500.1247\n",
      "Epoch 188/400\n",
      "0s - loss: 496.4263\n",
      "Epoch 189/400\n",
      "0s - loss: 491.1270\n",
      "Epoch 190/400\n",
      "0s - loss: 495.6773\n",
      "Epoch 191/400\n",
      "0s - loss: 517.5954\n",
      "Epoch 192/400\n",
      "0s - loss: 498.3818\n",
      "Epoch 193/400\n",
      "0s - loss: 492.7135\n",
      "Epoch 194/400\n",
      "0s - loss: 496.5302\n",
      "Epoch 195/400\n",
      "0s - loss: 518.4304\n",
      "Epoch 196/400\n",
      "0s - loss: 516.9056\n",
      "Epoch 197/400\n",
      "0s - loss: 488.2186\n",
      "Epoch 198/400\n",
      "0s - loss: 486.4219\n",
      "Epoch 199/400\n",
      "0s - loss: 485.4798\n",
      "Epoch 200/400\n",
      "0s - loss: 476.7416\n",
      "Epoch 201/400\n",
      "0s - loss: 495.8780\n",
      "Epoch 202/400\n",
      "0s - loss: 488.8331\n",
      "Epoch 203/400\n",
      "0s - loss: 494.1360\n",
      "Epoch 204/400\n",
      "0s - loss: 519.3888\n",
      "Epoch 205/400\n",
      "0s - loss: 507.0269\n",
      "Epoch 206/400\n",
      "0s - loss: 502.4275\n",
      "Epoch 207/400\n",
      "0s - loss: 493.1073\n",
      "Epoch 208/400\n",
      "0s - loss: 489.3981\n",
      "Epoch 209/400\n",
      "0s - loss: 506.7158\n",
      "Epoch 210/400\n",
      "0s - loss: 495.8374\n",
      "Epoch 211/400\n",
      "0s - loss: 486.2106\n",
      "Epoch 212/400\n",
      "0s - loss: 498.6301\n",
      "Epoch 213/400\n",
      "0s - loss: 489.7369\n",
      "Epoch 214/400\n",
      "0s - loss: 496.9848\n",
      "Epoch 215/400\n",
      "0s - loss: 494.7754\n",
      "Epoch 216/400\n",
      "0s - loss: 462.8117\n",
      "Epoch 217/400\n",
      "0s - loss: 530.1661\n",
      "Epoch 218/400\n",
      "0s - loss: 481.6143\n",
      "Epoch 219/400\n",
      "0s - loss: 494.7682\n",
      "Epoch 220/400\n",
      "0s - loss: 483.8630\n",
      "Epoch 221/400\n",
      "0s - loss: 485.1835\n",
      "Epoch 222/400\n",
      "0s - loss: 491.8296\n",
      "Epoch 223/400\n",
      "0s - loss: 486.3005\n",
      "Epoch 224/400\n",
      "0s - loss: 480.9807\n",
      "Epoch 225/400\n",
      "0s - loss: 482.4702\n",
      "Epoch 226/400\n",
      "0s - loss: 510.9055\n",
      "Epoch 227/400\n",
      "0s - loss: 494.9922\n",
      "Epoch 228/400\n",
      "0s - loss: 483.0607\n",
      "Epoch 229/400\n",
      "0s - loss: 504.3570\n",
      "Epoch 230/400\n",
      "0s - loss: 492.8678\n",
      "Epoch 231/400\n",
      "0s - loss: 495.8113\n",
      "Epoch 232/400\n",
      "0s - loss: 513.0920\n",
      "Epoch 233/400\n",
      "0s - loss: 486.5258\n",
      "Epoch 234/400\n",
      "0s - loss: 488.1349\n",
      "Epoch 235/400\n",
      "0s - loss: 499.8426\n",
      "Epoch 236/400\n",
      "0s - loss: 485.9634\n",
      "Epoch 237/400\n",
      "0s - loss: 497.1399\n",
      "Epoch 238/400\n",
      "0s - loss: 494.9047\n",
      "Epoch 239/400\n",
      "0s - loss: 496.9708\n",
      "Epoch 240/400\n",
      "0s - loss: 477.1685\n",
      "Epoch 241/400\n",
      "0s - loss: 506.1390\n",
      "Epoch 242/400\n",
      "0s - loss: 472.1398\n",
      "Epoch 243/400\n",
      "0s - loss: 502.5096\n",
      "Epoch 244/400\n",
      "0s - loss: 487.3088\n",
      "Epoch 245/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 478.0962\n",
      "Epoch 246/400\n",
      "0s - loss: 492.1194\n",
      "Epoch 247/400\n",
      "0s - loss: 522.0279\n",
      "Epoch 248/400\n",
      "0s - loss: 509.0993\n",
      "Epoch 249/400\n",
      "0s - loss: 530.2959\n",
      "Epoch 250/400\n",
      "0s - loss: 487.3739\n",
      "Epoch 251/400\n",
      "0s - loss: 475.3647\n",
      "Epoch 252/400\n",
      "0s - loss: 483.2267\n",
      "Epoch 253/400\n",
      "0s - loss: 498.1142\n",
      "Epoch 254/400\n",
      "0s - loss: 472.4041\n",
      "Epoch 255/400\n",
      "0s - loss: 491.2446\n",
      "Epoch 256/400\n",
      "0s - loss: 480.0005\n",
      "Epoch 257/400\n",
      "0s - loss: 550.0589\n",
      "Epoch 258/400\n",
      "0s - loss: 509.0072\n",
      "Epoch 259/400\n",
      "0s - loss: 507.0084\n",
      "Epoch 260/400\n",
      "0s - loss: 492.5423\n",
      "Epoch 261/400\n",
      "0s - loss: 487.7932\n",
      "Epoch 262/400\n",
      "0s - loss: 491.6370\n",
      "Epoch 263/400\n",
      "0s - loss: 481.2700\n",
      "Epoch 264/400\n",
      "0s - loss: 480.7143\n",
      "Epoch 265/400\n",
      "0s - loss: 487.6174\n",
      "Epoch 266/400\n",
      "0s - loss: 483.5291\n",
      "Epoch 267/400\n",
      "0s - loss: 502.9537\n",
      "Epoch 268/400\n",
      "0s - loss: 476.9438\n",
      "Epoch 269/400\n",
      "0s - loss: 501.1741\n",
      "Epoch 270/400\n",
      "0s - loss: 481.2252\n",
      "Epoch 271/400\n",
      "0s - loss: 478.6116\n",
      "Epoch 272/400\n",
      "0s - loss: 484.0215\n",
      "Epoch 273/400\n",
      "0s - loss: 504.3959\n",
      "Epoch 274/400\n",
      "0s - loss: 480.3117\n",
      "Epoch 275/400\n",
      "0s - loss: 490.7489\n",
      "Epoch 276/400\n",
      "0s - loss: 489.0296\n",
      "Epoch 277/400\n",
      "0s - loss: 481.6898\n",
      "Epoch 278/400\n",
      "0s - loss: 522.8918\n",
      "Epoch 279/400\n",
      "0s - loss: 473.2038\n",
      "Epoch 280/400\n",
      "0s - loss: 492.7110\n",
      "Epoch 281/400\n",
      "0s - loss: 505.2783\n",
      "Epoch 282/400\n",
      "0s - loss: 505.8768\n",
      "Epoch 283/400\n",
      "0s - loss: 504.8113\n",
      "Epoch 284/400\n",
      "0s - loss: 513.0217\n",
      "Epoch 285/400\n",
      "0s - loss: 481.6991\n",
      "Epoch 286/400\n",
      "0s - loss: 491.1210\n",
      "Epoch 287/400\n",
      "0s - loss: 501.7832\n",
      "Epoch 288/400\n",
      "0s - loss: 507.1470\n",
      "Epoch 289/400\n",
      "0s - loss: 489.4058\n",
      "Epoch 290/400\n",
      "0s - loss: 497.8880\n",
      "Epoch 291/400\n",
      "0s - loss: 532.6768\n",
      "Epoch 292/400\n",
      "0s - loss: 486.9657\n",
      "Epoch 293/400\n",
      "0s - loss: 474.4546\n",
      "Epoch 294/400\n",
      "0s - loss: 490.9173\n",
      "Epoch 295/400\n",
      "0s - loss: 470.4598\n",
      "Epoch 296/400\n",
      "0s - loss: 487.8057\n",
      "Epoch 297/400\n",
      "0s - loss: 472.2850\n",
      "Epoch 298/400\n",
      "0s - loss: 501.7994\n",
      "Epoch 299/400\n",
      "0s - loss: 477.8323\n",
      "Epoch 300/400\n",
      "0s - loss: 481.6613\n",
      "Epoch 301/400\n",
      "0s - loss: 489.8735\n",
      "Epoch 302/400\n",
      "0s - loss: 477.1550\n",
      "Epoch 303/400\n",
      "0s - loss: 492.2410\n",
      "Epoch 304/400\n",
      "0s - loss: 492.9497\n",
      "Epoch 305/400\n",
      "0s - loss: 480.1655\n",
      "Epoch 306/400\n",
      "0s - loss: 485.3011\n",
      "Epoch 307/400\n",
      "0s - loss: 482.3332\n",
      "Epoch 308/400\n",
      "0s - loss: 472.6983\n",
      "Epoch 309/400\n",
      "0s - loss: 488.2885\n",
      "Epoch 310/400\n",
      "0s - loss: 492.7312\n",
      "Epoch 311/400\n",
      "0s - loss: 500.9016\n",
      "Epoch 312/400\n",
      "0s - loss: 490.8430\n",
      "Epoch 313/400\n",
      "0s - loss: 534.1057\n",
      "Epoch 314/400\n",
      "0s - loss: 485.0775\n",
      "Epoch 315/400\n",
      "0s - loss: 506.2032\n",
      "Epoch 316/400\n",
      "0s - loss: 484.7851\n",
      "Epoch 317/400\n",
      "0s - loss: 529.0594\n",
      "Epoch 318/400\n",
      "0s - loss: 521.3993\n",
      "Epoch 319/400\n",
      "0s - loss: 509.0368\n",
      "Epoch 320/400\n",
      "0s - loss: 482.8558\n",
      "Epoch 321/400\n",
      "0s - loss: 490.7720\n",
      "Epoch 322/400\n",
      "0s - loss: 480.9946\n",
      "Epoch 323/400\n",
      "0s - loss: 515.4370\n",
      "Epoch 324/400\n",
      "0s - loss: 475.1935\n",
      "Epoch 325/400\n",
      "0s - loss: 498.2725\n",
      "Epoch 326/400\n",
      "0s - loss: 479.9440\n",
      "Epoch 327/400\n",
      "0s - loss: 501.8754\n",
      "Epoch 328/400\n",
      "0s - loss: 487.7875\n",
      "Epoch 329/400\n",
      "0s - loss: 485.8916\n",
      "Epoch 330/400\n",
      "0s - loss: 487.4916\n",
      "Epoch 331/400\n",
      "0s - loss: 528.4268\n",
      "Epoch 332/400\n",
      "0s - loss: 517.4466\n",
      "Epoch 333/400\n",
      "0s - loss: 496.1027\n",
      "Epoch 334/400\n",
      "0s - loss: 483.7123\n",
      "Epoch 335/400\n",
      "0s - loss: 477.5121\n",
      "Epoch 336/400\n",
      "0s - loss: 472.3096\n",
      "Epoch 337/400\n",
      "0s - loss: 511.1099\n",
      "Epoch 338/400\n",
      "0s - loss: 467.4785\n",
      "Epoch 339/400\n",
      "0s - loss: 525.6956\n",
      "Epoch 340/400\n",
      "0s - loss: 462.0456\n",
      "Epoch 341/400\n",
      "0s - loss: 481.2938\n",
      "Epoch 342/400\n",
      "0s - loss: 480.1150\n",
      "Epoch 343/400\n",
      "0s - loss: 511.2656\n",
      "Epoch 344/400\n",
      "0s - loss: 489.2227\n",
      "Epoch 345/400\n",
      "0s - loss: 483.7860\n",
      "Epoch 346/400\n",
      "0s - loss: 469.6893\n",
      "Epoch 347/400\n",
      "0s - loss: 492.1763\n",
      "Epoch 348/400\n",
      "0s - loss: 478.4462\n",
      "Epoch 349/400\n",
      "0s - loss: 547.3900\n",
      "Epoch 350/400\n",
      "0s - loss: 507.7260\n",
      "Epoch 351/400\n",
      "0s - loss: 495.3012\n",
      "Epoch 352/400\n",
      "0s - loss: 498.8334\n",
      "Epoch 353/400\n",
      "0s - loss: 471.9917\n",
      "Epoch 354/400\n",
      "0s - loss: 477.4990\n",
      "Epoch 355/400\n",
      "0s - loss: 491.0685\n",
      "Epoch 356/400\n",
      "0s - loss: 480.6020\n",
      "Epoch 357/400\n",
      "0s - loss: 480.0438\n",
      "Epoch 358/400\n",
      "0s - loss: 483.9012\n",
      "Epoch 359/400\n",
      "0s - loss: 489.6974\n",
      "Epoch 360/400\n",
      "0s - loss: 490.0392\n",
      "Epoch 361/400\n",
      "0s - loss: 512.2724\n",
      "Epoch 362/400\n",
      "0s - loss: 483.3334\n",
      "Epoch 363/400\n",
      "0s - loss: 475.0609\n",
      "Epoch 364/400\n",
      "0s - loss: 484.2663\n",
      "Epoch 365/400\n",
      "0s - loss: 483.3295\n",
      "Epoch 366/400\n",
      "0s - loss: 496.7861\n",
      "Epoch 367/400\n",
      "0s - loss: 480.0897\n",
      "Epoch 368/400\n",
      "0s - loss: 497.9848\n",
      "Epoch 369/400\n",
      "0s - loss: 491.1441\n",
      "Epoch 370/400\n",
      "0s - loss: 497.0908\n",
      "Epoch 371/400\n",
      "0s - loss: 484.4581\n",
      "Epoch 372/400\n",
      "0s - loss: 489.4137\n",
      "Epoch 373/400\n",
      "0s - loss: 503.9749\n",
      "Epoch 374/400\n",
      "0s - loss: 485.3076\n",
      "Epoch 375/400\n",
      "0s - loss: 474.3535\n",
      "Epoch 376/400\n",
      "0s - loss: 473.4182\n",
      "Epoch 377/400\n",
      "0s - loss: 500.5226\n",
      "Epoch 378/400\n",
      "0s - loss: 520.2592\n",
      "Epoch 379/400\n",
      "0s - loss: 490.8450\n",
      "Epoch 380/400\n",
      "0s - loss: 478.0500\n",
      "Epoch 381/400\n",
      "0s - loss: 475.7843\n",
      "Epoch 382/400\n",
      "0s - loss: 540.6599\n",
      "Epoch 383/400\n",
      "0s - loss: 484.4485\n",
      "Epoch 384/400\n",
      "0s - loss: 479.0173\n",
      "Epoch 385/400\n",
      "0s - loss: 484.7360\n",
      "Epoch 386/400\n",
      "0s - loss: 497.5118\n",
      "Epoch 387/400\n",
      "0s - loss: 498.7552\n",
      "Epoch 388/400\n",
      "0s - loss: 506.9962\n",
      "Epoch 389/400\n",
      "0s - loss: 492.6111\n",
      "Epoch 390/400\n",
      "0s - loss: 504.9501\n",
      "Epoch 391/400\n",
      "0s - loss: 508.1766\n",
      "Epoch 392/400\n",
      "0s - loss: 526.2937\n",
      "Epoch 393/400\n",
      "0s - loss: 485.5711\n",
      "Epoch 394/400\n",
      "0s - loss: 483.8895\n",
      "Epoch 395/400\n",
      "0s - loss: 486.5028\n",
      "Epoch 396/400\n",
      "0s - loss: 484.5328\n",
      "Epoch 397/400\n",
      "0s - loss: 488.1154\n",
      "Epoch 398/400\n",
      "0s - loss: 490.5573\n",
      "Epoch 399/400\n",
      "0s - loss: 500.1684\n",
      "Epoch 400/400\n",
      "0s - loss: 525.7872\n",
      "Train Score: 544.35 MSE (23.33 RMSE) \n",
      " Test Score: 2087.45 MSE (45.69 RMSE) \n"
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=400, batch_size=2, verbose=2)\n",
    "# Estimate model performance\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE) ' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print(' Test Score: %.2f MSE (%.2f RMSE) ' % (testScore, math.sqrt(testScore)))\n",
    "# generate predictions for training\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npXIsDYSIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146ace190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2s - loss: 51486.2586\n",
      "Epoch 2/100\n",
      "0s - loss: 51443.6892\n",
      "Epoch 3/100\n",
      "0s - loss: 51405.5642\n",
      "Epoch 4/100\n",
      "0s - loss: 51367.9532\n",
      "Epoch 5/100\n",
      "0s - loss: 51330.3687\n",
      "Epoch 6/100\n",
      "1s - loss: 51292.7791\n",
      "Epoch 7/100\n",
      "1s - loss: 51255.2483\n",
      "Epoch 8/100\n",
      "1s - loss: 51217.7126\n",
      "Epoch 9/100\n",
      "1s - loss: 51180.1661\n",
      "Epoch 10/100\n",
      "1s - loss: 51142.5535\n",
      "Epoch 11/100\n",
      "0s - loss: 51105.0397\n",
      "Epoch 12/100\n",
      "1s - loss: 51067.6648\n",
      "Epoch 13/100\n",
      "0s - loss: 51030.1841\n",
      "Epoch 14/100\n",
      "0s - loss: 50992.6766\n",
      "Epoch 15/100\n",
      "1s - loss: 50955.3145\n",
      "Epoch 16/100\n",
      "0s - loss: 50917.9752\n",
      "Epoch 17/100\n",
      "0s - loss: 50880.4794\n",
      "Epoch 18/100\n",
      "1s - loss: 50843.1897\n",
      "Epoch 19/100\n",
      "1s - loss: 50805.7773\n",
      "Epoch 20/100\n",
      "1s - loss: 50768.5678\n",
      "Epoch 21/100\n",
      "1s - loss: 50731.1971\n",
      "Epoch 22/100\n",
      "1s - loss: 50693.9877\n",
      "Epoch 23/100\n",
      "1s - loss: 50656.7341\n",
      "Epoch 24/100\n",
      "0s - loss: 50619.5546\n",
      "Epoch 25/100\n",
      "0s - loss: 50582.2595\n",
      "Epoch 26/100\n",
      "1s - loss: 50545.1143\n",
      "Epoch 27/100\n",
      "1s - loss: 50508.0222\n",
      "Epoch 28/100\n",
      "0s - loss: 50470.7602\n",
      "Epoch 29/100\n",
      "0s - loss: 50433.6606\n",
      "Epoch 30/100\n",
      "0s - loss: 50396.5578\n",
      "Epoch 31/100\n",
      "0s - loss: 50359.3913\n",
      "Epoch 32/100\n",
      "0s - loss: 50322.2578\n",
      "Epoch 33/100\n",
      "1s - loss: 50285.1946\n",
      "Epoch 34/100\n",
      "0s - loss: 50248.1068\n",
      "Epoch 35/100\n",
      "1s - loss: 50211.0670\n",
      "Epoch 36/100\n",
      "1s - loss: 50174.0544\n",
      "Epoch 37/100\n",
      "0s - loss: 50137.0558\n",
      "Epoch 38/100\n",
      "1s - loss: 50100.0506\n",
      "Epoch 39/100\n",
      "1s - loss: 50063.1003\n",
      "Epoch 40/100\n",
      "0s - loss: 50026.1872\n",
      "Epoch 41/100\n",
      "0s - loss: 49989.1655\n",
      "Epoch 42/100\n",
      "1s - loss: 49952.2993\n",
      "Epoch 43/100\n",
      "0s - loss: 49915.4678\n",
      "Epoch 44/100\n",
      "0s - loss: 49878.5253\n",
      "Epoch 45/100\n",
      "1s - loss: 49841.7027\n",
      "Epoch 46/100\n",
      "0s - loss: 49804.7913\n",
      "Epoch 47/100\n",
      "0s - loss: 49768.0525\n",
      "Epoch 48/100\n",
      "0s - loss: 49731.1538\n",
      "Epoch 49/100\n",
      "0s - loss: 49694.3857\n",
      "Epoch 50/100\n",
      "0s - loss: 49657.5059\n",
      "Epoch 51/100\n",
      "1s - loss: 49620.7478\n",
      "Epoch 52/100\n",
      "0s - loss: 49583.9308\n",
      "Epoch 53/100\n",
      "0s - loss: 49547.2658\n",
      "Epoch 54/100\n",
      "1s - loss: 49510.4693\n",
      "Epoch 55/100\n",
      "1s - loss: 49473.8546\n",
      "Epoch 56/100\n",
      "1s - loss: 49437.1772\n",
      "Epoch 57/100\n",
      "1s - loss: 49400.5416\n",
      "Epoch 58/100\n",
      "0s - loss: 49363.9376\n",
      "Epoch 59/100\n",
      "1s - loss: 49327.2373\n",
      "Epoch 60/100\n",
      "0s - loss: 49290.7730\n",
      "Epoch 61/100\n",
      "0s - loss: 49254.1184\n",
      "Epoch 62/100\n",
      "0s - loss: 49217.6127\n",
      "Epoch 63/100\n",
      "0s - loss: 49181.1192\n",
      "Epoch 64/100\n",
      "0s - loss: 49144.5836\n",
      "Epoch 65/100\n",
      "1s - loss: 49108.1103\n",
      "Epoch 66/100\n",
      "1s - loss: 49071.5227\n",
      "Epoch 67/100\n",
      "1s - loss: 49035.1154\n",
      "Epoch 68/100\n",
      "0s - loss: 48998.5813\n",
      "Epoch 69/100\n",
      "0s - loss: 48962.2062\n",
      "Epoch 70/100\n",
      "0s - loss: 48925.6977\n",
      "Epoch 71/100\n",
      "0s - loss: 48889.3082\n",
      "Epoch 72/100\n",
      "0s - loss: 48852.9182\n",
      "Epoch 73/100\n",
      "0s - loss: 48816.5673\n",
      "Epoch 74/100\n",
      "0s - loss: 48780.2358\n",
      "Epoch 75/100\n",
      "0s - loss: 48743.7700\n",
      "Epoch 76/100\n",
      "0s - loss: 48707.4976\n",
      "Epoch 77/100\n",
      "0s - loss: 48671.1535\n",
      "Epoch 78/100\n",
      "0s - loss: 48634.8596\n",
      "Epoch 79/100\n",
      "0s - loss: 48598.6330\n",
      "Epoch 80/100\n",
      "1s - loss: 48562.3432\n",
      "Epoch 81/100\n",
      "1s - loss: 48526.0512\n",
      "Epoch 82/100\n",
      "0s - loss: 48489.7740\n",
      "Epoch 83/100\n",
      "0s - loss: 48453.7179\n",
      "Epoch 84/100\n",
      "0s - loss: 48417.4239\n",
      "Epoch 85/100\n",
      "0s - loss: 48381.2699\n",
      "Epoch 86/100\n",
      "0s - loss: 48345.1685\n",
      "Epoch 87/100\n",
      "0s - loss: 48309.0025\n",
      "Epoch 88/100\n",
      "1s - loss: 48272.8447\n",
      "Epoch 89/100\n",
      "1s - loss: 48236.7444\n",
      "Epoch 90/100\n",
      "0s - loss: 48200.5577\n",
      "Epoch 91/100\n",
      "0s - loss: 48164.5255\n",
      "Epoch 92/100\n",
      "1s - loss: 48128.3840\n",
      "Epoch 93/100\n",
      "0s - loss: 48092.3753\n",
      "Epoch 94/100\n",
      "0s - loss: 48056.3110\n",
      "Epoch 95/100\n",
      "0s - loss: 48020.2516\n",
      "Epoch 96/100\n",
      "1s - loss: 47984.2410\n",
      "Epoch 97/100\n",
      "1s - loss: 47948.2965\n",
      "Epoch 98/100\n",
      "0s - loss: 47912.2628\n",
      "Epoch 99/100\n",
      "0s - loss: 47876.4159\n",
      "Epoch 100/100\n",
      "0s - loss: 47840.4366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146605a50>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss= 'mean_squared_error' , optimizer= 'adam' )\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', '->', 'B')\n",
      "('B', '->', 'C')\n",
      "('C', '->', 'D')\n",
      "('D', '->', 'E')\n",
      "('E', '->', 'F')\n",
      "('F', '->', 'G')\n",
      "('G', '->', 'H')\n",
      "('H', '->', 'I')\n",
      "('I', '->', 'J')\n",
      "('J', '->', 'K')\n",
      "('K', '->', 'L')\n",
      "('L', '->', 'M')\n",
      "('M', '->', 'N')\n",
      "('N', '->', 'O')\n",
      "('O', '->', 'P')\n",
      "('P', '->', 'Q')\n",
      "('Q', '->', 'R')\n",
      "('R', '->', 'S')\n",
      "('S', '->', 'T')\n",
      "('T', '->', 'U')\n",
      "('U', '->', 'V')\n",
      "('V', '->', 'W')\n",
      "('W', '->', 'X')\n",
      "('X', '->', 'Y')\n",
      "('Y', '->', 'Z')\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "  seq_in = alphabet[i:i + seq_length]\n",
    "  seq_out = alphabet[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "  print(seq_in, '->' , seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1s - loss: 3.2661 - acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "0s - loss: 3.2582 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "0s - loss: 3.2551 - acc: 0.0400\n",
      "Epoch 4/500\n",
      "0s - loss: 3.2524 - acc: 0.0400\n",
      "Epoch 5/500\n",
      "0s - loss: 3.2495 - acc: 0.0400\n",
      "Epoch 6/500\n",
      "0s - loss: 3.2471 - acc: 0.0400\n",
      "Epoch 7/500\n",
      "0s - loss: 3.2440 - acc: 0.0400\n",
      "Epoch 8/500\n",
      "0s - loss: 3.2412 - acc: 0.0400\n",
      "Epoch 9/500\n",
      "0s - loss: 3.2378 - acc: 0.0400\n",
      "Epoch 10/500\n",
      "0s - loss: 3.2348 - acc: 0.0400\n",
      "Epoch 11/500\n",
      "0s - loss: 3.2312 - acc: 0.0400\n",
      "Epoch 12/500\n",
      "0s - loss: 3.2276 - acc: 0.0400\n",
      "Epoch 13/500\n",
      "0s - loss: 3.2237 - acc: 0.0400\n",
      "Epoch 14/500\n",
      "0s - loss: 3.2203 - acc: 0.0400\n",
      "Epoch 15/500\n",
      "0s - loss: 3.2160 - acc: 0.0400\n",
      "Epoch 16/500\n",
      "0s - loss: 3.2116 - acc: 0.0400\n",
      "Epoch 17/500\n",
      "0s - loss: 3.2066 - acc: 0.0400\n",
      "Epoch 18/500\n",
      "0s - loss: 3.2016 - acc: 0.0400\n",
      "Epoch 19/500\n",
      "0s - loss: 3.1969 - acc: 0.0400\n",
      "Epoch 20/500\n",
      "0s - loss: 3.1911 - acc: 0.0400\n",
      "Epoch 21/500\n",
      "0s - loss: 3.1854 - acc: 0.0400\n",
      "Epoch 22/500\n",
      "0s - loss: 3.1789 - acc: 0.0400\n",
      "Epoch 23/500\n",
      "0s - loss: 3.1725 - acc: 0.0400\n",
      "Epoch 24/500\n",
      "0s - loss: 3.1665 - acc: 0.0400\n",
      "Epoch 25/500\n",
      "0s - loss: 3.1594 - acc: 0.0400\n",
      "Epoch 26/500\n",
      "0s - loss: 3.1517 - acc: 0.0400\n",
      "Epoch 27/500\n",
      "0s - loss: 3.1458 - acc: 0.0400\n",
      "Epoch 28/500\n",
      "0s - loss: 3.1367 - acc: 0.0800\n",
      "Epoch 29/500\n",
      "0s - loss: 3.1286 - acc: 0.1200\n",
      "Epoch 30/500\n",
      "0s - loss: 3.1203 - acc: 0.1200\n",
      "Epoch 31/500\n",
      "0s - loss: 3.1120 - acc: 0.1200\n",
      "Epoch 32/500\n",
      "0s - loss: 3.1019 - acc: 0.0800\n",
      "Epoch 33/500\n",
      "0s - loss: 3.0928 - acc: 0.0800\n",
      "Epoch 34/500\n",
      "0s - loss: 3.0834 - acc: 0.0400\n",
      "Epoch 35/500\n",
      "0s - loss: 3.0729 - acc: 0.0400\n",
      "Epoch 36/500\n",
      "0s - loss: 3.0624 - acc: 0.0800\n",
      "Epoch 37/500\n",
      "0s - loss: 3.0515 - acc: 0.0800\n",
      "Epoch 38/500\n",
      "0s - loss: 3.0411 - acc: 0.0400\n",
      "Epoch 39/500\n",
      "0s - loss: 3.0307 - acc: 0.0800\n",
      "Epoch 40/500\n",
      "0s - loss: 3.0191 - acc: 0.0800\n",
      "Epoch 41/500\n",
      "0s - loss: 3.0064 - acc: 0.0400\n",
      "Epoch 42/500\n",
      "0s - loss: 2.9934 - acc: 0.0800\n",
      "Epoch 43/500\n",
      "0s - loss: 2.9809 - acc: 0.0400\n",
      "Epoch 44/500\n",
      "0s - loss: 2.9685 - acc: 0.0400\n",
      "Epoch 45/500\n",
      "0s - loss: 2.9547 - acc: 0.0400\n",
      "Epoch 46/500\n",
      "0s - loss: 2.9421 - acc: 0.0400\n",
      "Epoch 47/500\n",
      "0s - loss: 2.9297 - acc: 0.0800\n",
      "Epoch 48/500\n",
      "0s - loss: 2.9157 - acc: 0.0800\n",
      "Epoch 49/500\n",
      "0s - loss: 2.9027 - acc: 0.0800\n",
      "Epoch 50/500\n",
      "0s - loss: 2.8896 - acc: 0.0800\n",
      "Epoch 51/500\n",
      "0s - loss: 2.8768 - acc: 0.0800\n",
      "Epoch 52/500\n",
      "0s - loss: 2.8637 - acc: 0.0800\n",
      "Epoch 53/500\n",
      "0s - loss: 2.8513 - acc: 0.0800\n",
      "Epoch 54/500\n",
      "0s - loss: 2.8382 - acc: 0.0400\n",
      "Epoch 55/500\n",
      "0s - loss: 2.8249 - acc: 0.0800\n",
      "Epoch 56/500\n",
      "0s - loss: 2.8148 - acc: 0.0800\n",
      "Epoch 57/500\n",
      "0s - loss: 2.8021 - acc: 0.0800\n",
      "Epoch 58/500\n",
      "0s - loss: 2.7904 - acc: 0.0800\n",
      "Epoch 59/500\n",
      "0s - loss: 2.7782 - acc: 0.0800\n",
      "Epoch 60/500\n",
      "0s - loss: 2.7679 - acc: 0.0800\n",
      "Epoch 61/500\n",
      "0s - loss: 2.7569 - acc: 0.0800\n",
      "Epoch 62/500\n",
      "0s - loss: 2.7464 - acc: 0.0800\n",
      "Epoch 63/500\n",
      "0s - loss: 2.7367 - acc: 0.0800\n",
      "Epoch 64/500\n",
      "0s - loss: 2.7266 - acc: 0.0800\n",
      "Epoch 65/500\n",
      "0s - loss: 2.7165 - acc: 0.0400\n",
      "Epoch 66/500\n",
      "0s - loss: 2.7081 - acc: 0.1200\n",
      "Epoch 67/500\n",
      "0s - loss: 2.6989 - acc: 0.0800\n",
      "Epoch 68/500\n",
      "0s - loss: 2.6901 - acc: 0.1200\n",
      "Epoch 69/500\n",
      "0s - loss: 2.6817 - acc: 0.1200\n",
      "Epoch 70/500\n",
      "0s - loss: 2.6731 - acc: 0.1200\n",
      "Epoch 71/500\n",
      "0s - loss: 2.6658 - acc: 0.0800\n",
      "Epoch 72/500\n",
      "0s - loss: 2.6578 - acc: 0.1200\n",
      "Epoch 73/500\n",
      "0s - loss: 2.6510 - acc: 0.1200\n",
      "Epoch 74/500\n",
      "0s - loss: 2.6443 - acc: 0.1200\n",
      "Epoch 75/500\n",
      "0s - loss: 2.6371 - acc: 0.1200\n",
      "Epoch 76/500\n",
      "0s - loss: 2.6311 - acc: 0.1200\n",
      "Epoch 77/500\n",
      "0s - loss: 2.6237 - acc: 0.1200\n",
      "Epoch 78/500\n",
      "0s - loss: 2.6161 - acc: 0.0800\n",
      "Epoch 79/500\n",
      "0s - loss: 2.6097 - acc: 0.1200\n",
      "Epoch 80/500\n",
      "0s - loss: 2.6038 - acc: 0.1200\n",
      "Epoch 81/500\n",
      "0s - loss: 2.5976 - acc: 0.0800\n",
      "Epoch 82/500\n",
      "0s - loss: 2.5913 - acc: 0.0800\n",
      "Epoch 83/500\n",
      "0s - loss: 2.5864 - acc: 0.1200\n",
      "Epoch 84/500\n",
      "0s - loss: 2.5801 - acc: 0.1200\n",
      "Epoch 85/500\n",
      "0s - loss: 2.5747 - acc: 0.1200\n",
      "Epoch 86/500\n",
      "0s - loss: 2.5700 - acc: 0.1200\n",
      "Epoch 87/500\n",
      "0s - loss: 2.5633 - acc: 0.1200\n",
      "Epoch 88/500\n",
      "0s - loss: 2.5568 - acc: 0.1200\n",
      "Epoch 89/500\n",
      "0s - loss: 2.5506 - acc: 0.1200\n",
      "Epoch 90/500\n",
      "0s - loss: 2.5467 - acc: 0.1200\n",
      "Epoch 91/500\n",
      "0s - loss: 2.5418 - acc: 0.1200\n",
      "Epoch 92/500\n",
      "0s - loss: 2.5363 - acc: 0.1200\n",
      "Epoch 93/500\n",
      "0s - loss: 2.5305 - acc: 0.1200\n",
      "Epoch 94/500\n",
      "0s - loss: 2.5256 - acc: 0.1200\n",
      "Epoch 95/500\n",
      "0s - loss: 2.5202 - acc: 0.1600\n",
      "Epoch 96/500\n",
      "0s - loss: 2.5147 - acc: 0.1600\n",
      "Epoch 97/500\n",
      "0s - loss: 2.5096 - acc: 0.1600\n",
      "Epoch 98/500\n",
      "0s - loss: 2.5039 - acc: 0.1600\n",
      "Epoch 99/500\n",
      "0s - loss: 2.4983 - acc: 0.1600\n",
      "Epoch 100/500\n",
      "0s - loss: 2.4945 - acc: 0.1600\n",
      "Epoch 101/500\n",
      "0s - loss: 2.4877 - acc: 0.1600\n",
      "Epoch 102/500\n",
      "0s - loss: 2.4835 - acc: 0.1200\n",
      "Epoch 103/500\n",
      "0s - loss: 2.4785 - acc: 0.1600\n",
      "Epoch 104/500\n",
      "0s - loss: 2.4732 - acc: 0.1600\n",
      "Epoch 105/500\n",
      "0s - loss: 2.4682 - acc: 0.1200\n",
      "Epoch 106/500\n",
      "0s - loss: 2.4634 - acc: 0.1600\n",
      "Epoch 107/500\n",
      "0s - loss: 2.4584 - acc: 0.2000\n",
      "Epoch 108/500\n",
      "0s - loss: 2.4520 - acc: 0.2000\n",
      "Epoch 109/500\n",
      "0s - loss: 2.4474 - acc: 0.2000\n",
      "Epoch 110/500\n",
      "0s - loss: 2.4410 - acc: 0.2000\n",
      "Epoch 111/500\n",
      "0s - loss: 2.4384 - acc: 0.2000\n",
      "Epoch 112/500\n",
      "0s - loss: 2.4330 - acc: 0.2000\n",
      "Epoch 113/500\n",
      "0s - loss: 2.4275 - acc: 0.2000\n",
      "Epoch 114/500\n",
      "0s - loss: 2.4212 - acc: 0.1600\n",
      "Epoch 115/500\n",
      "0s - loss: 2.4162 - acc: 0.1600\n",
      "Epoch 116/500\n",
      "0s - loss: 2.4118 - acc: 0.1600\n",
      "Epoch 117/500\n",
      "0s - loss: 2.4087 - acc: 0.2000\n",
      "Epoch 118/500\n",
      "0s - loss: 2.4011 - acc: 0.2400\n",
      "Epoch 119/500\n",
      "0s - loss: 2.3977 - acc: 0.2000\n",
      "Epoch 120/500\n",
      "0s - loss: 2.3926 - acc: 0.2400\n",
      "Epoch 121/500\n",
      "0s - loss: 2.3882 - acc: 0.2000\n",
      "Epoch 122/500\n",
      "0s - loss: 2.3826 - acc: 0.2400\n",
      "Epoch 123/500\n",
      "0s - loss: 2.3782 - acc: 0.2400\n",
      "Epoch 124/500\n",
      "0s - loss: 2.3720 - acc: 0.2400\n",
      "Epoch 125/500\n",
      "0s - loss: 2.3700 - acc: 0.2000\n",
      "Epoch 126/500\n",
      "0s - loss: 2.3637 - acc: 0.2000\n",
      "Epoch 127/500\n",
      "0s - loss: 2.3585 - acc: 0.2400\n",
      "Epoch 128/500\n",
      "0s - loss: 2.3560 - acc: 0.2400\n",
      "Epoch 129/500\n",
      "0s - loss: 2.3503 - acc: 0.2000\n",
      "Epoch 130/500\n",
      "0s - loss: 2.3462 - acc: 0.2400\n",
      "Epoch 131/500\n",
      "0s - loss: 2.3427 - acc: 0.2400\n",
      "Epoch 132/500\n",
      "0s - loss: 2.3374 - acc: 0.2800\n",
      "Epoch 133/500\n",
      "0s - loss: 2.3335 - acc: 0.2400\n",
      "Epoch 134/500\n",
      "0s - loss: 2.3292 - acc: 0.2400\n",
      "Epoch 135/500\n",
      "0s - loss: 2.3240 - acc: 0.2000\n",
      "Epoch 136/500\n",
      "0s - loss: 2.3217 - acc: 0.2400\n",
      "Epoch 137/500\n",
      "0s - loss: 2.3166 - acc: 0.2000\n",
      "Epoch 138/500\n",
      "0s - loss: 2.3134 - acc: 0.2400\n",
      "Epoch 139/500\n",
      "0s - loss: 2.3085 - acc: 0.2800\n",
      "Epoch 140/500\n",
      "0s - loss: 2.3054 - acc: 0.2800\n",
      "Epoch 141/500\n",
      "0s - loss: 2.3015 - acc: 0.2400\n",
      "Epoch 142/500\n",
      "0s - loss: 2.2976 - acc: 0.2400\n",
      "Epoch 143/500\n",
      "0s - loss: 2.2930 - acc: 0.2000\n",
      "Epoch 144/500\n",
      "0s - loss: 2.2908 - acc: 0.2000\n",
      "Epoch 145/500\n",
      "0s - loss: 2.2866 - acc: 0.2400\n",
      "Epoch 146/500\n",
      "0s - loss: 2.2850 - acc: 0.2800\n",
      "Epoch 147/500\n",
      "0s - loss: 2.2799 - acc: 0.2800\n",
      "Epoch 148/500\n",
      "0s - loss: 2.2750 - acc: 0.2400\n",
      "Epoch 149/500\n",
      "0s - loss: 2.2739 - acc: 0.2800\n",
      "Epoch 150/500\n",
      "0s - loss: 2.2688 - acc: 0.2800\n",
      "Epoch 151/500\n",
      "0s - loss: 2.2663 - acc: 0.2000\n",
      "Epoch 152/500\n",
      "0s - loss: 2.2636 - acc: 0.2400\n",
      "Epoch 153/500\n",
      "0s - loss: 2.2589 - acc: 0.2800\n",
      "Epoch 154/500\n",
      "0s - loss: 2.2563 - acc: 0.2800\n",
      "Epoch 155/500\n",
      "0s - loss: 2.2514 - acc: 0.2400\n",
      "Epoch 156/500\n",
      "0s - loss: 2.2510 - acc: 0.2400\n",
      "Epoch 157/500\n",
      "0s - loss: 2.2460 - acc: 0.3200\n",
      "Epoch 158/500\n",
      "0s - loss: 2.2420 - acc: 0.3200\n",
      "Epoch 159/500\n",
      "0s - loss: 2.2419 - acc: 0.2800\n",
      "Epoch 160/500\n",
      "0s - loss: 2.2379 - acc: 0.2400\n",
      "Epoch 161/500\n",
      "0s - loss: 2.2333 - acc: 0.2400\n",
      "Epoch 162/500\n",
      "0s - loss: 2.2320 - acc: 0.2400\n",
      "Epoch 163/500\n",
      "0s - loss: 2.2285 - acc: 0.3200\n",
      "Epoch 164/500\n",
      "0s - loss: 2.2256 - acc: 0.2800\n",
      "Epoch 165/500\n",
      "0s - loss: 2.2216 - acc: 0.2800\n",
      "Epoch 166/500\n",
      "0s - loss: 2.2181 - acc: 0.2800\n",
      "Epoch 167/500\n",
      "0s - loss: 2.2160 - acc: 0.2400\n",
      "Epoch 168/500\n",
      "0s - loss: 2.2130 - acc: 0.2000\n",
      "Epoch 169/500\n",
      "0s - loss: 2.2116 - acc: 0.2400\n",
      "Epoch 170/500\n",
      "0s - loss: 2.2077 - acc: 0.3200\n",
      "Epoch 171/500\n",
      "0s - loss: 2.2058 - acc: 0.3600\n",
      "Epoch 172/500\n",
      "0s - loss: 2.2018 - acc: 0.3200\n",
      "Epoch 173/500\n",
      "0s - loss: 2.1996 - acc: 0.4000\n",
      "Epoch 174/500\n",
      "0s - loss: 2.1961 - acc: 0.2800\n",
      "Epoch 175/500\n",
      "0s - loss: 2.1940 - acc: 0.2800\n",
      "Epoch 176/500\n",
      "0s - loss: 2.1903 - acc: 0.3200\n",
      "Epoch 177/500\n",
      "0s - loss: 2.1875 - acc: 0.2400\n",
      "Epoch 178/500\n",
      "0s - loss: 2.1848 - acc: 0.3600\n",
      "Epoch 179/500\n",
      "0s - loss: 2.1823 - acc: 0.2800\n",
      "Epoch 180/500\n",
      "0s - loss: 2.1798 - acc: 0.2400\n",
      "Epoch 181/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 2.1759 - acc: 0.2800\n",
      "Epoch 182/500\n",
      "0s - loss: 2.1749 - acc: 0.3200\n",
      "Epoch 183/500\n",
      "0s - loss: 2.1720 - acc: 0.3200\n",
      "Epoch 184/500\n",
      "0s - loss: 2.1686 - acc: 0.3200\n",
      "Epoch 185/500\n",
      "0s - loss: 2.1651 - acc: 0.3200\n",
      "Epoch 186/500\n",
      "0s - loss: 2.1639 - acc: 0.3200\n",
      "Epoch 187/500\n",
      "0s - loss: 2.1613 - acc: 0.2800\n",
      "Epoch 188/500\n",
      "0s - loss: 2.1571 - acc: 0.3600\n",
      "Epoch 189/500\n",
      "0s - loss: 2.1558 - acc: 0.4000\n",
      "Epoch 190/500\n",
      "0s - loss: 2.1540 - acc: 0.4000\n",
      "Epoch 191/500\n",
      "0s - loss: 2.1511 - acc: 0.2800\n",
      "Epoch 192/500\n",
      "0s - loss: 2.1498 - acc: 0.4400\n",
      "Epoch 193/500\n",
      "0s - loss: 2.1465 - acc: 0.4000\n",
      "Epoch 194/500\n",
      "0s - loss: 2.1432 - acc: 0.3200\n",
      "Epoch 195/500\n",
      "0s - loss: 2.1406 - acc: 0.4000\n",
      "Epoch 196/500\n",
      "0s - loss: 2.1395 - acc: 0.4400\n",
      "Epoch 197/500\n",
      "0s - loss: 2.1380 - acc: 0.4000\n",
      "Epoch 198/500\n",
      "0s - loss: 2.1340 - acc: 0.3600\n",
      "Epoch 199/500\n",
      "0s - loss: 2.1314 - acc: 0.3200\n",
      "Epoch 200/500\n",
      "0s - loss: 2.1280 - acc: 0.4000\n",
      "Epoch 201/500\n",
      "0s - loss: 2.1269 - acc: 0.3200\n",
      "Epoch 202/500\n",
      "0s - loss: 2.1249 - acc: 0.2800\n",
      "Epoch 203/500\n",
      "0s - loss: 2.1221 - acc: 0.3200\n",
      "Epoch 204/500\n",
      "0s - loss: 2.1210 - acc: 0.2800\n",
      "Epoch 205/500\n",
      "0s - loss: 2.1187 - acc: 0.3200\n",
      "Epoch 206/500\n",
      "0s - loss: 2.1161 - acc: 0.3600\n",
      "Epoch 207/500\n",
      "0s - loss: 2.1124 - acc: 0.3600\n",
      "Epoch 208/500\n",
      "0s - loss: 2.1104 - acc: 0.2800\n",
      "Epoch 209/500\n",
      "0s - loss: 2.1099 - acc: 0.3600\n",
      "Epoch 210/500\n",
      "0s - loss: 2.1069 - acc: 0.4400\n",
      "Epoch 211/500\n",
      "0s - loss: 2.1044 - acc: 0.3600\n",
      "Epoch 212/500\n",
      "0s - loss: 2.1014 - acc: 0.3200\n",
      "Epoch 213/500\n",
      "0s - loss: 2.1015 - acc: 0.3200\n",
      "Epoch 214/500\n",
      "0s - loss: 2.0986 - acc: 0.3600\n",
      "Epoch 215/500\n",
      "0s - loss: 2.0953 - acc: 0.3200\n",
      "Epoch 216/500\n",
      "0s - loss: 2.0941 - acc: 0.4000\n",
      "Epoch 217/500\n",
      "0s - loss: 2.0926 - acc: 0.4400\n",
      "Epoch 218/500\n",
      "0s - loss: 2.0898 - acc: 0.4000\n",
      "Epoch 219/500\n",
      "0s - loss: 2.0875 - acc: 0.3200\n",
      "Epoch 220/500\n",
      "0s - loss: 2.0847 - acc: 0.3600\n",
      "Epoch 221/500\n",
      "0s - loss: 2.0837 - acc: 0.2800\n",
      "Epoch 222/500\n",
      "0s - loss: 2.0820 - acc: 0.3600\n",
      "Epoch 223/500\n",
      "0s - loss: 2.0785 - acc: 0.2800\n",
      "Epoch 224/500\n",
      "0s - loss: 2.0756 - acc: 0.4000\n",
      "Epoch 225/500\n",
      "0s - loss: 2.0755 - acc: 0.4000\n",
      "Epoch 226/500\n",
      "0s - loss: 2.0756 - acc: 0.4400\n",
      "Epoch 227/500\n",
      "0s - loss: 2.0728 - acc: 0.4000\n",
      "Epoch 228/500\n",
      "0s - loss: 2.0665 - acc: 0.4000\n",
      "Epoch 229/500\n",
      "0s - loss: 2.0670 - acc: 0.5200\n",
      "Epoch 230/500\n",
      "0s - loss: 2.0651 - acc: 0.5200\n",
      "Epoch 231/500\n",
      "0s - loss: 2.0631 - acc: 0.4400\n",
      "Epoch 232/500\n",
      "0s - loss: 2.0611 - acc: 0.4000\n",
      "Epoch 233/500\n",
      "0s - loss: 2.0605 - acc: 0.4000\n",
      "Epoch 234/500\n",
      "0s - loss: 2.0575 - acc: 0.4400\n",
      "Epoch 235/500\n",
      "0s - loss: 2.0564 - acc: 0.4800\n",
      "Epoch 236/500\n",
      "0s - loss: 2.0550 - acc: 0.4800\n",
      "Epoch 237/500\n",
      "0s - loss: 2.0504 - acc: 0.4400\n",
      "Epoch 238/500\n",
      "0s - loss: 2.0484 - acc: 0.4800\n",
      "Epoch 239/500\n",
      "0s - loss: 2.0482 - acc: 0.4000\n",
      "Epoch 240/500\n",
      "0s - loss: 2.0452 - acc: 0.2800\n",
      "Epoch 241/500\n",
      "0s - loss: 2.0423 - acc: 0.4400\n",
      "Epoch 242/500\n",
      "0s - loss: 2.0429 - acc: 0.3200\n",
      "Epoch 243/500\n",
      "0s - loss: 2.0413 - acc: 0.4000\n",
      "Epoch 244/500\n",
      "0s - loss: 2.0373 - acc: 0.4400\n",
      "Epoch 245/500\n",
      "0s - loss: 2.0362 - acc: 0.4800\n",
      "Epoch 246/500\n",
      "0s - loss: 2.0356 - acc: 0.4800\n",
      "Epoch 247/500\n",
      "0s - loss: 2.0331 - acc: 0.5200\n",
      "Epoch 248/500\n",
      "0s - loss: 2.0316 - acc: 0.4000\n",
      "Epoch 249/500\n",
      "0s - loss: 2.0288 - acc: 0.3600\n",
      "Epoch 250/500\n",
      "0s - loss: 2.0262 - acc: 0.5200\n",
      "Epoch 251/500\n",
      "0s - loss: 2.0257 - acc: 0.4400\n",
      "Epoch 252/500\n",
      "0s - loss: 2.0252 - acc: 0.4000\n",
      "Epoch 253/500\n",
      "0s - loss: 2.0214 - acc: 0.4400\n",
      "Epoch 254/500\n",
      "0s - loss: 2.0203 - acc: 0.4000\n",
      "Epoch 255/500\n",
      "0s - loss: 2.0179 - acc: 0.4400\n",
      "Epoch 256/500\n",
      "0s - loss: 2.0161 - acc: 0.5600\n",
      "Epoch 257/500\n",
      "0s - loss: 2.0149 - acc: 0.4800\n",
      "Epoch 258/500\n",
      "0s - loss: 2.0119 - acc: 0.5600\n",
      "Epoch 259/500\n",
      "0s - loss: 2.0124 - acc: 0.4400\n",
      "Epoch 260/500\n",
      "0s - loss: 2.0089 - acc: 0.4400\n",
      "Epoch 261/500\n",
      "0s - loss: 2.0080 - acc: 0.5200\n",
      "Epoch 262/500\n",
      "0s - loss: 2.0068 - acc: 0.3200\n",
      "Epoch 263/500\n",
      "0s - loss: 2.0059 - acc: 0.4400\n",
      "Epoch 264/500\n",
      "0s - loss: 2.0038 - acc: 0.4400\n",
      "Epoch 265/500\n",
      "0s - loss: 2.0004 - acc: 0.5600\n",
      "Epoch 266/500\n",
      "0s - loss: 1.9994 - acc: 0.4400\n",
      "Epoch 267/500\n",
      "0s - loss: 1.9982 - acc: 0.5200\n",
      "Epoch 268/500\n",
      "0s - loss: 1.9940 - acc: 0.5600\n",
      "Epoch 269/500\n",
      "0s - loss: 1.9947 - acc: 0.5200\n",
      "Epoch 270/500\n",
      "0s - loss: 1.9917 - acc: 0.4800\n",
      "Epoch 271/500\n",
      "0s - loss: 1.9898 - acc: 0.4800\n",
      "Epoch 272/500\n",
      "0s - loss: 1.9872 - acc: 0.5600\n",
      "Epoch 273/500\n",
      "0s - loss: 1.9862 - acc: 0.5600\n",
      "Epoch 274/500\n",
      "0s - loss: 1.9849 - acc: 0.4400\n",
      "Epoch 275/500\n",
      "0s - loss: 1.9828 - acc: 0.4800\n",
      "Epoch 276/500\n",
      "0s - loss: 1.9798 - acc: 0.4800\n",
      "Epoch 277/500\n",
      "0s - loss: 1.9793 - acc: 0.4800\n",
      "Epoch 278/500\n",
      "0s - loss: 1.9798 - acc: 0.5600\n",
      "Epoch 279/500\n",
      "0s - loss: 1.9755 - acc: 0.5200\n",
      "Epoch 280/500\n",
      "0s - loss: 1.9753 - acc: 0.4400\n",
      "Epoch 281/500\n",
      "0s - loss: 1.9747 - acc: 0.5600\n",
      "Epoch 282/500\n",
      "0s - loss: 1.9722 - acc: 0.4800\n",
      "Epoch 283/500\n",
      "0s - loss: 1.9701 - acc: 0.4800\n",
      "Epoch 284/500\n",
      "0s - loss: 1.9698 - acc: 0.5600\n",
      "Epoch 285/500\n",
      "0s - loss: 1.9672 - acc: 0.4800\n",
      "Epoch 286/500\n",
      "0s - loss: 1.9647 - acc: 0.5600\n",
      "Epoch 287/500\n",
      "0s - loss: 1.9622 - acc: 0.5200\n",
      "Epoch 288/500\n",
      "0s - loss: 1.9613 - acc: 0.5200\n",
      "Epoch 289/500\n",
      "0s - loss: 1.9602 - acc: 0.5600\n",
      "Epoch 290/500\n",
      "0s - loss: 1.9590 - acc: 0.5200\n",
      "Epoch 291/500\n",
      "0s - loss: 1.9568 - acc: 0.6000\n",
      "Epoch 292/500\n",
      "0s - loss: 1.9543 - acc: 0.5600\n",
      "Epoch 293/500\n",
      "0s - loss: 1.9560 - acc: 0.5600\n",
      "Epoch 294/500\n",
      "0s - loss: 1.9513 - acc: 0.4800\n",
      "Epoch 295/500\n",
      "0s - loss: 1.9515 - acc: 0.6400\n",
      "Epoch 296/500\n",
      "0s - loss: 1.9482 - acc: 0.4800\n",
      "Epoch 297/500\n",
      "0s - loss: 1.9472 - acc: 0.4800\n",
      "Epoch 298/500\n",
      "0s - loss: 1.9444 - acc: 0.5600\n",
      "Epoch 299/500\n",
      "0s - loss: 1.9458 - acc: 0.5600\n",
      "Epoch 300/500\n",
      "0s - loss: 1.9422 - acc: 0.4800\n",
      "Epoch 301/500\n",
      "0s - loss: 1.9412 - acc: 0.4800\n",
      "Epoch 302/500\n",
      "0s - loss: 1.9403 - acc: 0.5600\n",
      "Epoch 303/500\n",
      "0s - loss: 1.9387 - acc: 0.5200\n",
      "Epoch 304/500\n",
      "0s - loss: 1.9352 - acc: 0.5600\n",
      "Epoch 305/500\n",
      "0s - loss: 1.9367 - acc: 0.6000\n",
      "Epoch 306/500\n",
      "0s - loss: 1.9338 - acc: 0.5600\n",
      "Epoch 307/500\n",
      "0s - loss: 1.9336 - acc: 0.5600\n",
      "Epoch 308/500\n",
      "0s - loss: 1.9314 - acc: 0.4800\n",
      "Epoch 309/500\n",
      "0s - loss: 1.9292 - acc: 0.6000\n",
      "Epoch 310/500\n",
      "0s - loss: 1.9292 - acc: 0.5200\n",
      "Epoch 311/500\n",
      "0s - loss: 1.9271 - acc: 0.6000\n",
      "Epoch 312/500\n",
      "0s - loss: 1.9266 - acc: 0.5600\n",
      "Epoch 313/500\n",
      "0s - loss: 1.9237 - acc: 0.5600\n",
      "Epoch 314/500\n",
      "0s - loss: 1.9232 - acc: 0.5600\n",
      "Epoch 315/500\n",
      "0s - loss: 1.9208 - acc: 0.6000\n",
      "Epoch 316/500\n",
      "0s - loss: 1.9195 - acc: 0.6000\n",
      "Epoch 317/500\n",
      "0s - loss: 1.9182 - acc: 0.4800\n",
      "Epoch 318/500\n",
      "0s - loss: 1.9156 - acc: 0.6400\n",
      "Epoch 319/500\n",
      "0s - loss: 1.9141 - acc: 0.6400\n",
      "Epoch 320/500\n",
      "0s - loss: 1.9134 - acc: 0.5600\n",
      "Epoch 321/500\n",
      "0s - loss: 1.9122 - acc: 0.4800\n",
      "Epoch 322/500\n",
      "0s - loss: 1.9096 - acc: 0.6000\n",
      "Epoch 323/500\n",
      "0s - loss: 1.9086 - acc: 0.6000\n",
      "Epoch 324/500\n",
      "0s - loss: 1.9070 - acc: 0.5600\n",
      "Epoch 325/500\n",
      "0s - loss: 1.9070 - acc: 0.5600\n",
      "Epoch 326/500\n",
      "0s - loss: 1.9021 - acc: 0.6400\n",
      "Epoch 327/500\n",
      "0s - loss: 1.9024 - acc: 0.6000\n",
      "Epoch 328/500\n",
      "0s - loss: 1.9006 - acc: 0.5600\n",
      "Epoch 329/500\n",
      "0s - loss: 1.8984 - acc: 0.6800\n",
      "Epoch 330/500\n",
      "0s - loss: 1.8980 - acc: 0.6000\n",
      "Epoch 331/500\n",
      "0s - loss: 1.8975 - acc: 0.6000\n",
      "Epoch 332/500\n",
      "0s - loss: 1.8943 - acc: 0.6800\n",
      "Epoch 333/500\n",
      "0s - loss: 1.8919 - acc: 0.6000\n",
      "Epoch 334/500\n",
      "0s - loss: 1.8914 - acc: 0.6000\n",
      "Epoch 335/500\n",
      "0s - loss: 1.8909 - acc: 0.6400\n",
      "Epoch 336/500\n",
      "0s - loss: 1.8918 - acc: 0.4800\n",
      "Epoch 337/500\n",
      "0s - loss: 1.8883 - acc: 0.6400\n",
      "Epoch 338/500\n",
      "0s - loss: 1.8884 - acc: 0.6000\n",
      "Epoch 339/500\n",
      "0s - loss: 1.8877 - acc: 0.6400\n",
      "Epoch 340/500\n",
      "0s - loss: 1.8830 - acc: 0.6000\n",
      "Epoch 341/500\n",
      "0s - loss: 1.8842 - acc: 0.6800\n",
      "Epoch 342/500\n",
      "0s - loss: 1.8826 - acc: 0.5600\n",
      "Epoch 343/500\n",
      "0s - loss: 1.8814 - acc: 0.5200\n",
      "Epoch 344/500\n",
      "0s - loss: 1.8782 - acc: 0.5600\n",
      "Epoch 345/500\n",
      "0s - loss: 1.8775 - acc: 0.6000\n",
      "Epoch 346/500\n",
      "0s - loss: 1.8756 - acc: 0.6400\n",
      "Epoch 347/500\n",
      "0s - loss: 1.8744 - acc: 0.5600\n",
      "Epoch 348/500\n",
      "0s - loss: 1.8741 - acc: 0.6400\n",
      "Epoch 349/500\n",
      "0s - loss: 1.8726 - acc: 0.5600\n",
      "Epoch 350/500\n",
      "0s - loss: 1.8715 - acc: 0.6000\n",
      "Epoch 351/500\n",
      "0s - loss: 1.8695 - acc: 0.7200\n",
      "Epoch 352/500\n",
      "0s - loss: 1.8673 - acc: 0.5200\n",
      "Epoch 353/500\n",
      "0s - loss: 1.8690 - acc: 0.6000\n",
      "Epoch 354/500\n",
      "0s - loss: 1.8657 - acc: 0.6800\n",
      "Epoch 355/500\n",
      "0s - loss: 1.8639 - acc: 0.5600\n",
      "Epoch 356/500\n",
      "0s - loss: 1.8619 - acc: 0.6000\n",
      "Epoch 357/500\n",
      "0s - loss: 1.8623 - acc: 0.6400\n",
      "Epoch 358/500\n",
      "0s - loss: 1.8611 - acc: 0.5600\n",
      "Epoch 359/500\n",
      "0s - loss: 1.8593 - acc: 0.6800\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 1.8598 - acc: 0.6800\n",
      "Epoch 361/500\n",
      "0s - loss: 1.8552 - acc: 0.6800\n",
      "Epoch 362/500\n",
      "0s - loss: 1.8541 - acc: 0.6800\n",
      "Epoch 363/500\n",
      "0s - loss: 1.8554 - acc: 0.6800\n",
      "Epoch 364/500\n",
      "0s - loss: 1.8523 - acc: 0.7600\n",
      "Epoch 365/500\n",
      "0s - loss: 1.8518 - acc: 0.6000\n",
      "Epoch 366/500\n",
      "0s - loss: 1.8517 - acc: 0.6400\n",
      "Epoch 367/500\n",
      "0s - loss: 1.8492 - acc: 0.6000\n",
      "Epoch 368/500\n",
      "0s - loss: 1.8467 - acc: 0.6400\n",
      "Epoch 369/500\n",
      "0s - loss: 1.8446 - acc: 0.6800\n",
      "Epoch 370/500\n",
      "0s - loss: 1.8456 - acc: 0.6000\n",
      "Epoch 371/500\n",
      "0s - loss: 1.8442 - acc: 0.6000\n",
      "Epoch 372/500\n",
      "0s - loss: 1.8415 - acc: 0.6400\n",
      "Epoch 373/500\n",
      "0s - loss: 1.8429 - acc: 0.7200\n",
      "Epoch 374/500\n",
      "0s - loss: 1.8398 - acc: 0.6400\n",
      "Epoch 375/500\n",
      "0s - loss: 1.8390 - acc: 0.7200\n",
      "Epoch 376/500\n",
      "0s - loss: 1.8370 - acc: 0.7200\n",
      "Epoch 377/500\n",
      "0s - loss: 1.8363 - acc: 0.6400\n",
      "Epoch 378/500\n",
      "0s - loss: 1.8372 - acc: 0.7200\n",
      "Epoch 379/500\n",
      "0s - loss: 1.8341 - acc: 0.6800\n",
      "Epoch 380/500\n",
      "0s - loss: 1.8325 - acc: 0.6400\n",
      "Epoch 381/500\n",
      "0s - loss: 1.8315 - acc: 0.7200\n",
      "Epoch 382/500\n",
      "0s - loss: 1.8304 - acc: 0.6800\n",
      "Epoch 383/500\n",
      "0s - loss: 1.8271 - acc: 0.7200\n",
      "Epoch 384/500\n",
      "0s - loss: 1.8265 - acc: 0.6400\n",
      "Epoch 385/500\n",
      "0s - loss: 1.8284 - acc: 0.7600\n",
      "Epoch 386/500\n",
      "0s - loss: 1.8230 - acc: 0.6800\n",
      "Epoch 387/500\n",
      "0s - loss: 1.8222 - acc: 0.6400\n",
      "Epoch 388/500\n",
      "0s - loss: 1.8216 - acc: 0.7200\n",
      "Epoch 389/500\n",
      "0s - loss: 1.8227 - acc: 0.6400\n",
      "Epoch 390/500\n",
      "0s - loss: 1.8183 - acc: 0.6400\n",
      "Epoch 391/500\n",
      "0s - loss: 1.8197 - acc: 0.5600\n",
      "Epoch 392/500\n",
      "0s - loss: 1.8173 - acc: 0.6800\n",
      "Epoch 393/500\n",
      "0s - loss: 1.8165 - acc: 0.6400\n",
      "Epoch 394/500\n",
      "0s - loss: 1.8145 - acc: 0.6000\n",
      "Epoch 395/500\n",
      "0s - loss: 1.8138 - acc: 0.6400\n",
      "Epoch 396/500\n",
      "0s - loss: 1.8139 - acc: 0.6400\n",
      "Epoch 397/500\n",
      "0s - loss: 1.8110 - acc: 0.7200\n",
      "Epoch 398/500\n",
      "0s - loss: 1.8105 - acc: 0.6400\n",
      "Epoch 399/500\n",
      "0s - loss: 1.8101 - acc: 0.8000\n",
      "Epoch 400/500\n",
      "0s - loss: 1.8095 - acc: 0.6400\n",
      "Epoch 401/500\n",
      "0s - loss: 1.8090 - acc: 0.6400\n",
      "Epoch 402/500\n",
      "0s - loss: 1.8044 - acc: 0.7200\n",
      "Epoch 403/500\n",
      "0s - loss: 1.8064 - acc: 0.6800\n",
      "Epoch 404/500\n",
      "0s - loss: 1.8054 - acc: 0.6800\n",
      "Epoch 405/500\n",
      "0s - loss: 1.8032 - acc: 0.7200\n",
      "Epoch 406/500\n",
      "0s - loss: 1.8013 - acc: 0.7200\n",
      "Epoch 407/500\n",
      "0s - loss: 1.8011 - acc: 0.7600\n",
      "Epoch 408/500\n",
      "0s - loss: 1.8003 - acc: 0.7200\n",
      "Epoch 409/500\n",
      "0s - loss: 1.7988 - acc: 0.8000\n",
      "Epoch 410/500\n",
      "0s - loss: 1.7996 - acc: 0.6400\n",
      "Epoch 411/500\n",
      "0s - loss: 1.7961 - acc: 0.6800\n",
      "Epoch 412/500\n",
      "0s - loss: 1.7943 - acc: 0.7200\n",
      "Epoch 413/500\n",
      "0s - loss: 1.7934 - acc: 0.7600\n",
      "Epoch 414/500\n",
      "0s - loss: 1.7925 - acc: 0.7200\n",
      "Epoch 415/500\n",
      "0s - loss: 1.7918 - acc: 0.6800\n",
      "Epoch 416/500\n",
      "0s - loss: 1.7899 - acc: 0.6800\n",
      "Epoch 417/500\n",
      "0s - loss: 1.7906 - acc: 0.6400\n",
      "Epoch 418/500\n",
      "0s - loss: 1.7898 - acc: 0.7200\n",
      "Epoch 419/500\n",
      "0s - loss: 1.7862 - acc: 0.6800\n",
      "Epoch 420/500\n",
      "0s - loss: 1.7850 - acc: 0.7200\n",
      "Epoch 421/500\n",
      "0s - loss: 1.7857 - acc: 0.6400\n",
      "Epoch 422/500\n",
      "0s - loss: 1.7855 - acc: 0.6800\n",
      "Epoch 423/500\n",
      "0s - loss: 1.7843 - acc: 0.7200\n",
      "Epoch 424/500\n",
      "0s - loss: 1.7830 - acc: 0.6800\n",
      "Epoch 425/500\n",
      "0s - loss: 1.7796 - acc: 0.7200\n",
      "Epoch 426/500\n",
      "0s - loss: 1.7792 - acc: 0.7200\n",
      "Epoch 427/500\n",
      "0s - loss: 1.7785 - acc: 0.7200\n",
      "Epoch 428/500\n",
      "0s - loss: 1.7768 - acc: 0.7200\n",
      "Epoch 429/500\n",
      "0s - loss: 1.7749 - acc: 0.6800\n",
      "Epoch 430/500\n",
      "0s - loss: 1.7737 - acc: 0.7600\n",
      "Epoch 431/500\n",
      "0s - loss: 1.7740 - acc: 0.7200\n",
      "Epoch 432/500\n",
      "0s - loss: 1.7737 - acc: 0.7600\n",
      "Epoch 433/500\n",
      "0s - loss: 1.7709 - acc: 0.6800\n",
      "Epoch 434/500\n",
      "0s - loss: 1.7712 - acc: 0.7600\n",
      "Epoch 435/500\n",
      "0s - loss: 1.7682 - acc: 0.7600\n",
      "Epoch 436/500\n",
      "0s - loss: 1.7685 - acc: 0.7600\n",
      "Epoch 437/500\n",
      "0s - loss: 1.7669 - acc: 0.6400\n",
      "Epoch 438/500\n",
      "0s - loss: 1.7683 - acc: 0.7200\n",
      "Epoch 439/500\n",
      "0s - loss: 1.7658 - acc: 0.6800\n",
      "Epoch 440/500\n",
      "0s - loss: 1.7623 - acc: 0.7200\n",
      "Epoch 441/500\n",
      "0s - loss: 1.7636 - acc: 0.7600\n",
      "Epoch 442/500\n",
      "0s - loss: 1.7637 - acc: 0.7200\n",
      "Epoch 443/500\n",
      "0s - loss: 1.7619 - acc: 0.7200\n",
      "Epoch 444/500\n",
      "0s - loss: 1.7621 - acc: 0.7200\n",
      "Epoch 445/500\n",
      "0s - loss: 1.7608 - acc: 0.7600\n",
      "Epoch 446/500\n",
      "0s - loss: 1.7547 - acc: 0.7200\n",
      "Epoch 447/500\n",
      "0s - loss: 1.7582 - acc: 0.6800\n",
      "Epoch 448/500\n",
      "0s - loss: 1.7560 - acc: 0.6400\n",
      "Epoch 449/500\n",
      "0s - loss: 1.7544 - acc: 0.7600\n",
      "Epoch 450/500\n",
      "0s - loss: 1.7530 - acc: 0.8000\n",
      "Epoch 451/500\n",
      "0s - loss: 1.7531 - acc: 0.7200\n",
      "Epoch 452/500\n",
      "0s - loss: 1.7517 - acc: 0.7200\n",
      "Epoch 453/500\n",
      "0s - loss: 1.7497 - acc: 0.7600\n",
      "Epoch 454/500\n",
      "0s - loss: 1.7504 - acc: 0.8000\n",
      "Epoch 455/500\n",
      "0s - loss: 1.7495 - acc: 0.8000\n",
      "Epoch 456/500\n",
      "0s - loss: 1.7459 - acc: 0.6800\n",
      "Epoch 457/500\n",
      "0s - loss: 1.7466 - acc: 0.8000\n",
      "Epoch 458/500\n",
      "0s - loss: 1.7466 - acc: 0.6800\n",
      "Epoch 459/500\n",
      "0s - loss: 1.7462 - acc: 0.6800\n",
      "Epoch 460/500\n",
      "0s - loss: 1.7433 - acc: 0.8000\n",
      "Epoch 461/500\n",
      "0s - loss: 1.7425 - acc: 0.7200\n",
      "Epoch 462/500\n",
      "0s - loss: 1.7411 - acc: 0.7600\n",
      "Epoch 463/500\n",
      "0s - loss: 1.7406 - acc: 0.7200\n",
      "Epoch 464/500\n",
      "0s - loss: 1.7389 - acc: 0.7200\n",
      "Epoch 465/500\n",
      "0s - loss: 1.7409 - acc: 0.7600\n",
      "Epoch 466/500\n",
      "0s - loss: 1.7383 - acc: 0.7200\n",
      "Epoch 467/500\n",
      "0s - loss: 1.7355 - acc: 0.8000\n",
      "Epoch 468/500\n",
      "0s - loss: 1.7348 - acc: 0.7600\n",
      "Epoch 469/500\n",
      "0s - loss: 1.7343 - acc: 0.8000\n",
      "Epoch 470/500\n",
      "0s - loss: 1.7327 - acc: 0.6800\n",
      "Epoch 471/500\n",
      "0s - loss: 1.7345 - acc: 0.8000\n",
      "Epoch 472/500\n",
      "0s - loss: 1.7326 - acc: 0.8400\n",
      "Epoch 473/500\n",
      "0s - loss: 1.7299 - acc: 0.7200\n",
      "Epoch 474/500\n",
      "0s - loss: 1.7315 - acc: 0.7600\n",
      "Epoch 475/500\n",
      "0s - loss: 1.7286 - acc: 0.8400\n",
      "Epoch 476/500\n",
      "0s - loss: 1.7316 - acc: 0.6800\n",
      "Epoch 477/500\n",
      "0s - loss: 1.7281 - acc: 0.8000\n",
      "Epoch 478/500\n",
      "0s - loss: 1.7258 - acc: 0.7600\n",
      "Epoch 479/500\n",
      "0s - loss: 1.7224 - acc: 0.8000\n",
      "Epoch 480/500\n",
      "0s - loss: 1.7225 - acc: 0.8000\n",
      "Epoch 481/500\n",
      "0s - loss: 1.7236 - acc: 0.8000\n",
      "Epoch 482/500\n",
      "0s - loss: 1.7218 - acc: 0.7600\n",
      "Epoch 483/500\n",
      "0s - loss: 1.7234 - acc: 0.7200\n",
      "Epoch 484/500\n",
      "0s - loss: 1.7189 - acc: 0.7200\n",
      "Epoch 485/500\n",
      "0s - loss: 1.7207 - acc: 0.8000\n",
      "Epoch 486/500\n",
      "0s - loss: 1.7186 - acc: 0.8400\n",
      "Epoch 487/500\n",
      "0s - loss: 1.7179 - acc: 0.7600\n",
      "Epoch 488/500\n",
      "0s - loss: 1.7172 - acc: 0.7600\n",
      "Epoch 489/500\n",
      "0s - loss: 1.7162 - acc: 0.7600\n",
      "Epoch 490/500\n",
      "0s - loss: 1.7139 - acc: 0.7200\n",
      "Epoch 491/500\n",
      "0s - loss: 1.7125 - acc: 0.7600\n",
      "Epoch 492/500\n",
      "0s - loss: 1.7114 - acc: 0.8800\n",
      "Epoch 493/500\n",
      "0s - loss: 1.7105 - acc: 0.7600\n",
      "Epoch 494/500\n",
      "0s - loss: 1.7109 - acc: 0.8400\n",
      "Epoch 495/500\n",
      "0s - loss: 1.7077 - acc: 0.7200\n",
      "Epoch 496/500\n",
      "0s - loss: 1.7086 - acc: 0.8000\n",
      "Epoch 497/500\n",
      "0s - loss: 1.7080 - acc: 0.6800\n",
      "Epoch 498/500\n",
      "0s - loss: 1.7079 - acc: 0.7600\n",
      "Epoch 499/500\n",
      "0s - loss: 1.7085 - acc: 0.8400\n",
      "Epoch 500/500\n",
      "0s - loss: 1.7049 - acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1477268d0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation= 'softmax' ))\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ABC', '->', 'D')\n",
      "('BCD', '->', 'E')\n",
      "('CDE', '->', 'F')\n",
      "('DEF', '->', 'G')\n",
      "('EFG', '->', 'H')\n",
      "('FGH', '->', 'I')\n",
      "('GHI', '->', 'J')\n",
      "('HIJ', '->', 'K')\n",
      "('IJK', '->', 'L')\n",
      "('JKL', '->', 'M')\n",
      "('KLM', '->', 'N')\n",
      "('LMN', '->', 'O')\n",
      "('MNO', '->', 'P')\n",
      "('NOP', '->', 'Q')\n",
      "('OPQ', '->', 'R')\n",
      "('PQR', '->', 'S')\n",
      "('QRS', '->', 'T')\n",
      "('RST', '->', 'U')\n",
      "('STU', '->', 'V')\n",
      "('TUV', '->', 'W')\n",
      "('UVW', '->', 'X')\n",
      "('VWX', '->', 'Y')\n",
      "('WXY', '->', 'Z')\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "  seq_in = alphabet[i:i + seq_length]\n",
    "  seq_out = alphabet[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "  print(seq_in, '->' , seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1s - loss: 3.2669 - acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "0s - loss: 3.2545 - acc: 0.0435\n",
      "Epoch 3/500\n",
      "0s - loss: 3.2480 - acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "0s - loss: 3.2411 - acc: 0.0435\n",
      "Epoch 5/500\n",
      "0s - loss: 3.2338 - acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "0s - loss: 3.2271 - acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "0s - loss: 3.2204 - acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "0s - loss: 3.2124 - acc: 0.0435\n",
      "Epoch 9/500\n",
      "0s - loss: 3.2052 - acc: 0.0435\n",
      "Epoch 10/500\n",
      "0s - loss: 3.1965 - acc: 0.0435\n",
      "Epoch 11/500\n",
      "0s - loss: 3.1873 - acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "0s - loss: 3.1782 - acc: 0.0435\n",
      "Epoch 13/500\n",
      "0s - loss: 3.1692 - acc: 0.0435\n",
      "Epoch 14/500\n",
      "0s - loss: 3.1597 - acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "0s - loss: 3.1498 - acc: 0.0435\n",
      "Epoch 16/500\n",
      "0s - loss: 3.1404 - acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "0s - loss: 3.1309 - acc: 0.0435\n",
      "Epoch 18/500\n",
      "0s - loss: 3.1212 - acc: 0.0435\n",
      "Epoch 19/500\n",
      "0s - loss: 3.1122 - acc: 0.0435\n",
      "Epoch 20/500\n",
      "0s - loss: 3.1027 - acc: 0.0435\n",
      "Epoch 21/500\n",
      "0s - loss: 3.0949 - acc: 0.0435\n",
      "Epoch 22/500\n",
      "0s - loss: 3.0870 - acc: 0.0435\n",
      "Epoch 23/500\n",
      "0s - loss: 3.0786 - acc: 0.0435\n",
      "Epoch 24/500\n",
      "0s - loss: 3.0697 - acc: 0.0435\n",
      "Epoch 25/500\n",
      "0s - loss: 3.0629 - acc: 0.0435\n",
      "Epoch 26/500\n",
      "0s - loss: 3.0550 - acc: 0.0435\n",
      "Epoch 27/500\n",
      "0s - loss: 3.0482 - acc: 0.0435\n",
      "Epoch 28/500\n",
      "0s - loss: 3.0415 - acc: 0.0435\n",
      "Epoch 29/500\n",
      "0s - loss: 3.0344 - acc: 0.0435\n",
      "Epoch 30/500\n",
      "0s - loss: 3.0278 - acc: 0.0435\n",
      "Epoch 31/500\n",
      "0s - loss: 3.0194 - acc: 0.0435\n",
      "Epoch 32/500\n",
      "0s - loss: 3.0126 - acc: 0.0435\n",
      "Epoch 33/500\n",
      "0s - loss: 3.0065 - acc: 0.0870\n",
      "Epoch 34/500\n",
      "0s - loss: 2.9982 - acc: 0.0870\n",
      "Epoch 35/500\n",
      "0s - loss: 2.9923 - acc: 0.0870\n",
      "Epoch 36/500\n",
      "0s - loss: 2.9835 - acc: 0.0870\n",
      "Epoch 37/500\n",
      "0s - loss: 2.9766 - acc: 0.0870\n",
      "Epoch 38/500\n",
      "0s - loss: 2.9688 - acc: 0.0870\n",
      "Epoch 39/500\n",
      "0s - loss: 2.9598 - acc: 0.0870\n",
      "Epoch 40/500\n",
      "0s - loss: 2.9515 - acc: 0.0870\n",
      "Epoch 41/500\n",
      "0s - loss: 2.9428 - acc: 0.1304\n",
      "Epoch 42/500\n",
      "0s - loss: 2.9340 - acc: 0.1304\n",
      "Epoch 43/500\n",
      "0s - loss: 2.9240 - acc: 0.0870\n",
      "Epoch 44/500\n",
      "0s - loss: 2.9164 - acc: 0.0870\n",
      "Epoch 45/500\n",
      "0s - loss: 2.9064 - acc: 0.0870\n",
      "Epoch 46/500\n",
      "0s - loss: 2.8985 - acc: 0.0870\n",
      "Epoch 47/500\n",
      "0s - loss: 2.8861 - acc: 0.0870\n",
      "Epoch 48/500\n",
      "0s - loss: 2.8763 - acc: 0.0870\n",
      "Epoch 49/500\n",
      "0s - loss: 2.8667 - acc: 0.0870\n",
      "Epoch 50/500\n",
      "0s - loss: 2.8566 - acc: 0.0870\n",
      "Epoch 51/500\n",
      "0s - loss: 2.8468 - acc: 0.0870\n",
      "Epoch 52/500\n",
      "0s - loss: 2.8356 - acc: 0.0870\n",
      "Epoch 53/500\n",
      "0s - loss: 2.8265 - acc: 0.1304\n",
      "Epoch 54/500\n",
      "0s - loss: 2.8165 - acc: 0.1304\n",
      "Epoch 55/500\n",
      "0s - loss: 2.8058 - acc: 0.1304\n",
      "Epoch 56/500\n",
      "0s - loss: 2.7931 - acc: 0.1304\n",
      "Epoch 57/500\n",
      "0s - loss: 2.7835 - acc: 0.0870\n",
      "Epoch 58/500\n",
      "0s - loss: 2.7721 - acc: 0.0870\n",
      "Epoch 59/500\n",
      "0s - loss: 2.7623 - acc: 0.0870\n",
      "Epoch 60/500\n",
      "0s - loss: 2.7523 - acc: 0.0870\n",
      "Epoch 61/500\n",
      "0s - loss: 2.7384 - acc: 0.0870\n",
      "Epoch 62/500\n",
      "0s - loss: 2.7280 - acc: 0.0870\n",
      "Epoch 63/500\n",
      "0s - loss: 2.7193 - acc: 0.0870\n",
      "Epoch 64/500\n",
      "0s - loss: 2.7072 - acc: 0.0870\n",
      "Epoch 65/500\n",
      "0s - loss: 2.6967 - acc: 0.0870\n",
      "Epoch 66/500\n",
      "0s - loss: 2.6881 - acc: 0.0870\n",
      "Epoch 67/500\n",
      "0s - loss: 2.6768 - acc: 0.0870\n",
      "Epoch 68/500\n",
      "0s - loss: 2.6687 - acc: 0.0870\n",
      "Epoch 69/500\n",
      "0s - loss: 2.6567 - acc: 0.0870\n",
      "Epoch 70/500\n",
      "0s - loss: 2.6471 - acc: 0.0870\n",
      "Epoch 71/500\n",
      "0s - loss: 2.6384 - acc: 0.0870\n",
      "Epoch 72/500\n",
      "0s - loss: 2.6290 - acc: 0.0870\n",
      "Epoch 73/500\n",
      "0s - loss: 2.6186 - acc: 0.0870\n",
      "Epoch 74/500\n",
      "0s - loss: 2.6110 - acc: 0.0870\n",
      "Epoch 75/500\n",
      "0s - loss: 2.6023 - acc: 0.0870\n",
      "Epoch 76/500\n",
      "0s - loss: 2.5918 - acc: 0.0870\n",
      "Epoch 77/500\n",
      "0s - loss: 2.5848 - acc: 0.1304\n",
      "Epoch 78/500\n",
      "0s - loss: 2.5762 - acc: 0.1304\n",
      "Epoch 79/500\n",
      "0s - loss: 2.5689 - acc: 0.0870\n",
      "Epoch 80/500\n",
      "0s - loss: 2.5618 - acc: 0.0870\n",
      "Epoch 81/500\n",
      "0s - loss: 2.5545 - acc: 0.0870\n",
      "Epoch 82/500\n",
      "0s - loss: 2.5475 - acc: 0.1304\n",
      "Epoch 83/500\n",
      "0s - loss: 2.5391 - acc: 0.1304\n",
      "Epoch 84/500\n",
      "0s - loss: 2.5333 - acc: 0.1304\n",
      "Epoch 85/500\n",
      "0s - loss: 2.5254 - acc: 0.1304\n",
      "Epoch 86/500\n",
      "0s - loss: 2.5196 - acc: 0.0870\n",
      "Epoch 87/500\n",
      "0s - loss: 2.5133 - acc: 0.1739\n",
      "Epoch 88/500\n",
      "0s - loss: 2.5060 - acc: 0.1304\n",
      "Epoch 89/500\n",
      "0s - loss: 2.5001 - acc: 0.1739\n",
      "Epoch 90/500\n",
      "0s - loss: 2.4935 - acc: 0.1739\n",
      "Epoch 91/500\n",
      "0s - loss: 2.4895 - acc: 0.1739\n",
      "Epoch 92/500\n",
      "0s - loss: 2.4819 - acc: 0.1739\n",
      "Epoch 93/500\n",
      "0s - loss: 2.4768 - acc: 0.1304\n",
      "Epoch 94/500\n",
      "0s - loss: 2.4692 - acc: 0.1739\n",
      "Epoch 95/500\n",
      "0s - loss: 2.4651 - acc: 0.1739\n",
      "Epoch 96/500\n",
      "0s - loss: 2.4591 - acc: 0.1739\n",
      "Epoch 97/500\n",
      "0s - loss: 2.4522 - acc: 0.1739\n",
      "Epoch 98/500\n",
      "0s - loss: 2.4471 - acc: 0.1739\n",
      "Epoch 99/500\n",
      "0s - loss: 2.4433 - acc: 0.1739\n",
      "Epoch 100/500\n",
      "0s - loss: 2.4371 - acc: 0.1739\n",
      "Epoch 101/500\n",
      "0s - loss: 2.4298 - acc: 0.1739\n",
      "Epoch 102/500\n",
      "0s - loss: 2.4255 - acc: 0.1304\n",
      "Epoch 103/500\n",
      "0s - loss: 2.4197 - acc: 0.1739\n",
      "Epoch 104/500\n",
      "0s - loss: 2.4138 - acc: 0.1739\n",
      "Epoch 105/500\n",
      "0s - loss: 2.4100 - acc: 0.1739\n",
      "Epoch 106/500\n",
      "0s - loss: 2.4045 - acc: 0.1739\n",
      "Epoch 107/500\n",
      "0s - loss: 2.4010 - acc: 0.1739\n",
      "Epoch 108/500\n",
      "0s - loss: 2.3953 - acc: 0.1304\n",
      "Epoch 109/500\n",
      "0s - loss: 2.3891 - acc: 0.1739\n",
      "Epoch 110/500\n",
      "0s - loss: 2.3840 - acc: 0.1739\n",
      "Epoch 111/500\n",
      "0s - loss: 2.3788 - acc: 0.1739\n",
      "Epoch 112/500\n",
      "0s - loss: 2.3717 - acc: 0.1304\n",
      "Epoch 113/500\n",
      "0s - loss: 2.3693 - acc: 0.1739\n",
      "Epoch 114/500\n",
      "0s - loss: 2.3632 - acc: 0.1739\n",
      "Epoch 115/500\n",
      "0s - loss: 2.3585 - acc: 0.1304\n",
      "Epoch 116/500\n",
      "0s - loss: 2.3561 - acc: 0.1739\n",
      "Epoch 117/500\n",
      "0s - loss: 2.3491 - acc: 0.1739\n",
      "Epoch 118/500\n",
      "0s - loss: 2.3440 - acc: 0.1739\n",
      "Epoch 119/500\n",
      "0s - loss: 2.3395 - acc: 0.1739\n",
      "Epoch 120/500\n",
      "0s - loss: 2.3364 - acc: 0.1304\n",
      "Epoch 121/500\n",
      "0s - loss: 2.3312 - acc: 0.1739\n",
      "Epoch 122/500\n",
      "0s - loss: 2.3270 - acc: 0.1739\n",
      "Epoch 123/500\n",
      "0s - loss: 2.3213 - acc: 0.1304\n",
      "Epoch 124/500\n",
      "0s - loss: 2.3162 - acc: 0.1739\n",
      "Epoch 125/500\n",
      "0s - loss: 2.3111 - acc: 0.1304\n",
      "Epoch 126/500\n",
      "0s - loss: 2.3083 - acc: 0.1304\n",
      "Epoch 127/500\n",
      "0s - loss: 2.3059 - acc: 0.1739\n",
      "Epoch 128/500\n",
      "0s - loss: 2.2996 - acc: 0.1739\n",
      "Epoch 129/500\n",
      "0s - loss: 2.2952 - acc: 0.1304\n",
      "Epoch 130/500\n",
      "0s - loss: 2.2906 - acc: 0.1304\n",
      "Epoch 131/500\n",
      "0s - loss: 2.2864 - acc: 0.1739\n",
      "Epoch 132/500\n",
      "0s - loss: 2.2826 - acc: 0.1304\n",
      "Epoch 133/500\n",
      "0s - loss: 2.2785 - acc: 0.1739\n",
      "Epoch 134/500\n",
      "0s - loss: 2.2770 - acc: 0.1739\n",
      "Epoch 135/500\n",
      "0s - loss: 2.2708 - acc: 0.1304\n",
      "Epoch 136/500\n",
      "0s - loss: 2.2658 - acc: 0.1304\n",
      "Epoch 137/500\n",
      "0s - loss: 2.2619 - acc: 0.1304\n",
      "Epoch 138/500\n",
      "0s - loss: 2.2577 - acc: 0.1304\n",
      "Epoch 139/500\n",
      "0s - loss: 2.2552 - acc: 0.1739\n",
      "Epoch 140/500\n",
      "0s - loss: 2.2505 - acc: 0.1739\n",
      "Epoch 141/500\n",
      "0s - loss: 2.2459 - acc: 0.1739\n",
      "Epoch 142/500\n",
      "0s - loss: 2.2431 - acc: 0.1739\n",
      "Epoch 143/500\n",
      "0s - loss: 2.2395 - acc: 0.1739\n",
      "Epoch 144/500\n",
      "0s - loss: 2.2346 - acc: 0.1739\n",
      "Epoch 145/500\n",
      "0s - loss: 2.2310 - acc: 0.1739\n",
      "Epoch 146/500\n",
      "0s - loss: 2.2277 - acc: 0.1739\n",
      "Epoch 147/500\n",
      "0s - loss: 2.2233 - acc: 0.2609\n",
      "Epoch 148/500\n",
      "0s - loss: 2.2191 - acc: 0.1739\n",
      "Epoch 149/500\n",
      "0s - loss: 2.2186 - acc: 0.2174\n",
      "Epoch 150/500\n",
      "0s - loss: 2.2143 - acc: 0.1739\n",
      "Epoch 151/500\n",
      "0s - loss: 2.2100 - acc: 0.2174\n",
      "Epoch 152/500\n",
      "0s - loss: 2.2062 - acc: 0.2609\n",
      "Epoch 153/500\n",
      "0s - loss: 2.2024 - acc: 0.2609\n",
      "Epoch 154/500\n",
      "0s - loss: 2.2011 - acc: 0.2609\n",
      "Epoch 155/500\n",
      "0s - loss: 2.1949 - acc: 0.3043\n",
      "Epoch 156/500\n",
      "0s - loss: 2.1912 - acc: 0.3043\n",
      "Epoch 157/500\n",
      "0s - loss: 2.1887 - acc: 0.2609\n",
      "Epoch 158/500\n",
      "0s - loss: 2.1879 - acc: 0.2174\n",
      "Epoch 159/500\n",
      "0s - loss: 2.1835 - acc: 0.2174\n",
      "Epoch 160/500\n",
      "0s - loss: 2.1789 - acc: 0.2174\n",
      "Epoch 161/500\n",
      "0s - loss: 2.1774 - acc: 0.3043\n",
      "Epoch 162/500\n",
      "0s - loss: 2.1726 - acc: 0.1739\n",
      "Epoch 163/500\n",
      "0s - loss: 2.1695 - acc: 0.2174\n",
      "Epoch 164/500\n",
      "0s - loss: 2.1656 - acc: 0.2174\n",
      "Epoch 165/500\n",
      "0s - loss: 2.1621 - acc: 0.2609\n",
      "Epoch 166/500\n",
      "0s - loss: 2.1591 - acc: 0.2609\n",
      "Epoch 167/500\n",
      "0s - loss: 2.1582 - acc: 0.3043\n",
      "Epoch 168/500\n",
      "0s - loss: 2.1533 - acc: 0.2174\n",
      "Epoch 169/500\n",
      "0s - loss: 2.1509 - acc: 0.2174\n",
      "Epoch 170/500\n",
      "0s - loss: 2.1481 - acc: 0.2609\n",
      "Epoch 171/500\n",
      "0s - loss: 2.1443 - acc: 0.2609\n",
      "Epoch 172/500\n",
      "0s - loss: 2.1423 - acc: 0.2174\n",
      "Epoch 173/500\n",
      "0s - loss: 2.1377 - acc: 0.3043\n",
      "Epoch 174/500\n",
      "0s - loss: 2.1340 - acc: 0.3043\n",
      "Epoch 175/500\n",
      "0s - loss: 2.1334 - acc: 0.3043\n",
      "Epoch 176/500\n",
      "0s - loss: 2.1269 - acc: 0.3043\n",
      "Epoch 177/500\n",
      "0s - loss: 2.1253 - acc: 0.2174\n",
      "Epoch 178/500\n",
      "0s - loss: 2.1212 - acc: 0.2609\n",
      "Epoch 179/500\n",
      "0s - loss: 2.1192 - acc: 0.3043\n",
      "Epoch 180/500\n",
      "0s - loss: 2.1151 - acc: 0.2609\n",
      "Epoch 181/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 2.1134 - acc: 0.3478\n",
      "Epoch 182/500\n",
      "0s - loss: 2.1096 - acc: 0.3478\n",
      "Epoch 183/500\n",
      "0s - loss: 2.1056 - acc: 0.3043\n",
      "Epoch 184/500\n",
      "0s - loss: 2.1028 - acc: 0.3043\n",
      "Epoch 185/500\n",
      "0s - loss: 2.0994 - acc: 0.3913\n",
      "Epoch 186/500\n",
      "0s - loss: 2.0978 - acc: 0.3478\n",
      "Epoch 187/500\n",
      "0s - loss: 2.0940 - acc: 0.3478\n",
      "Epoch 188/500\n",
      "0s - loss: 2.0910 - acc: 0.3043\n",
      "Epoch 189/500\n",
      "0s - loss: 2.0874 - acc: 0.2609\n",
      "Epoch 190/500\n",
      "0s - loss: 2.0852 - acc: 0.3043\n",
      "Epoch 191/500\n",
      "0s - loss: 2.0832 - acc: 0.3913\n",
      "Epoch 192/500\n",
      "0s - loss: 2.0819 - acc: 0.3913\n",
      "Epoch 193/500\n",
      "0s - loss: 2.0769 - acc: 0.3478\n",
      "Epoch 194/500\n",
      "0s - loss: 2.0734 - acc: 0.3913\n",
      "Epoch 195/500\n",
      "0s - loss: 2.0715 - acc: 0.3913\n",
      "Epoch 196/500\n",
      "0s - loss: 2.0673 - acc: 0.2609\n",
      "Epoch 197/500\n",
      "0s - loss: 2.0646 - acc: 0.3478\n",
      "Epoch 198/500\n",
      "0s - loss: 2.0631 - acc: 0.3478\n",
      "Epoch 199/500\n",
      "0s - loss: 2.0586 - acc: 0.3913\n",
      "Epoch 200/500\n",
      "0s - loss: 2.0586 - acc: 0.4783\n",
      "Epoch 201/500\n",
      "0s - loss: 2.0542 - acc: 0.3478\n",
      "Epoch 202/500\n",
      "0s - loss: 2.0518 - acc: 0.3913\n",
      "Epoch 203/500\n",
      "0s - loss: 2.0502 - acc: 0.3478\n",
      "Epoch 204/500\n",
      "0s - loss: 2.0472 - acc: 0.3478\n",
      "Epoch 205/500\n",
      "0s - loss: 2.0434 - acc: 0.3913\n",
      "Epoch 206/500\n",
      "0s - loss: 2.0410 - acc: 0.3913\n",
      "Epoch 207/500\n",
      "0s - loss: 2.0377 - acc: 0.4783\n",
      "Epoch 208/500\n",
      "0s - loss: 2.0361 - acc: 0.3478\n",
      "Epoch 209/500\n",
      "0s - loss: 2.0341 - acc: 0.4783\n",
      "Epoch 210/500\n",
      "0s - loss: 2.0319 - acc: 0.3913\n",
      "Epoch 211/500\n",
      "0s - loss: 2.0263 - acc: 0.3478\n",
      "Epoch 212/500\n",
      "0s - loss: 2.0264 - acc: 0.3913\n",
      "Epoch 213/500\n",
      "0s - loss: 2.0225 - acc: 0.3478\n",
      "Epoch 214/500\n",
      "0s - loss: 2.0183 - acc: 0.4348\n",
      "Epoch 215/500\n",
      "0s - loss: 2.0168 - acc: 0.5217\n",
      "Epoch 216/500\n",
      "0s - loss: 2.0155 - acc: 0.4783\n",
      "Epoch 217/500\n",
      "0s - loss: 2.0109 - acc: 0.3913\n",
      "Epoch 218/500\n",
      "0s - loss: 2.0085 - acc: 0.5217\n",
      "Epoch 219/500\n",
      "0s - loss: 2.0065 - acc: 0.4783\n",
      "Epoch 220/500\n",
      "0s - loss: 2.0049 - acc: 0.4348\n",
      "Epoch 221/500\n",
      "0s - loss: 2.0012 - acc: 0.4348\n",
      "Epoch 222/500\n",
      "0s - loss: 2.0006 - acc: 0.4348\n",
      "Epoch 223/500\n",
      "0s - loss: 1.9981 - acc: 0.4348\n",
      "Epoch 224/500\n",
      "0s - loss: 1.9932 - acc: 0.4783\n",
      "Epoch 225/500\n",
      "0s - loss: 1.9935 - acc: 0.3913\n",
      "Epoch 226/500\n",
      "0s - loss: 1.9901 - acc: 0.3913\n",
      "Epoch 227/500\n",
      "0s - loss: 1.9856 - acc: 0.3913\n",
      "Epoch 228/500\n",
      "0s - loss: 1.9837 - acc: 0.4348\n",
      "Epoch 229/500\n",
      "0s - loss: 1.9817 - acc: 0.4783\n",
      "Epoch 230/500\n",
      "0s - loss: 1.9805 - acc: 0.5217\n",
      "Epoch 231/500\n",
      "0s - loss: 1.9771 - acc: 0.4783\n",
      "Epoch 232/500\n",
      "0s - loss: 1.9760 - acc: 0.5217\n",
      "Epoch 233/500\n",
      "0s - loss: 1.9738 - acc: 0.4348\n",
      "Epoch 234/500\n",
      "0s - loss: 1.9722 - acc: 0.4348\n",
      "Epoch 235/500\n",
      "0s - loss: 1.9696 - acc: 0.4783\n",
      "Epoch 236/500\n",
      "0s - loss: 1.9656 - acc: 0.3913\n",
      "Epoch 237/500\n",
      "0s - loss: 1.9633 - acc: 0.4783\n",
      "Epoch 238/500\n",
      "0s - loss: 1.9644 - acc: 0.5217\n",
      "Epoch 239/500\n",
      "0s - loss: 1.9589 - acc: 0.5217\n",
      "Epoch 240/500\n",
      "0s - loss: 1.9564 - acc: 0.4783\n",
      "Epoch 241/500\n",
      "0s - loss: 1.9569 - acc: 0.4783\n",
      "Epoch 242/500\n",
      "0s - loss: 1.9521 - acc: 0.5217\n",
      "Epoch 243/500\n",
      "0s - loss: 1.9499 - acc: 0.4783\n",
      "Epoch 244/500\n",
      "0s - loss: 1.9478 - acc: 0.4348\n",
      "Epoch 245/500\n",
      "0s - loss: 1.9447 - acc: 0.4783\n",
      "Epoch 246/500\n",
      "0s - loss: 1.9427 - acc: 0.4783\n",
      "Epoch 247/500\n",
      "0s - loss: 1.9387 - acc: 0.4348\n",
      "Epoch 248/500\n",
      "0s - loss: 1.9378 - acc: 0.6087\n",
      "Epoch 249/500\n",
      "0s - loss: 1.9361 - acc: 0.5652\n",
      "Epoch 250/500\n",
      "0s - loss: 1.9328 - acc: 0.4783\n",
      "Epoch 251/500\n",
      "0s - loss: 1.9312 - acc: 0.5217\n",
      "Epoch 252/500\n",
      "0s - loss: 1.9326 - acc: 0.4783\n",
      "Epoch 253/500\n",
      "0s - loss: 1.9288 - acc: 0.3913\n",
      "Epoch 254/500\n",
      "0s - loss: 1.9255 - acc: 0.4348\n",
      "Epoch 255/500\n",
      "0s - loss: 1.9237 - acc: 0.5652\n",
      "Epoch 256/500\n",
      "0s - loss: 1.9221 - acc: 0.5217\n",
      "Epoch 257/500\n",
      "0s - loss: 1.9190 - acc: 0.4348\n",
      "Epoch 258/500\n",
      "0s - loss: 1.9172 - acc: 0.5217\n",
      "Epoch 259/500\n",
      "0s - loss: 1.9148 - acc: 0.5217\n",
      "Epoch 260/500\n",
      "0s - loss: 1.9115 - acc: 0.5217\n",
      "Epoch 261/500\n",
      "0s - loss: 1.9114 - acc: 0.5217\n",
      "Epoch 262/500\n",
      "0s - loss: 1.9076 - acc: 0.5217\n",
      "Epoch 263/500\n",
      "0s - loss: 1.9067 - acc: 0.5652\n",
      "Epoch 264/500\n",
      "0s - loss: 1.9037 - acc: 0.5652\n",
      "Epoch 265/500\n",
      "0s - loss: 1.9038 - acc: 0.5217\n",
      "Epoch 266/500\n",
      "0s - loss: 1.8996 - acc: 0.5217\n",
      "Epoch 267/500\n",
      "0s - loss: 1.8961 - acc: 0.5652\n",
      "Epoch 268/500\n",
      "0s - loss: 1.8927 - acc: 0.5652\n",
      "Epoch 269/500\n",
      "0s - loss: 1.8963 - acc: 0.5652\n",
      "Epoch 270/500\n",
      "0s - loss: 1.8903 - acc: 0.5217\n",
      "Epoch 271/500\n",
      "0s - loss: 1.8894 - acc: 0.5652\n",
      "Epoch 272/500\n",
      "0s - loss: 1.8877 - acc: 0.6087\n",
      "Epoch 273/500\n",
      "0s - loss: 1.8843 - acc: 0.5652\n",
      "Epoch 274/500\n",
      "0s - loss: 1.8846 - acc: 0.5652\n",
      "Epoch 275/500\n",
      "0s - loss: 1.8830 - acc: 0.4783\n",
      "Epoch 276/500\n",
      "0s - loss: 1.8795 - acc: 0.5652\n",
      "Epoch 277/500\n",
      "0s - loss: 1.8775 - acc: 0.5217\n",
      "Epoch 278/500\n",
      "0s - loss: 1.8748 - acc: 0.4783\n",
      "Epoch 279/500\n",
      "0s - loss: 1.8736 - acc: 0.6087\n",
      "Epoch 280/500\n",
      "0s - loss: 1.8722 - acc: 0.5652\n",
      "Epoch 281/500\n",
      "0s - loss: 1.8685 - acc: 0.5652\n",
      "Epoch 282/500\n",
      "0s - loss: 1.8683 - acc: 0.5217\n",
      "Epoch 283/500\n",
      "0s - loss: 1.8651 - acc: 0.4783\n",
      "Epoch 284/500\n",
      "0s - loss: 1.8635 - acc: 0.5652\n",
      "Epoch 285/500\n",
      "0s - loss: 1.8630 - acc: 0.6087\n",
      "Epoch 286/500\n",
      "0s - loss: 1.8594 - acc: 0.5652\n",
      "Epoch 287/500\n",
      "0s - loss: 1.8581 - acc: 0.6087\n",
      "Epoch 288/500\n",
      "0s - loss: 1.8557 - acc: 0.5652\n",
      "Epoch 289/500\n",
      "0s - loss: 1.8535 - acc: 0.6087\n",
      "Epoch 290/500\n",
      "0s - loss: 1.8513 - acc: 0.5652\n",
      "Epoch 291/500\n",
      "0s - loss: 1.8499 - acc: 0.5217\n",
      "Epoch 292/500\n",
      "0s - loss: 1.8482 - acc: 0.6087\n",
      "Epoch 293/500\n",
      "0s - loss: 1.8468 - acc: 0.6087\n",
      "Epoch 294/500\n",
      "0s - loss: 1.8434 - acc: 0.6087\n",
      "Epoch 295/500\n",
      "0s - loss: 1.8418 - acc: 0.6522\n",
      "Epoch 296/500\n",
      "0s - loss: 1.8424 - acc: 0.5217\n",
      "Epoch 297/500\n",
      "0s - loss: 1.8412 - acc: 0.5652\n",
      "Epoch 298/500\n",
      "0s - loss: 1.8378 - acc: 0.6522\n",
      "Epoch 299/500\n",
      "0s - loss: 1.8364 - acc: 0.6087\n",
      "Epoch 300/500\n",
      "0s - loss: 1.8352 - acc: 0.6087\n",
      "Epoch 301/500\n",
      "0s - loss: 1.8321 - acc: 0.5652\n",
      "Epoch 302/500\n",
      "0s - loss: 1.8337 - acc: 0.5652\n",
      "Epoch 303/500\n",
      "0s - loss: 1.8294 - acc: 0.6522\n",
      "Epoch 304/500\n",
      "0s - loss: 1.8279 - acc: 0.5652\n",
      "Epoch 305/500\n",
      "0s - loss: 1.8245 - acc: 0.6087\n",
      "Epoch 306/500\n",
      "0s - loss: 1.8230 - acc: 0.6522\n",
      "Epoch 307/500\n",
      "0s - loss: 1.8202 - acc: 0.6522\n",
      "Epoch 308/500\n",
      "0s - loss: 1.8209 - acc: 0.6522\n",
      "Epoch 309/500\n",
      "0s - loss: 1.8193 - acc: 0.6522\n",
      "Epoch 310/500\n",
      "0s - loss: 1.8178 - acc: 0.5652\n",
      "Epoch 311/500\n",
      "0s - loss: 1.8164 - acc: 0.6522\n",
      "Epoch 312/500\n",
      "0s - loss: 1.8134 - acc: 0.6957\n",
      "Epoch 313/500\n",
      "0s - loss: 1.8119 - acc: 0.5652\n",
      "Epoch 314/500\n",
      "0s - loss: 1.8113 - acc: 0.6522\n",
      "Epoch 315/500\n",
      "0s - loss: 1.8101 - acc: 0.6522\n",
      "Epoch 316/500\n",
      "0s - loss: 1.8078 - acc: 0.6522\n",
      "Epoch 317/500\n",
      "0s - loss: 1.8053 - acc: 0.6957\n",
      "Epoch 318/500\n",
      "0s - loss: 1.8029 - acc: 0.6957\n",
      "Epoch 319/500\n",
      "0s - loss: 1.8039 - acc: 0.6957\n",
      "Epoch 320/500\n",
      "0s - loss: 1.7997 - acc: 0.6087\n",
      "Epoch 321/500\n",
      "0s - loss: 1.8007 - acc: 0.5652\n",
      "Epoch 322/500\n",
      "0s - loss: 1.7980 - acc: 0.6522\n",
      "Epoch 323/500\n",
      "0s - loss: 1.7944 - acc: 0.7391\n",
      "Epoch 324/500\n",
      "0s - loss: 1.7931 - acc: 0.6957\n",
      "Epoch 325/500\n",
      "0s - loss: 1.7939 - acc: 0.6957\n",
      "Epoch 326/500\n",
      "0s - loss: 1.7895 - acc: 0.6522\n",
      "Epoch 327/500\n",
      "0s - loss: 1.7914 - acc: 0.6087\n",
      "Epoch 328/500\n",
      "0s - loss: 1.7891 - acc: 0.5652\n",
      "Epoch 329/500\n",
      "0s - loss: 1.7871 - acc: 0.6522\n",
      "Epoch 330/500\n",
      "0s - loss: 1.7858 - acc: 0.7391\n",
      "Epoch 331/500\n",
      "0s - loss: 1.7827 - acc: 0.6957\n",
      "Epoch 332/500\n",
      "0s - loss: 1.7828 - acc: 0.6522\n",
      "Epoch 333/500\n",
      "0s - loss: 1.7796 - acc: 0.6522\n",
      "Epoch 334/500\n",
      "0s - loss: 1.7806 - acc: 0.6087\n",
      "Epoch 335/500\n",
      "0s - loss: 1.7775 - acc: 0.6087\n",
      "Epoch 336/500\n",
      "0s - loss: 1.7770 - acc: 0.5652\n",
      "Epoch 337/500\n",
      "0s - loss: 1.7725 - acc: 0.6087\n",
      "Epoch 338/500\n",
      "0s - loss: 1.7738 - acc: 0.6522\n",
      "Epoch 339/500\n",
      "0s - loss: 1.7715 - acc: 0.5652\n",
      "Epoch 340/500\n",
      "0s - loss: 1.7705 - acc: 0.6087\n",
      "Epoch 341/500\n",
      "0s - loss: 1.7695 - acc: 0.6522\n",
      "Epoch 342/500\n",
      "0s - loss: 1.7655 - acc: 0.6957\n",
      "Epoch 343/500\n",
      "0s - loss: 1.7688 - acc: 0.6087\n",
      "Epoch 344/500\n",
      "0s - loss: 1.7652 - acc: 0.5652\n",
      "Epoch 345/500\n",
      "0s - loss: 1.7641 - acc: 0.6522\n",
      "Epoch 346/500\n",
      "0s - loss: 1.7605 - acc: 0.6522\n",
      "Epoch 347/500\n",
      "0s - loss: 1.7607 - acc: 0.6957\n",
      "Epoch 348/500\n",
      "0s - loss: 1.7568 - acc: 0.6087\n",
      "Epoch 349/500\n",
      "0s - loss: 1.7581 - acc: 0.6522\n",
      "Epoch 350/500\n",
      "0s - loss: 1.7545 - acc: 0.6957\n",
      "Epoch 351/500\n",
      "0s - loss: 1.7562 - acc: 0.6087\n",
      "Epoch 352/500\n",
      "0s - loss: 1.7538 - acc: 0.6522\n",
      "Epoch 353/500\n",
      "0s - loss: 1.7509 - acc: 0.6957\n",
      "Epoch 354/500\n",
      "0s - loss: 1.7512 - acc: 0.6522\n",
      "Epoch 355/500\n",
      "0s - loss: 1.7499 - acc: 0.6522\n",
      "Epoch 356/500\n",
      "0s - loss: 1.7488 - acc: 0.6957\n",
      "Epoch 357/500\n",
      "0s - loss: 1.7464 - acc: 0.6957\n",
      "Epoch 358/500\n",
      "0s - loss: 1.7454 - acc: 0.6957\n",
      "Epoch 359/500\n",
      "0s - loss: 1.7406 - acc: 0.6087\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 1.7415 - acc: 0.6087\n",
      "Epoch 361/500\n",
      "0s - loss: 1.7409 - acc: 0.7391\n",
      "Epoch 362/500\n",
      "0s - loss: 1.7375 - acc: 0.7391\n",
      "Epoch 363/500\n",
      "0s - loss: 1.7377 - acc: 0.5652\n",
      "Epoch 364/500\n",
      "0s - loss: 1.7365 - acc: 0.6522\n",
      "Epoch 365/500\n",
      "0s - loss: 1.7344 - acc: 0.6087\n",
      "Epoch 366/500\n",
      "0s - loss: 1.7341 - acc: 0.7391\n",
      "Epoch 367/500\n",
      "0s - loss: 1.7310 - acc: 0.6957\n",
      "Epoch 368/500\n",
      "0s - loss: 1.7311 - acc: 0.6957\n",
      "Epoch 369/500\n",
      "0s - loss: 1.7290 - acc: 0.7826\n",
      "Epoch 370/500\n",
      "0s - loss: 1.7283 - acc: 0.6957\n",
      "Epoch 371/500\n",
      "0s - loss: 1.7278 - acc: 0.7391\n",
      "Epoch 372/500\n",
      "0s - loss: 1.7251 - acc: 0.6957\n",
      "Epoch 373/500\n",
      "0s - loss: 1.7233 - acc: 0.6957\n",
      "Epoch 374/500\n",
      "0s - loss: 1.7235 - acc: 0.6957\n",
      "Epoch 375/500\n",
      "0s - loss: 1.7202 - acc: 0.7391\n",
      "Epoch 376/500\n",
      "0s - loss: 1.7225 - acc: 0.6957\n",
      "Epoch 377/500\n",
      "0s - loss: 1.7192 - acc: 0.7391\n",
      "Epoch 378/500\n",
      "0s - loss: 1.7172 - acc: 0.6957\n",
      "Epoch 379/500\n",
      "0s - loss: 1.7161 - acc: 0.6522\n",
      "Epoch 380/500\n",
      "0s - loss: 1.7159 - acc: 0.7391\n",
      "Epoch 381/500\n",
      "0s - loss: 1.7156 - acc: 0.7391\n",
      "Epoch 382/500\n",
      "0s - loss: 1.7122 - acc: 0.6522\n",
      "Epoch 383/500\n",
      "0s - loss: 1.7117 - acc: 0.7391\n",
      "Epoch 384/500\n",
      "0s - loss: 1.7083 - acc: 0.6957\n",
      "Epoch 385/500\n",
      "0s - loss: 1.7113 - acc: 0.7391\n",
      "Epoch 386/500\n",
      "0s - loss: 1.7084 - acc: 0.7391\n",
      "Epoch 387/500\n",
      "0s - loss: 1.7058 - acc: 0.6957\n",
      "Epoch 388/500\n",
      "0s - loss: 1.7058 - acc: 0.6957\n",
      "Epoch 389/500\n",
      "0s - loss: 1.7030 - acc: 0.8261\n",
      "Epoch 390/500\n",
      "0s - loss: 1.7007 - acc: 0.7391\n",
      "Epoch 391/500\n",
      "0s - loss: 1.7018 - acc: 0.7826\n",
      "Epoch 392/500\n",
      "0s - loss: 1.7028 - acc: 0.7826\n",
      "Epoch 393/500\n",
      "0s - loss: 1.7000 - acc: 0.7826\n",
      "Epoch 394/500\n",
      "0s - loss: 1.6963 - acc: 0.7826\n",
      "Epoch 395/500\n",
      "0s - loss: 1.6961 - acc: 0.6957\n",
      "Epoch 396/500\n",
      "0s - loss: 1.6945 - acc: 0.6957\n",
      "Epoch 397/500\n",
      "0s - loss: 1.6922 - acc: 0.7826\n",
      "Epoch 398/500\n",
      "0s - loss: 1.6934 - acc: 0.7826\n",
      "Epoch 399/500\n",
      "0s - loss: 1.6894 - acc: 0.7391\n",
      "Epoch 400/500\n",
      "0s - loss: 1.6902 - acc: 0.7391\n",
      "Epoch 401/500\n",
      "0s - loss: 1.6879 - acc: 0.6957\n",
      "Epoch 402/500\n",
      "0s - loss: 1.6868 - acc: 0.8261\n",
      "Epoch 403/500\n",
      "0s - loss: 1.6876 - acc: 0.7391\n",
      "Epoch 404/500\n",
      "0s - loss: 1.6839 - acc: 0.7391\n",
      "Epoch 405/500\n",
      "0s - loss: 1.6849 - acc: 0.7391\n",
      "Epoch 406/500\n",
      "0s - loss: 1.6817 - acc: 0.8261\n",
      "Epoch 407/500\n",
      "0s - loss: 1.6818 - acc: 0.7391\n",
      "Epoch 408/500\n",
      "0s - loss: 1.6787 - acc: 0.7826\n",
      "Epoch 409/500\n",
      "0s - loss: 1.6802 - acc: 0.7391\n",
      "Epoch 410/500\n",
      "0s - loss: 1.6777 - acc: 0.7391\n",
      "Epoch 411/500\n",
      "0s - loss: 1.6770 - acc: 0.7391\n",
      "Epoch 412/500\n",
      "0s - loss: 1.6768 - acc: 0.6957\n",
      "Epoch 413/500\n",
      "0s - loss: 1.6746 - acc: 0.6957\n",
      "Epoch 414/500\n",
      "0s - loss: 1.6754 - acc: 0.6957\n",
      "Epoch 415/500\n",
      "0s - loss: 1.6719 - acc: 0.7826\n",
      "Epoch 416/500\n",
      "0s - loss: 1.6722 - acc: 0.7391\n",
      "Epoch 417/500\n",
      "0s - loss: 1.6693 - acc: 0.7391\n",
      "Epoch 418/500\n",
      "0s - loss: 1.6707 - acc: 0.6087\n",
      "Epoch 419/500\n",
      "0s - loss: 1.6658 - acc: 0.7826\n",
      "Epoch 420/500\n",
      "0s - loss: 1.6675 - acc: 0.6957\n",
      "Epoch 421/500\n",
      "0s - loss: 1.6648 - acc: 0.7826\n",
      "Epoch 422/500\n",
      "0s - loss: 1.6647 - acc: 0.7826\n",
      "Epoch 423/500\n",
      "0s - loss: 1.6601 - acc: 0.6957\n",
      "Epoch 424/500\n",
      "0s - loss: 1.6609 - acc: 0.7826\n",
      "Epoch 425/500\n",
      "0s - loss: 1.6621 - acc: 0.6957\n",
      "Epoch 426/500\n",
      "0s - loss: 1.6614 - acc: 0.6522\n",
      "Epoch 427/500\n",
      "0s - loss: 1.6595 - acc: 0.6957\n",
      "Epoch 428/500\n",
      "0s - loss: 1.6576 - acc: 0.7391\n",
      "Epoch 429/500\n",
      "0s - loss: 1.6579 - acc: 0.7391\n",
      "Epoch 430/500\n",
      "0s - loss: 1.6546 - acc: 0.8261\n",
      "Epoch 431/500\n",
      "0s - loss: 1.6549 - acc: 0.7826\n",
      "Epoch 432/500\n",
      "0s - loss: 1.6528 - acc: 0.8261\n",
      "Epoch 433/500\n",
      "0s - loss: 1.6519 - acc: 0.7826\n",
      "Epoch 434/500\n",
      "0s - loss: 1.6497 - acc: 0.7391\n",
      "Epoch 435/500\n",
      "0s - loss: 1.6483 - acc: 0.7391\n",
      "Epoch 436/500\n",
      "0s - loss: 1.6471 - acc: 0.7391\n",
      "Epoch 437/500\n",
      "0s - loss: 1.6479 - acc: 0.7826\n",
      "Epoch 438/500\n",
      "0s - loss: 1.6474 - acc: 0.7391\n",
      "Epoch 439/500\n",
      "0s - loss: 1.6466 - acc: 0.7391\n",
      "Epoch 440/500\n",
      "0s - loss: 1.6458 - acc: 0.7826\n",
      "Epoch 441/500\n",
      "0s - loss: 1.6446 - acc: 0.8261\n",
      "Epoch 442/500\n",
      "0s - loss: 1.6432 - acc: 0.7826\n",
      "Epoch 443/500\n",
      "0s - loss: 1.6394 - acc: 0.8261\n",
      "Epoch 444/500\n",
      "0s - loss: 1.6381 - acc: 0.8261\n",
      "Epoch 445/500\n",
      "0s - loss: 1.6378 - acc: 0.7391\n",
      "Epoch 446/500\n",
      "0s - loss: 1.6374 - acc: 0.7391\n",
      "Epoch 447/500\n",
      "0s - loss: 1.6367 - acc: 0.7826\n",
      "Epoch 448/500\n",
      "0s - loss: 1.6357 - acc: 0.7826\n",
      "Epoch 449/500\n",
      "0s - loss: 1.6338 - acc: 0.7826\n",
      "Epoch 450/500\n",
      "0s - loss: 1.6334 - acc: 0.7391\n",
      "Epoch 451/500\n",
      "0s - loss: 1.6302 - acc: 0.7391\n",
      "Epoch 452/500\n",
      "0s - loss: 1.6339 - acc: 0.6957\n",
      "Epoch 453/500\n",
      "0s - loss: 1.6303 - acc: 0.7391\n",
      "Epoch 454/500\n",
      "0s - loss: 1.6282 - acc: 0.7826\n",
      "Epoch 455/500\n",
      "0s - loss: 1.6300 - acc: 0.7391\n",
      "Epoch 456/500\n",
      "0s - loss: 1.6266 - acc: 0.7826\n",
      "Epoch 457/500\n",
      "0s - loss: 1.6277 - acc: 0.8261\n",
      "Epoch 458/500\n",
      "0s - loss: 1.6269 - acc: 0.7391\n",
      "Epoch 459/500\n",
      "0s - loss: 1.6243 - acc: 0.6957\n",
      "Epoch 460/500\n",
      "0s - loss: 1.6228 - acc: 0.7826\n",
      "Epoch 461/500\n",
      "0s - loss: 1.6219 - acc: 0.7826\n",
      "Epoch 462/500\n",
      "0s - loss: 1.6200 - acc: 0.7391\n",
      "Epoch 463/500\n",
      "0s - loss: 1.6208 - acc: 0.7826\n",
      "Epoch 464/500\n",
      "0s - loss: 1.6187 - acc: 0.7391\n",
      "Epoch 465/500\n",
      "0s - loss: 1.6162 - acc: 0.7391\n",
      "Epoch 466/500\n",
      "0s - loss: 1.6170 - acc: 0.7826\n",
      "Epoch 467/500\n",
      "0s - loss: 1.6143 - acc: 0.7826\n",
      "Epoch 468/500\n",
      "0s - loss: 1.6128 - acc: 0.7826\n",
      "Epoch 469/500\n",
      "0s - loss: 1.6129 - acc: 0.7826\n",
      "Epoch 470/500\n",
      "0s - loss: 1.6121 - acc: 0.6957\n",
      "Epoch 471/500\n",
      "0s - loss: 1.6115 - acc: 0.7391\n",
      "Epoch 472/500\n",
      "0s - loss: 1.6097 - acc: 0.8261\n",
      "Epoch 473/500\n",
      "0s - loss: 1.6106 - acc: 0.8261\n",
      "Epoch 474/500\n",
      "0s - loss: 1.6061 - acc: 0.8261\n",
      "Epoch 475/500\n",
      "0s - loss: 1.6092 - acc: 0.8261\n",
      "Epoch 476/500\n",
      "0s - loss: 1.6053 - acc: 0.7826\n",
      "Epoch 477/500\n",
      "0s - loss: 1.6029 - acc: 0.7826\n",
      "Epoch 478/500\n",
      "0s - loss: 1.6015 - acc: 0.8261\n",
      "Epoch 479/500\n",
      "0s - loss: 1.6039 - acc: 0.7826\n",
      "Epoch 480/500\n",
      "0s - loss: 1.6016 - acc: 0.7826\n",
      "Epoch 481/500\n",
      "0s - loss: 1.5991 - acc: 0.7391\n",
      "Epoch 482/500\n",
      "0s - loss: 1.5993 - acc: 0.6957\n",
      "Epoch 483/500\n",
      "0s - loss: 1.5974 - acc: 0.8261\n",
      "Epoch 484/500\n",
      "0s - loss: 1.5976 - acc: 0.7826\n",
      "Epoch 485/500\n",
      "0s - loss: 1.5959 - acc: 0.8261\n",
      "Epoch 486/500\n",
      "0s - loss: 1.5978 - acc: 0.7391\n",
      "Epoch 487/500\n",
      "0s - loss: 1.5922 - acc: 0.7391\n",
      "Epoch 488/500\n",
      "0s - loss: 1.5929 - acc: 0.8696\n",
      "Epoch 489/500\n",
      "0s - loss: 1.5916 - acc: 0.8261\n",
      "Epoch 490/500\n",
      "0s - loss: 1.5900 - acc: 0.8261\n",
      "Epoch 491/500\n",
      "0s - loss: 1.5906 - acc: 0.8261\n",
      "Epoch 492/500\n",
      "0s - loss: 1.5911 - acc: 0.7826\n",
      "Epoch 493/500\n",
      "0s - loss: 1.5879 - acc: 0.7826\n",
      "Epoch 494/500\n",
      "0s - loss: 1.5876 - acc: 0.8261\n",
      "Epoch 495/500\n",
      "0s - loss: 1.5885 - acc: 0.7826\n",
      "Epoch 496/500\n",
      "0s - loss: 1.5856 - acc: 0.8261\n",
      "Epoch 497/500\n",
      "0s - loss: 1.5861 - acc: 0.7826\n",
      "Epoch 498/500\n",
      "0s - loss: 1.5846 - acc: 0.7391\n",
      "Epoch 499/500\n",
      "0s - loss: 1.5847 - acc: 0.7826\n",
      "Epoch 500/500\n",
      "0s - loss: 1.5836 - acc: 0.7826\n",
      "Model Accuracy: 82.61%\n",
      "(['A', 'B', 'C'], '->', 'D')\n",
      "(['B', 'C', 'D'], '->', 'E')\n",
      "(['C', 'D', 'E'], '->', 'F')\n",
      "(['D', 'E', 'F'], '->', 'G')\n",
      "(['E', 'F', 'G'], '->', 'H')\n",
      "(['F', 'G', 'H'], '->', 'I')\n",
      "(['G', 'H', 'I'], '->', 'J')\n",
      "(['H', 'I', 'J'], '->', 'K')\n",
      "(['I', 'J', 'K'], '->', 'L')\n",
      "(['J', 'K', 'L'], '->', 'M')\n",
      "(['K', 'L', 'M'], '->', 'N')\n",
      "(['L', 'M', 'N'], '->', 'O')\n",
      "(['M', 'N', 'O'], '->', 'P')\n",
      "(['N', 'O', 'P'], '->', 'Q')\n",
      "(['O', 'P', 'Q'], '->', 'R')\n",
      "(['P', 'Q', 'R'], '->', 'S')\n",
      "(['Q', 'R', 'S'], '->', 'T')\n",
      "(['R', 'S', 'T'], '->', 'U')\n",
      "(['S', 'T', 'U'], '->', 'W')\n",
      "(['T', 'U', 'V'], '->', 'X')\n",
      "(['U', 'V', 'W'], '->', 'Z')\n",
      "(['V', 'W', 'X'], '->', 'Z')\n",
      "(['W', 'X', 'Y'], '->', 'Z')\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation= 'softmax' ))\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "  x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "  x = x / float(len(alphabet))\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ABC', '->', 'D')\n",
      "('BCD', '->', 'E')\n",
      "('CDE', '->', 'F')\n",
      "('DEF', '->', 'G')\n",
      "('EFG', '->', 'H')\n",
      "('FGH', '->', 'I')\n",
      "('GHI', '->', 'J')\n",
      "('HIJ', '->', 'K')\n",
      "('IJK', '->', 'L')\n",
      "('JKL', '->', 'M')\n",
      "('KLM', '->', 'N')\n",
      "('LMN', '->', 'O')\n",
      "('MNO', '->', 'P')\n",
      "('NOP', '->', 'Q')\n",
      "('OPQ', '->', 'R')\n",
      "('PQR', '->', 'S')\n",
      "('QRS', '->', 'T')\n",
      "('RST', '->', 'U')\n",
      "('STU', '->', 'V')\n",
      "('TUV', '->', 'W')\n",
      "('UVW', '->', 'X')\n",
      "('VWX', '->', 'Y')\n",
      "('WXY', '->', 'Z')\n",
      "Epoch 1/500\n",
      "1s - loss: 3.2771 - acc: 0.0435\n",
      "Epoch 2/500\n",
      "0s - loss: 3.2605 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "0s - loss: 3.2522 - acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "0s - loss: 3.2442 - acc: 0.0435\n",
      "Epoch 5/500\n",
      "0s - loss: 3.2369 - acc: 0.0435\n",
      "Epoch 6/500\n",
      "0s - loss: 3.2287 - acc: 0.0435\n",
      "Epoch 7/500\n",
      "0s - loss: 3.2196 - acc: 0.0870\n",
      "Epoch 8/500\n",
      "0s - loss: 3.2097 - acc: 0.0435\n",
      "Epoch 9/500\n",
      "0s - loss: 3.2006 - acc: 0.0435\n",
      "Epoch 10/500\n",
      "0s - loss: 3.1865 - acc: 0.0435\n",
      "Epoch 11/500\n",
      "0s - loss: 3.1739 - acc: 0.0435\n",
      "Epoch 12/500\n",
      "0s - loss: 3.1576 - acc: 0.0435\n",
      "Epoch 13/500\n",
      "0s - loss: 3.1439 - acc: 0.0435\n",
      "Epoch 14/500\n",
      "0s - loss: 3.1250 - acc: 0.0435\n",
      "Epoch 15/500\n",
      "0s - loss: 3.1084 - acc: 0.0435\n",
      "Epoch 16/500\n",
      "0s - loss: 3.0870 - acc: 0.0435\n",
      "Epoch 17/500\n",
      "0s - loss: 3.0714 - acc: 0.0435\n",
      "Epoch 18/500\n",
      "0s - loss: 3.0554 - acc: 0.0435\n",
      "Epoch 19/500\n",
      "0s - loss: 3.0354 - acc: 0.0435\n",
      "Epoch 20/500\n",
      "0s - loss: 3.0177 - acc: 0.0435\n",
      "Epoch 21/500\n",
      "0s - loss: 2.9985 - acc: 0.0870\n",
      "Epoch 22/500\n",
      "0s - loss: 2.9821 - acc: 0.0870\n",
      "Epoch 23/500\n",
      "0s - loss: 2.9590 - acc: 0.0870\n",
      "Epoch 24/500\n",
      "0s - loss: 2.9374 - acc: 0.0870\n",
      "Epoch 25/500\n",
      "0s - loss: 2.9137 - acc: 0.1304\n",
      "Epoch 26/500\n",
      "0s - loss: 2.8891 - acc: 0.0870\n",
      "Epoch 27/500\n",
      "0s - loss: 2.8651 - acc: 0.0870\n",
      "Epoch 28/500\n",
      "0s - loss: 2.8376 - acc: 0.0870\n",
      "Epoch 29/500\n",
      "0s - loss: 2.8057 - acc: 0.0870\n",
      "Epoch 30/500\n",
      "0s - loss: 2.7822 - acc: 0.0870\n",
      "Epoch 31/500\n",
      "0s - loss: 2.7475 - acc: 0.0870\n",
      "Epoch 32/500\n",
      "0s - loss: 2.7168 - acc: 0.0870\n",
      "Epoch 33/500\n",
      "0s - loss: 2.6892 - acc: 0.0870\n",
      "Epoch 34/500\n",
      "0s - loss: 2.6612 - acc: 0.0870\n",
      "Epoch 35/500\n",
      "0s - loss: 2.6334 - acc: 0.1304\n",
      "Epoch 36/500\n",
      "0s - loss: 2.6141 - acc: 0.1304\n",
      "Epoch 37/500\n",
      "0s - loss: 2.5893 - acc: 0.1304\n",
      "Epoch 38/500\n",
      "0s - loss: 2.5618 - acc: 0.1304\n",
      "Epoch 39/500\n",
      "0s - loss: 2.5381 - acc: 0.1304\n",
      "Epoch 40/500\n",
      "0s - loss: 2.5225 - acc: 0.1304\n",
      "Epoch 41/500\n",
      "0s - loss: 2.4964 - acc: 0.1304\n",
      "Epoch 42/500\n",
      "0s - loss: 2.4802 - acc: 0.1304\n",
      "Epoch 43/500\n",
      "0s - loss: 2.4620 - acc: 0.1304\n",
      "Epoch 44/500\n",
      "0s - loss: 2.4435 - acc: 0.1304\n",
      "Epoch 45/500\n",
      "0s - loss: 2.4255 - acc: 0.1304\n",
      "Epoch 46/500\n",
      "0s - loss: 2.4013 - acc: 0.1739\n",
      "Epoch 47/500\n",
      "0s - loss: 2.3919 - acc: 0.1304\n",
      "Epoch 48/500\n",
      "0s - loss: 2.3632 - acc: 0.1739\n",
      "Epoch 49/500\n",
      "0s - loss: 2.3528 - acc: 0.1739\n",
      "Epoch 50/500\n",
      "0s - loss: 2.3241 - acc: 0.1739\n",
      "Epoch 51/500\n",
      "0s - loss: 2.3098 - acc: 0.1739\n",
      "Epoch 52/500\n",
      "0s - loss: 2.2857 - acc: 0.1739\n",
      "Epoch 53/500\n",
      "0s - loss: 2.2662 - acc: 0.1739\n",
      "Epoch 54/500\n",
      "0s - loss: 2.2490 - acc: 0.1739\n",
      "Epoch 55/500\n",
      "0s - loss: 2.2322 - acc: 0.1739\n",
      "Epoch 56/500\n",
      "0s - loss: 2.2131 - acc: 0.2174\n",
      "Epoch 57/500\n",
      "0s - loss: 2.1929 - acc: 0.2609\n",
      "Epoch 58/500\n",
      "0s - loss: 2.1730 - acc: 0.2174\n",
      "Epoch 59/500\n",
      "0s - loss: 2.1575 - acc: 0.2174\n",
      "Epoch 60/500\n",
      "0s - loss: 2.1403 - acc: 0.3043\n",
      "Epoch 61/500\n",
      "0s - loss: 2.1166 - acc: 0.2174\n",
      "Epoch 62/500\n",
      "0s - loss: 2.0947 - acc: 0.2609\n",
      "Epoch 63/500\n",
      "0s - loss: 2.0842 - acc: 0.2609\n",
      "Epoch 64/500\n",
      "0s - loss: 2.0666 - acc: 0.3043\n",
      "Epoch 65/500\n",
      "0s - loss: 2.0476 - acc: 0.3478\n",
      "Epoch 66/500\n",
      "0s - loss: 2.0353 - acc: 0.2609\n",
      "Epoch 67/500\n",
      "0s - loss: 2.0066 - acc: 0.3043\n",
      "Epoch 68/500\n",
      "0s - loss: 1.9982 - acc: 0.2174\n",
      "Epoch 69/500\n",
      "0s - loss: 1.9769 - acc: 0.3043\n",
      "Epoch 70/500\n",
      "0s - loss: 1.9644 - acc: 0.3913\n",
      "Epoch 71/500\n",
      "0s - loss: 1.9412 - acc: 0.3478\n",
      "Epoch 72/500\n",
      "0s - loss: 1.9304 - acc: 0.3913\n",
      "Epoch 73/500\n",
      "0s - loss: 1.9101 - acc: 0.2609\n",
      "Epoch 74/500\n",
      "0s - loss: 1.9079 - acc: 0.3478\n",
      "Epoch 75/500\n",
      "0s - loss: 1.8818 - acc: 0.3043\n",
      "Epoch 76/500\n",
      "0s - loss: 1.8718 - acc: 0.2609\n",
      "Epoch 77/500\n",
      "0s - loss: 1.8571 - acc: 0.3913\n",
      "Epoch 78/500\n",
      "0s - loss: 1.8409 - acc: 0.4348\n",
      "Epoch 79/500\n",
      "0s - loss: 1.8289 - acc: 0.3478\n",
      "Epoch 80/500\n",
      "0s - loss: 1.8220 - acc: 0.2609\n",
      "Epoch 81/500\n",
      "0s - loss: 1.8021 - acc: 0.4348\n",
      "Epoch 82/500\n",
      "0s - loss: 1.7949 - acc: 0.4348\n",
      "Epoch 83/500\n",
      "0s - loss: 1.7781 - acc: 0.4348\n",
      "Epoch 84/500\n",
      "0s - loss: 1.7691 - acc: 0.4783\n",
      "Epoch 85/500\n",
      "0s - loss: 1.7617 - acc: 0.4348\n",
      "Epoch 86/500\n",
      "0s - loss: 1.7504 - acc: 0.4348\n",
      "Epoch 87/500\n",
      "0s - loss: 1.7392 - acc: 0.4348\n",
      "Epoch 88/500\n",
      "0s - loss: 1.7334 - acc: 0.3913\n",
      "Epoch 89/500\n",
      "0s - loss: 1.7197 - acc: 0.5217\n",
      "Epoch 90/500\n",
      "0s - loss: 1.7086 - acc: 0.5217\n",
      "Epoch 91/500\n",
      "0s - loss: 1.6956 - acc: 0.5652\n",
      "Epoch 92/500\n",
      "0s - loss: 1.6921 - acc: 0.5652\n",
      "Epoch 93/500\n",
      "0s - loss: 1.6839 - acc: 0.4348\n",
      "Epoch 94/500\n",
      "0s - loss: 1.6738 - acc: 0.6087\n",
      "Epoch 95/500\n",
      "0s - loss: 1.6620 - acc: 0.4783\n",
      "Epoch 96/500\n",
      "0s - loss: 1.6543 - acc: 0.4783\n",
      "Epoch 97/500\n",
      "0s - loss: 1.6434 - acc: 0.4783\n",
      "Epoch 98/500\n",
      "0s - loss: 1.6353 - acc: 0.5652\n",
      "Epoch 99/500\n",
      "0s - loss: 1.6289 - acc: 0.5217\n",
      "Epoch 100/500\n",
      "0s - loss: 1.6184 - acc: 0.6522\n",
      "Epoch 101/500\n",
      "0s - loss: 1.6085 - acc: 0.5217\n",
      "Epoch 102/500\n",
      "0s - loss: 1.5964 - acc: 0.6087\n",
      "Epoch 103/500\n",
      "0s - loss: 1.5862 - acc: 0.6957\n",
      "Epoch 104/500\n",
      "0s - loss: 1.5813 - acc: 0.5652\n",
      "Epoch 105/500\n",
      "0s - loss: 1.5796 - acc: 0.6087\n",
      "Epoch 106/500\n",
      "0s - loss: 1.5665 - acc: 0.6522\n",
      "Epoch 107/500\n",
      "0s - loss: 1.5587 - acc: 0.6957\n",
      "Epoch 108/500\n",
      "0s - loss: 1.5539 - acc: 0.6522\n",
      "Epoch 109/500\n",
      "0s - loss: 1.5500 - acc: 0.5217\n",
      "Epoch 110/500\n",
      "0s - loss: 1.5419 - acc: 0.6087\n",
      "Epoch 111/500\n",
      "0s - loss: 1.5311 - acc: 0.6087\n",
      "Epoch 112/500\n",
      "0s - loss: 1.5234 - acc: 0.7391\n",
      "Epoch 113/500\n",
      "0s - loss: 1.5138 - acc: 0.6522\n",
      "Epoch 114/500\n",
      "0s - loss: 1.5105 - acc: 0.6087\n",
      "Epoch 115/500\n",
      "0s - loss: 1.5017 - acc: 0.7391\n",
      "Epoch 116/500\n",
      "0s - loss: 1.4990 - acc: 0.6087\n",
      "Epoch 117/500\n",
      "0s - loss: 1.4913 - acc: 0.7391\n",
      "Epoch 118/500\n",
      "0s - loss: 1.4853 - acc: 0.6957\n",
      "Epoch 119/500\n",
      "0s - loss: 1.4757 - acc: 0.6522\n",
      "Epoch 120/500\n",
      "0s - loss: 1.4712 - acc: 0.7391\n",
      "Epoch 121/500\n",
      "0s - loss: 1.4699 - acc: 0.6522\n",
      "Epoch 122/500\n",
      "0s - loss: 1.4568 - acc: 0.7391\n",
      "Epoch 123/500\n",
      "0s - loss: 1.4520 - acc: 0.6957\n",
      "Epoch 124/500\n",
      "0s - loss: 1.4445 - acc: 0.6522\n",
      "Epoch 125/500\n",
      "0s - loss: 1.4420 - acc: 0.6957\n",
      "Epoch 126/500\n",
      "0s - loss: 1.4335 - acc: 0.6957\n",
      "Epoch 127/500\n",
      "0s - loss: 1.4342 - acc: 0.6957\n",
      "Epoch 128/500\n",
      "0s - loss: 1.4196 - acc: 0.7391\n",
      "Epoch 129/500\n",
      "0s - loss: 1.4131 - acc: 0.7826\n",
      "Epoch 130/500\n",
      "0s - loss: 1.4099 - acc: 0.6522\n",
      "Epoch 131/500\n",
      "0s - loss: 1.4049 - acc: 0.6522\n",
      "Epoch 132/500\n",
      "0s - loss: 1.3978 - acc: 0.6957\n",
      "Epoch 133/500\n",
      "0s - loss: 1.3944 - acc: 0.6957\n",
      "Epoch 134/500\n",
      "0s - loss: 1.3917 - acc: 0.6522\n",
      "Epoch 135/500\n",
      "0s - loss: 1.3754 - acc: 0.7826\n",
      "Epoch 136/500\n",
      "0s - loss: 1.3691 - acc: 0.7826\n",
      "Epoch 137/500\n",
      "0s - loss: 1.3678 - acc: 0.7826\n",
      "Epoch 138/500\n",
      "0s - loss: 1.3584 - acc: 0.7826\n",
      "Epoch 139/500\n",
      "0s - loss: 1.3545 - acc: 0.7391\n",
      "Epoch 140/500\n",
      "0s - loss: 1.3412 - acc: 0.7826\n",
      "Epoch 141/500\n",
      "0s - loss: 1.3399 - acc: 0.7826\n",
      "Epoch 142/500\n",
      "0s - loss: 1.3341 - acc: 0.7826\n",
      "Epoch 143/500\n",
      "0s - loss: 1.3359 - acc: 0.7826\n",
      "Epoch 144/500\n",
      "0s - loss: 1.3222 - acc: 0.7826\n",
      "Epoch 145/500\n",
      "0s - loss: 1.3185 - acc: 0.7826\n",
      "Epoch 146/500\n",
      "0s - loss: 1.3175 - acc: 0.7826\n",
      "Epoch 147/500\n",
      "0s - loss: 1.3090 - acc: 0.7826\n",
      "Epoch 148/500\n",
      "0s - loss: 1.3013 - acc: 0.7826\n",
      "Epoch 149/500\n",
      "0s - loss: 1.2941 - acc: 0.8261\n",
      "Epoch 150/500\n",
      "0s - loss: 1.2940 - acc: 0.8261\n",
      "Epoch 151/500\n",
      "0s - loss: 1.2877 - acc: 0.8261\n",
      "Epoch 152/500\n",
      "0s - loss: 1.2842 - acc: 0.8261\n",
      "Epoch 153/500\n",
      "0s - loss: 1.2689 - acc: 0.8261\n",
      "Epoch 154/500\n",
      "0s - loss: 1.2639 - acc: 0.8261\n",
      "Epoch 155/500\n",
      "0s - loss: 1.2582 - acc: 0.8261\n",
      "Epoch 156/500\n",
      "0s - loss: 1.2518 - acc: 0.8261\n",
      "Epoch 157/500\n",
      "0s - loss: 1.2523 - acc: 0.8261\n",
      "Epoch 158/500\n",
      "0s - loss: 1.2462 - acc: 0.8261\n",
      "Epoch 159/500\n",
      "0s - loss: 1.2426 - acc: 0.7826\n",
      "Epoch 160/500\n",
      "0s - loss: 1.2380 - acc: 0.7826\n",
      "Epoch 161/500\n",
      "0s - loss: 1.2310 - acc: 0.7826\n",
      "Epoch 162/500\n",
      "0s - loss: 1.2285 - acc: 0.7826\n",
      "Epoch 163/500\n",
      "0s - loss: 1.2257 - acc: 0.7826\n",
      "Epoch 164/500\n",
      "0s - loss: 1.2176 - acc: 0.8696\n",
      "Epoch 165/500\n",
      "0s - loss: 1.2116 - acc: 0.8696\n",
      "Epoch 166/500\n",
      "0s - loss: 1.2118 - acc: 0.7826\n",
      "Epoch 167/500\n",
      "0s - loss: 1.1996 - acc: 0.8696\n",
      "Epoch 168/500\n",
      "0s - loss: 1.1919 - acc: 0.8261\n",
      "Epoch 169/500\n",
      "0s - loss: 1.1890 - acc: 0.8261\n",
      "Epoch 170/500\n",
      "0s - loss: 1.1795 - acc: 0.8261\n",
      "Epoch 171/500\n",
      "0s - loss: 1.1783 - acc: 0.8261\n",
      "Epoch 172/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 1.1749 - acc: 0.8696\n",
      "Epoch 173/500\n",
      "0s - loss: 1.1678 - acc: 0.8261\n",
      "Epoch 174/500\n",
      "0s - loss: 1.1607 - acc: 0.8261\n",
      "Epoch 175/500\n",
      "0s - loss: 1.1572 - acc: 0.8696\n",
      "Epoch 176/500\n",
      "0s - loss: 1.1593 - acc: 0.8696\n",
      "Epoch 177/500\n",
      "0s - loss: 1.1606 - acc: 0.8261\n",
      "Epoch 178/500\n",
      "0s - loss: 1.1509 - acc: 0.8261\n",
      "Epoch 179/500\n",
      "0s - loss: 1.1393 - acc: 0.9130\n",
      "Epoch 180/500\n",
      "0s - loss: 1.1327 - acc: 0.8696\n",
      "Epoch 181/500\n",
      "0s - loss: 1.1313 - acc: 0.8261\n",
      "Epoch 182/500\n",
      "0s - loss: 1.1185 - acc: 0.8696\n",
      "Epoch 183/500\n",
      "0s - loss: 1.1220 - acc: 0.8261\n",
      "Epoch 184/500\n",
      "0s - loss: 1.1119 - acc: 0.8261\n",
      "Epoch 185/500\n",
      "0s - loss: 1.1121 - acc: 0.9130\n",
      "Epoch 186/500\n",
      "0s - loss: 1.1088 - acc: 0.8261\n",
      "Epoch 187/500\n",
      "0s - loss: 1.1066 - acc: 0.8261\n",
      "Epoch 188/500\n",
      "0s - loss: 1.0980 - acc: 0.8696\n",
      "Epoch 189/500\n",
      "0s - loss: 1.0941 - acc: 0.9130\n",
      "Epoch 190/500\n",
      "0s - loss: 1.0843 - acc: 0.8261\n",
      "Epoch 191/500\n",
      "0s - loss: 1.0837 - acc: 0.8261\n",
      "Epoch 192/500\n",
      "0s - loss: 1.0738 - acc: 0.9130\n",
      "Epoch 193/500\n",
      "0s - loss: 1.0740 - acc: 0.8696\n",
      "Epoch 194/500\n",
      "0s - loss: 1.0727 - acc: 0.8696\n",
      "Epoch 195/500\n",
      "0s - loss: 1.0690 - acc: 0.8261\n",
      "Epoch 196/500\n",
      "0s - loss: 1.0571 - acc: 0.8696\n",
      "Epoch 197/500\n",
      "0s - loss: 1.0501 - acc: 0.8696\n",
      "Epoch 198/500\n",
      "0s - loss: 1.0503 - acc: 0.8696\n",
      "Epoch 199/500\n",
      "0s - loss: 1.0471 - acc: 0.8696\n",
      "Epoch 200/500\n",
      "0s - loss: 1.0439 - acc: 0.8261\n",
      "Epoch 201/500\n",
      "0s - loss: 1.0353 - acc: 0.9130\n",
      "Epoch 202/500\n",
      "0s - loss: 1.0325 - acc: 0.9130\n",
      "Epoch 203/500\n",
      "0s - loss: 1.0254 - acc: 0.9130\n",
      "Epoch 204/500\n",
      "0s - loss: 1.0244 - acc: 0.9565\n",
      "Epoch 205/500\n",
      "0s - loss: 1.0167 - acc: 0.9130\n",
      "Epoch 206/500\n",
      "0s - loss: 1.0122 - acc: 0.8696\n",
      "Epoch 207/500\n",
      "0s - loss: 1.0076 - acc: 0.8696\n",
      "Epoch 208/500\n",
      "0s - loss: 1.0059 - acc: 0.8696\n",
      "Epoch 209/500\n",
      "0s - loss: 0.9985 - acc: 0.8696\n",
      "Epoch 210/500\n",
      "0s - loss: 0.9935 - acc: 0.9130\n",
      "Epoch 211/500\n",
      "0s - loss: 0.9887 - acc: 0.9130\n",
      "Epoch 212/500\n",
      "0s - loss: 0.9881 - acc: 0.8696\n",
      "Epoch 213/500\n",
      "0s - loss: 0.9858 - acc: 0.9130\n",
      "Epoch 214/500\n",
      "0s - loss: 0.9795 - acc: 0.9565\n",
      "Epoch 215/500\n",
      "0s - loss: 0.9732 - acc: 0.8261\n",
      "Epoch 216/500\n",
      "0s - loss: 0.9697 - acc: 0.8261\n",
      "Epoch 217/500\n",
      "0s - loss: 0.9599 - acc: 0.8696\n",
      "Epoch 218/500\n",
      "0s - loss: 0.9642 - acc: 0.9130\n",
      "Epoch 219/500\n",
      "0s - loss: 0.9528 - acc: 0.9130\n",
      "Epoch 220/500\n",
      "0s - loss: 0.9557 - acc: 0.8696\n",
      "Epoch 221/500\n",
      "0s - loss: 0.9482 - acc: 0.9130\n",
      "Epoch 222/500\n",
      "0s - loss: 0.9443 - acc: 0.9565\n",
      "Epoch 223/500\n",
      "0s - loss: 0.9459 - acc: 0.9565\n",
      "Epoch 224/500\n",
      "0s - loss: 0.9348 - acc: 0.9130\n",
      "Epoch 225/500\n",
      "0s - loss: 0.9331 - acc: 0.9130\n",
      "Epoch 226/500\n",
      "0s - loss: 0.9283 - acc: 0.8696\n",
      "Epoch 227/500\n",
      "0s - loss: 0.9269 - acc: 0.9565\n",
      "Epoch 228/500\n",
      "0s - loss: 0.9180 - acc: 0.9565\n",
      "Epoch 229/500\n",
      "0s - loss: 0.9158 - acc: 0.9565\n",
      "Epoch 230/500\n",
      "0s - loss: 0.9070 - acc: 0.9130\n",
      "Epoch 231/500\n",
      "0s - loss: 0.8988 - acc: 0.9130\n",
      "Epoch 232/500\n",
      "0s - loss: 0.9093 - acc: 0.9565\n",
      "Epoch 233/500\n",
      "0s - loss: 0.9024 - acc: 0.9565\n",
      "Epoch 234/500\n",
      "0s - loss: 0.8974 - acc: 0.9565\n",
      "Epoch 235/500\n",
      "0s - loss: 0.8883 - acc: 0.9565\n",
      "Epoch 236/500\n",
      "0s - loss: 0.8831 - acc: 0.8696\n",
      "Epoch 237/500\n",
      "0s - loss: 0.8831 - acc: 0.8696\n",
      "Epoch 238/500\n",
      "0s - loss: 0.8793 - acc: 0.9565\n",
      "Epoch 239/500\n",
      "0s - loss: 0.8719 - acc: 0.9565\n",
      "Epoch 240/500\n",
      "0s - loss: 0.8686 - acc: 0.8696\n",
      "Epoch 241/500\n",
      "0s - loss: 0.8605 - acc: 0.9130\n",
      "Epoch 242/500\n",
      "0s - loss: 0.8620 - acc: 0.8696\n",
      "Epoch 243/500\n",
      "0s - loss: 0.8564 - acc: 0.8696\n",
      "Epoch 244/500\n",
      "0s - loss: 0.8530 - acc: 0.9130\n",
      "Epoch 245/500\n",
      "0s - loss: 0.8481 - acc: 0.9130\n",
      "Epoch 246/500\n",
      "0s - loss: 0.8455 - acc: 0.9565\n",
      "Epoch 247/500\n",
      "0s - loss: 0.8427 - acc: 0.9130\n",
      "Epoch 248/500\n",
      "0s - loss: 0.8346 - acc: 0.9565\n",
      "Epoch 249/500\n",
      "0s - loss: 0.8354 - acc: 0.9130\n",
      "Epoch 250/500\n",
      "0s - loss: 0.8285 - acc: 0.9565\n",
      "Epoch 251/500\n",
      "0s - loss: 0.8234 - acc: 0.9565\n",
      "Epoch 252/500\n",
      "0s - loss: 0.8212 - acc: 0.9130\n",
      "Epoch 253/500\n",
      "0s - loss: 0.8158 - acc: 0.9565\n",
      "Epoch 254/500\n",
      "0s - loss: 0.8164 - acc: 0.9565\n",
      "Epoch 255/500\n",
      "0s - loss: 0.8075 - acc: 0.9130\n",
      "Epoch 256/500\n",
      "0s - loss: 0.8080 - acc: 0.9130\n",
      "Epoch 257/500\n",
      "0s - loss: 0.8047 - acc: 0.9130\n",
      "Epoch 258/500\n",
      "0s - loss: 0.7972 - acc: 0.9565\n",
      "Epoch 259/500\n",
      "0s - loss: 0.7882 - acc: 0.9565\n",
      "Epoch 260/500\n",
      "0s - loss: 0.7913 - acc: 0.8696\n",
      "Epoch 261/500\n",
      "0s - loss: 0.7876 - acc: 0.9130\n",
      "Epoch 262/500\n",
      "0s - loss: 0.7814 - acc: 0.9565\n",
      "Epoch 263/500\n",
      "0s - loss: 0.7781 - acc: 0.9565\n",
      "Epoch 264/500\n",
      "0s - loss: 0.7820 - acc: 0.9565\n",
      "Epoch 265/500\n",
      "0s - loss: 0.7725 - acc: 0.9565\n",
      "Epoch 266/500\n",
      "0s - loss: 0.7696 - acc: 0.9565\n",
      "Epoch 267/500\n",
      "0s - loss: 0.7658 - acc: 0.9565\n",
      "Epoch 268/500\n",
      "0s - loss: 0.7586 - acc: 0.9130\n",
      "Epoch 269/500\n",
      "0s - loss: 0.7579 - acc: 0.9565\n",
      "Epoch 270/500\n",
      "0s - loss: 0.7531 - acc: 0.9565\n",
      "Epoch 271/500\n",
      "0s - loss: 0.7517 - acc: 0.9565\n",
      "Epoch 272/500\n",
      "0s - loss: 0.7451 - acc: 0.9130\n",
      "Epoch 273/500\n",
      "0s - loss: 0.7384 - acc: 0.9565\n",
      "Epoch 274/500\n",
      "0s - loss: 0.7452 - acc: 0.9130\n",
      "Epoch 275/500\n",
      "0s - loss: 0.7415 - acc: 0.9565\n",
      "Epoch 276/500\n",
      "0s - loss: 0.7330 - acc: 0.9565\n",
      "Epoch 277/500\n",
      "0s - loss: 0.7355 - acc: 0.9565\n",
      "Epoch 278/500\n",
      "0s - loss: 0.7215 - acc: 0.9565\n",
      "Epoch 279/500\n",
      "0s - loss: 0.7151 - acc: 0.9565\n",
      "Epoch 280/500\n",
      "0s - loss: 0.7133 - acc: 0.9565\n",
      "Epoch 281/500\n",
      "0s - loss: 0.7213 - acc: 0.9565\n",
      "Epoch 282/500\n",
      "0s - loss: 0.7079 - acc: 0.9565\n",
      "Epoch 283/500\n",
      "0s - loss: 0.7087 - acc: 0.9565\n",
      "Epoch 284/500\n",
      "0s - loss: 0.6999 - acc: 0.9565\n",
      "Epoch 285/500\n",
      "0s - loss: 0.6959 - acc: 0.9565\n",
      "Epoch 286/500\n",
      "0s - loss: 0.6986 - acc: 0.9565\n",
      "Epoch 287/500\n",
      "0s - loss: 0.6957 - acc: 0.9130\n",
      "Epoch 288/500\n",
      "0s - loss: 0.6874 - acc: 0.9565\n",
      "Epoch 289/500\n",
      "0s - loss: 0.6869 - acc: 0.9130\n",
      "Epoch 290/500\n",
      "0s - loss: 0.6758 - acc: 0.9565\n",
      "Epoch 291/500\n",
      "0s - loss: 0.6750 - acc: 0.9565\n",
      "Epoch 292/500\n",
      "0s - loss: 0.6783 - acc: 0.9565\n",
      "Epoch 293/500\n",
      "0s - loss: 0.6670 - acc: 0.9565\n",
      "Epoch 294/500\n",
      "0s - loss: 0.6724 - acc: 0.9565\n",
      "Epoch 295/500\n",
      "0s - loss: 0.6647 - acc: 0.9565\n",
      "Epoch 296/500\n",
      "0s - loss: 0.6599 - acc: 0.9565\n",
      "Epoch 297/500\n",
      "0s - loss: 0.6591 - acc: 0.9565\n",
      "Epoch 298/500\n",
      "0s - loss: 0.6561 - acc: 0.9130\n",
      "Epoch 299/500\n",
      "0s - loss: 0.6527 - acc: 0.9565\n",
      "Epoch 300/500\n",
      "0s - loss: 0.6489 - acc: 0.9565\n",
      "Epoch 301/500\n",
      "0s - loss: 0.6488 - acc: 0.9565\n",
      "Epoch 302/500\n",
      "0s - loss: 0.6410 - acc: 0.9565\n",
      "Epoch 303/500\n",
      "0s - loss: 0.6358 - acc: 0.9130\n",
      "Epoch 304/500\n",
      "0s - loss: 0.6312 - acc: 0.9565\n",
      "Epoch 305/500\n",
      "0s - loss: 0.6317 - acc: 0.9565\n",
      "Epoch 306/500\n",
      "0s - loss: 0.6258 - acc: 0.9565\n",
      "Epoch 307/500\n",
      "0s - loss: 0.6262 - acc: 0.9565\n",
      "Epoch 308/500\n",
      "0s - loss: 0.6247 - acc: 0.9565\n",
      "Epoch 309/500\n",
      "0s - loss: 0.6185 - acc: 0.9565\n",
      "Epoch 310/500\n",
      "0s - loss: 0.6123 - acc: 0.9565\n",
      "Epoch 311/500\n",
      "0s - loss: 0.6116 - acc: 0.9565\n",
      "Epoch 312/500\n",
      "0s - loss: 0.6070 - acc: 0.9565\n",
      "Epoch 313/500\n",
      "0s - loss: 0.6028 - acc: 0.9565\n",
      "Epoch 314/500\n",
      "0s - loss: 0.6036 - acc: 0.9565\n",
      "Epoch 315/500\n",
      "0s - loss: 0.5980 - acc: 0.9130\n",
      "Epoch 316/500\n",
      "0s - loss: 0.5995 - acc: 0.9565\n",
      "Epoch 317/500\n",
      "0s - loss: 0.5966 - acc: 0.9565\n",
      "Epoch 318/500\n",
      "0s - loss: 0.5921 - acc: 0.9565\n",
      "Epoch 319/500\n",
      "0s - loss: 0.5881 - acc: 0.9565\n",
      "Epoch 320/500\n",
      "0s - loss: 0.5857 - acc: 0.9565\n",
      "Epoch 321/500\n",
      "0s - loss: 0.5884 - acc: 0.9565\n",
      "Epoch 322/500\n",
      "0s - loss: 0.5864 - acc: 0.9565\n",
      "Epoch 323/500\n",
      "0s - loss: 0.5847 - acc: 0.9565\n",
      "Epoch 324/500\n",
      "0s - loss: 0.5787 - acc: 0.9565\n",
      "Epoch 325/500\n",
      "0s - loss: 0.5710 - acc: 0.9565\n",
      "Epoch 326/500\n",
      "0s - loss: 0.5742 - acc: 0.9565\n",
      "Epoch 327/500\n",
      "0s - loss: 0.5797 - acc: 0.9130\n",
      "Epoch 328/500\n",
      "0s - loss: 0.5698 - acc: 0.9565\n",
      "Epoch 329/500\n",
      "0s - loss: 0.5617 - acc: 0.9130\n",
      "Epoch 330/500\n",
      "0s - loss: 0.5628 - acc: 0.9565\n",
      "Epoch 331/500\n",
      "0s - loss: 0.5557 - acc: 0.9565\n",
      "Epoch 332/500\n",
      "0s - loss: 0.5480 - acc: 0.9565\n",
      "Epoch 333/500\n",
      "0s - loss: 0.5505 - acc: 0.9565\n",
      "Epoch 334/500\n",
      "0s - loss: 0.5447 - acc: 0.9565\n",
      "Epoch 335/500\n",
      "0s - loss: 0.5475 - acc: 0.9565\n",
      "Epoch 336/500\n",
      "0s - loss: 0.5375 - acc: 0.9565\n",
      "Epoch 337/500\n",
      "0s - loss: 0.5380 - acc: 0.9565\n",
      "Epoch 338/500\n",
      "0s - loss: 0.5324 - acc: 0.9565\n",
      "Epoch 339/500\n",
      "0s - loss: 0.5329 - acc: 0.9565\n",
      "Epoch 340/500\n",
      "0s - loss: 0.5367 - acc: 0.9565\n",
      "Epoch 341/500\n",
      "0s - loss: 0.5274 - acc: 0.9565\n",
      "Epoch 342/500\n",
      "0s - loss: 0.5223 - acc: 0.9565\n",
      "Epoch 343/500\n",
      "0s - loss: 0.5187 - acc: 0.9565\n",
      "Epoch 344/500\n",
      "0s - loss: 0.5195 - acc: 0.9565\n",
      "Epoch 345/500\n",
      "0s - loss: 0.5147 - acc: 0.9565\n",
      "Epoch 346/500\n",
      "0s - loss: 0.5112 - acc: 0.9565\n",
      "Epoch 347/500\n",
      "0s - loss: 0.5074 - acc: 0.9565\n",
      "Epoch 348/500\n",
      "0s - loss: 0.5090 - acc: 0.9565\n",
      "Epoch 349/500\n",
      "0s - loss: 0.5029 - acc: 0.9565\n",
      "Epoch 350/500\n",
      "0s - loss: 0.5072 - acc: 0.9565\n",
      "Epoch 351/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.5044 - acc: 0.9565\n",
      "Epoch 352/500\n",
      "0s - loss: 0.4945 - acc: 0.9565\n",
      "Epoch 353/500\n",
      "0s - loss: 0.5001 - acc: 0.9565\n",
      "Epoch 354/500\n",
      "0s - loss: 0.4909 - acc: 0.9565\n",
      "Epoch 355/500\n",
      "0s - loss: 0.4883 - acc: 0.9565\n",
      "Epoch 356/500\n",
      "0s - loss: 0.4883 - acc: 0.9565\n",
      "Epoch 357/500\n",
      "0s - loss: 0.4829 - acc: 0.9565\n",
      "Epoch 358/500\n",
      "0s - loss: 0.4869 - acc: 0.9565\n",
      "Epoch 359/500\n",
      "0s - loss: 0.4822 - acc: 0.9130\n",
      "Epoch 360/500\n",
      "0s - loss: 0.4793 - acc: 0.9565\n",
      "Epoch 361/500\n",
      "0s - loss: 0.4796 - acc: 0.9565\n",
      "Epoch 362/500\n",
      "0s - loss: 0.4710 - acc: 0.9565\n",
      "Epoch 363/500\n",
      "0s - loss: 0.4742 - acc: 0.9565\n",
      "Epoch 364/500\n",
      "0s - loss: 0.4717 - acc: 0.9130\n",
      "Epoch 365/500\n",
      "0s - loss: 0.4647 - acc: 0.9565\n",
      "Epoch 366/500\n",
      "0s - loss: 0.4654 - acc: 0.9565\n",
      "Epoch 367/500\n",
      "0s - loss: 0.4612 - acc: 0.9565\n",
      "Epoch 368/500\n",
      "0s - loss: 0.4598 - acc: 0.9565\n",
      "Epoch 369/500\n",
      "0s - loss: 0.4591 - acc: 0.9565\n",
      "Epoch 370/500\n",
      "0s - loss: 0.4560 - acc: 0.9565\n",
      "Epoch 371/500\n",
      "0s - loss: 0.4585 - acc: 0.9565\n",
      "Epoch 372/500\n",
      "0s - loss: 0.4530 - acc: 0.9565\n",
      "Epoch 373/500\n",
      "0s - loss: 0.4527 - acc: 0.9565\n",
      "Epoch 374/500\n",
      "0s - loss: 0.4444 - acc: 0.9565\n",
      "Epoch 375/500\n",
      "0s - loss: 0.4428 - acc: 0.9565\n",
      "Epoch 376/500\n",
      "0s - loss: 0.4401 - acc: 0.9565\n",
      "Epoch 377/500\n",
      "0s - loss: 0.4421 - acc: 0.9565\n",
      "Epoch 378/500\n",
      "0s - loss: 0.4322 - acc: 0.9565\n",
      "Epoch 379/500\n",
      "0s - loss: 0.4337 - acc: 0.9565\n",
      "Epoch 380/500\n",
      "0s - loss: 0.4384 - acc: 0.9565\n",
      "Epoch 381/500\n",
      "0s - loss: 0.4358 - acc: 0.9565\n",
      "Epoch 382/500\n",
      "0s - loss: 0.4302 - acc: 0.9565\n",
      "Epoch 383/500\n",
      "0s - loss: 0.4267 - acc: 0.9565\n",
      "Epoch 384/500\n",
      "0s - loss: 0.4236 - acc: 0.9565\n",
      "Epoch 385/500\n",
      "0s - loss: 0.4239 - acc: 0.9565\n",
      "Epoch 386/500\n",
      "0s - loss: 0.4240 - acc: 0.9565\n",
      "Epoch 387/500\n",
      "0s - loss: 0.4147 - acc: 0.9565\n",
      "Epoch 388/500\n",
      "0s - loss: 0.4157 - acc: 0.9565\n",
      "Epoch 389/500\n",
      "0s - loss: 0.4194 - acc: 0.9565\n",
      "Epoch 390/500\n",
      "0s - loss: 0.4147 - acc: 0.9565\n",
      "Epoch 391/500\n",
      "0s - loss: 0.4126 - acc: 0.9130\n",
      "Epoch 392/500\n",
      "0s - loss: 0.4103 - acc: 0.9565\n",
      "Epoch 393/500\n",
      "0s - loss: 0.4064 - acc: 0.9565\n",
      "Epoch 394/500\n",
      "0s - loss: 0.4086 - acc: 0.9565\n",
      "Epoch 395/500\n",
      "0s - loss: 0.4023 - acc: 0.9565\n",
      "Epoch 396/500\n",
      "0s - loss: 0.3958 - acc: 0.9565\n",
      "Epoch 397/500\n",
      "0s - loss: 0.3954 - acc: 0.9565\n",
      "Epoch 398/500\n",
      "0s - loss: 0.3967 - acc: 0.9565\n",
      "Epoch 399/500\n",
      "0s - loss: 0.3899 - acc: 0.9565\n",
      "Epoch 400/500\n",
      "0s - loss: 0.3963 - acc: 0.9565\n",
      "Epoch 401/500\n",
      "0s - loss: 0.3931 - acc: 0.9565\n",
      "Epoch 402/500\n",
      "0s - loss: 0.3851 - acc: 1.0000\n",
      "Epoch 403/500\n",
      "0s - loss: 0.3866 - acc: 0.9565\n",
      "Epoch 404/500\n",
      "0s - loss: 0.3816 - acc: 1.0000\n",
      "Epoch 405/500\n",
      "0s - loss: 0.3759 - acc: 0.9565\n",
      "Epoch 406/500\n",
      "0s - loss: 0.3759 - acc: 0.9565\n",
      "Epoch 407/500\n",
      "0s - loss: 0.3784 - acc: 1.0000\n",
      "Epoch 408/500\n",
      "0s - loss: 0.3747 - acc: 0.9565\n",
      "Epoch 409/500\n",
      "0s - loss: 0.3706 - acc: 0.9565\n",
      "Epoch 410/500\n",
      "0s - loss: 0.3725 - acc: 1.0000\n",
      "Epoch 411/500\n",
      "0s - loss: 0.3689 - acc: 1.0000\n",
      "Epoch 412/500\n",
      "0s - loss: 0.3674 - acc: 0.9565\n",
      "Epoch 413/500\n",
      "0s - loss: 0.3753 - acc: 0.9565\n",
      "Epoch 414/500\n",
      "0s - loss: 0.3851 - acc: 0.9565\n",
      "Epoch 415/500\n",
      "0s - loss: 0.3664 - acc: 1.0000\n",
      "Epoch 416/500\n",
      "0s - loss: 0.3604 - acc: 1.0000\n",
      "Epoch 417/500\n",
      "0s - loss: 0.3560 - acc: 0.9565\n",
      "Epoch 418/500\n",
      "0s - loss: 0.3550 - acc: 0.9565\n",
      "Epoch 419/500\n",
      "0s - loss: 0.3522 - acc: 0.9565\n",
      "Epoch 420/500\n",
      "0s - loss: 0.3506 - acc: 0.9565\n",
      "Epoch 421/500\n",
      "0s - loss: 0.3491 - acc: 0.9565\n",
      "Epoch 422/500\n",
      "0s - loss: 0.3505 - acc: 0.9565\n",
      "Epoch 423/500\n",
      "0s - loss: 0.3473 - acc: 1.0000\n",
      "Epoch 424/500\n",
      "0s - loss: 0.3459 - acc: 0.9565\n",
      "Epoch 425/500\n",
      "0s - loss: 0.3436 - acc: 0.9565\n",
      "Epoch 426/500\n",
      "0s - loss: 0.3387 - acc: 0.9565\n",
      "Epoch 427/500\n",
      "0s - loss: 0.3382 - acc: 1.0000\n",
      "Epoch 428/500\n",
      "0s - loss: 0.3404 - acc: 1.0000\n",
      "Epoch 429/500\n",
      "0s - loss: 0.3357 - acc: 0.9565\n",
      "Epoch 430/500\n",
      "0s - loss: 0.3474 - acc: 0.9565\n",
      "Epoch 431/500\n",
      "0s - loss: 0.3343 - acc: 0.9565\n",
      "Epoch 432/500\n",
      "0s - loss: 0.3291 - acc: 1.0000\n",
      "Epoch 433/500\n",
      "0s - loss: 0.3309 - acc: 1.0000\n",
      "Epoch 434/500\n",
      "0s - loss: 0.3289 - acc: 1.0000\n",
      "Epoch 435/500\n",
      "0s - loss: 0.3285 - acc: 0.9565\n",
      "Epoch 436/500\n",
      "0s - loss: 0.3231 - acc: 0.9565\n",
      "Epoch 437/500\n",
      "0s - loss: 0.3263 - acc: 1.0000\n",
      "Epoch 438/500\n",
      "0s - loss: 0.3198 - acc: 1.0000\n",
      "Epoch 439/500\n",
      "0s - loss: 0.3243 - acc: 1.0000\n",
      "Epoch 440/500\n",
      "0s - loss: 0.3173 - acc: 1.0000\n",
      "Epoch 441/500\n",
      "0s - loss: 0.3140 - acc: 0.9565\n",
      "Epoch 442/500\n",
      "0s - loss: 0.3160 - acc: 1.0000\n",
      "Epoch 443/500\n",
      "0s - loss: 0.3169 - acc: 0.9565\n",
      "Epoch 444/500\n",
      "0s - loss: 0.3090 - acc: 1.0000\n",
      "Epoch 445/500\n",
      "0s - loss: 0.3076 - acc: 0.9565\n",
      "Epoch 446/500\n",
      "0s - loss: 0.3105 - acc: 0.9565\n",
      "Epoch 447/500\n",
      "0s - loss: 0.3023 - acc: 0.9565\n",
      "Epoch 448/500\n",
      "0s - loss: 0.3047 - acc: 0.9565\n",
      "Epoch 449/500\n",
      "0s - loss: 0.3021 - acc: 1.0000\n",
      "Epoch 450/500\n",
      "0s - loss: 0.3056 - acc: 1.0000\n",
      "Epoch 451/500\n",
      "0s - loss: 0.3011 - acc: 1.0000\n",
      "Epoch 452/500\n",
      "0s - loss: 0.2990 - acc: 1.0000\n",
      "Epoch 453/500\n",
      "0s - loss: 0.2967 - acc: 1.0000\n",
      "Epoch 454/500\n",
      "0s - loss: 0.2970 - acc: 0.9565\n",
      "Epoch 455/500\n",
      "0s - loss: 0.2958 - acc: 1.0000\n",
      "Epoch 456/500\n",
      "0s - loss: 0.2947 - acc: 1.0000\n",
      "Epoch 457/500\n",
      "0s - loss: 0.2960 - acc: 0.9565\n",
      "Epoch 458/500\n",
      "0s - loss: 0.2895 - acc: 1.0000\n",
      "Epoch 459/500\n",
      "0s - loss: 0.2923 - acc: 1.0000\n",
      "Epoch 460/500\n",
      "0s - loss: 0.2847 - acc: 0.9565\n",
      "Epoch 461/500\n",
      "0s - loss: 0.3021 - acc: 1.0000\n",
      "Epoch 462/500\n",
      "0s - loss: 0.2957 - acc: 1.0000\n",
      "Epoch 463/500\n",
      "0s - loss: 0.2877 - acc: 1.0000\n",
      "Epoch 464/500\n",
      "0s - loss: 0.2815 - acc: 1.0000\n",
      "Epoch 465/500\n",
      "0s - loss: 0.2824 - acc: 0.9565\n",
      "Epoch 466/500\n",
      "0s - loss: 0.2810 - acc: 1.0000\n",
      "Epoch 467/500\n",
      "0s - loss: 0.2737 - acc: 1.0000\n",
      "Epoch 468/500\n",
      "0s - loss: 0.2745 - acc: 0.9565\n",
      "Epoch 469/500\n",
      "0s - loss: 0.2777 - acc: 0.9565\n",
      "Epoch 470/500\n",
      "0s - loss: 0.2745 - acc: 1.0000\n",
      "Epoch 471/500\n",
      "0s - loss: 0.2733 - acc: 1.0000\n",
      "Epoch 472/500\n",
      "0s - loss: 0.2688 - acc: 0.9565\n",
      "Epoch 473/500\n",
      "0s - loss: 0.2801 - acc: 1.0000\n",
      "Epoch 474/500\n",
      "0s - loss: 0.2737 - acc: 0.9565\n",
      "Epoch 475/500\n",
      "0s - loss: 0.2721 - acc: 1.0000\n",
      "Epoch 476/500\n",
      "0s - loss: 0.2758 - acc: 1.0000\n",
      "Epoch 477/500\n",
      "0s - loss: 0.2699 - acc: 0.9565\n",
      "Epoch 478/500\n",
      "0s - loss: 0.2606 - acc: 1.0000\n",
      "Epoch 479/500\n",
      "0s - loss: 0.2606 - acc: 1.0000\n",
      "Epoch 480/500\n",
      "0s - loss: 0.2637 - acc: 1.0000\n",
      "Epoch 481/500\n",
      "0s - loss: 0.2661 - acc: 1.0000\n",
      "Epoch 482/500\n",
      "0s - loss: 0.2580 - acc: 0.9565\n",
      "Epoch 483/500\n",
      "0s - loss: 0.2610 - acc: 1.0000\n",
      "Epoch 484/500\n",
      "0s - loss: 0.2572 - acc: 1.0000\n",
      "Epoch 485/500\n",
      "0s - loss: 0.2576 - acc: 1.0000\n",
      "Epoch 486/500\n",
      "0s - loss: 0.2537 - acc: 0.9565\n",
      "Epoch 487/500\n",
      "0s - loss: 0.2531 - acc: 1.0000\n",
      "Epoch 488/500\n",
      "0s - loss: 0.2525 - acc: 1.0000\n",
      "Epoch 489/500\n",
      "0s - loss: 0.2492 - acc: 1.0000\n",
      "Epoch 490/500\n",
      "0s - loss: 0.2485 - acc: 1.0000\n",
      "Epoch 491/500\n",
      "0s - loss: 0.2455 - acc: 1.0000\n",
      "Epoch 492/500\n",
      "0s - loss: 0.2454 - acc: 0.9565\n",
      "Epoch 493/500\n",
      "0s - loss: 0.2444 - acc: 1.0000\n",
      "Epoch 494/500\n",
      "0s - loss: 0.2434 - acc: 1.0000\n",
      "Epoch 495/500\n",
      "0s - loss: 0.2419 - acc: 1.0000\n",
      "Epoch 496/500\n",
      "0s - loss: 0.2396 - acc: 1.0000\n",
      "Epoch 497/500\n",
      "0s - loss: 0.2409 - acc: 1.0000\n",
      "Epoch 498/500\n",
      "0s - loss: 0.2371 - acc: 1.0000\n",
      "Epoch 499/500\n",
      "0s - loss: 0.2389 - acc: 1.0000\n",
      "Epoch 500/500\n",
      "0s - loss: 0.2363 - acc: 1.0000\n",
      "Model Accuracy: 100.00%\n",
      "(['A', 'B', 'C'], '->', 'D')\n",
      "(['B', 'C', 'D'], '->', 'E')\n",
      "(['C', 'D', 'E'], '->', 'F')\n",
      "(['D', 'E', 'F'], '->', 'G')\n",
      "(['E', 'F', 'G'], '->', 'H')\n",
      "(['F', 'G', 'H'], '->', 'I')\n",
      "(['G', 'H', 'I'], '->', 'J')\n",
      "(['H', 'I', 'J'], '->', 'K')\n",
      "(['I', 'J', 'K'], '->', 'L')\n",
      "(['J', 'K', 'L'], '->', 'M')\n",
      "(['K', 'L', 'M'], '->', 'N')\n",
      "(['L', 'M', 'N'], '->', 'O')\n",
      "(['M', 'N', 'O'], '->', 'P')\n",
      "(['N', 'O', 'P'], '->', 'Q')\n",
      "(['O', 'P', 'Q'], '->', 'R')\n",
      "(['P', 'Q', 'R'], '->', 'S')\n",
      "(['Q', 'R', 'S'], '->', 'T')\n",
      "(['R', 'S', 'T'], '->', 'U')\n",
      "(['S', 'T', 'U'], '->', 'V')\n",
      "(['T', 'U', 'V'], '->', 'W')\n",
      "(['U', 'V', 'W'], '->', 'X')\n",
      "(['V', 'W', 'X'], '->', 'Y')\n",
      "(['W', 'X', 'Y'], '->', 'Z')\n"
     ]
    }
   ],
   "source": [
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "  seq_in = alphabet[i:i + seq_length]\n",
    "  seq_out = alphabet[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "  print(seq_in,  \"->\", seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation= 'softmax' ))\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "\n",
    "visible = Input(shape=(10,))\n",
    "hidden1 = Dense(10, activation='relu')(visible) \n",
    "hidden2 = Dense(20, activation='relu')(hidden1) \n",
    "hidden3 = Dense(10, activation='relu')(hidden2) \n",
    "output = Dense(1, activation='sigmoid')(hidden3) \n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 4 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,64,1,64], [4,4,64,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-7908ed462091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvisible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3164\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   3165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         op=op)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[0;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dilation_rate must be positive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36mop\u001b[0;34m(input_converted, _, padding)\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[0;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NDHWC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    398\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PML/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,64,1,64], [4,4,64,32]."
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "visible = Input(shape=(64,64,1))\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible) \n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1) \n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "hidden1 = Dense(10, activation='relu')(pool2) \n",
    "output = Dense(1, activation='sigmoid')(hidden1) \n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['1', 'b', 'e']\n",
    "re.sub('l', 'qq', a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(a)):\n",
    "# #     a[i] = a[i].replace('l', '')\n",
    "#     re.sub('l', 'qq', a[i])\n",
    "b = [re.sub('b', 'qq', i) for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'qq', 'e']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [word.upper() if word is not 'e' else word for word in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'QQ', 'e']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(1, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "    \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence # define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence # define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "from keras.preprocessing.text import text_to_word_sequence # define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[3.0, 6.0, 4.0, 6.0, 9.0, 7.0, 3.0, 9.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence \n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_wine()\n",
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ['a', 'b', 'c', 'd', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vv = ''.join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcde'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
